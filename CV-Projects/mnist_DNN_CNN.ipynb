{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#MNIST-Dataset\" data-toc-modified-id=\"MNIST-Dataset-1\">MNIST Dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Classifying-Digits-via-DenseNet-in-Tensorflow\" data-toc-modified-id=\"Classifying-Digits-via-DenseNet-in-Tensorflow-1.1\">Classifying Digits via DenseNet in Tensorflow</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ben-Fox-|-CS-281B-|-04/14/2019\" data-toc-modified-id=\"Ben-Fox-|-CS-281B-|-04/14/2019-1.1.1\">Ben Fox | CS 281B | 04/14/2019</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset\n",
    "## Classifying Digits via DenseNet in Tensorflow\n",
    "### Ben Fox | CS 281B | 04/14/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from keras.utils.np_utils import to_categorical\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mnist data\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb2823a278>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_train[10], cmap = 'gray_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000, 10) (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set tf placeholders for X and Y data\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1]) # placerholder, images are 28,28,1, None refers to batch size, will be filled during training\n",
    "Y = tf.placeholder(tf.float32, [None, 10]) # placeholder, output is a 1 hot vector for the 10 possible classes\n",
    "step = tf.placeholder(tf.float32) # place holder for the learning rate function\n",
    "pkeep = tf.placeholder(tf.float32) # placeholder, dropout of neurons \n",
    "\n",
    "# 5 layers and corresponding layer dims\n",
    "L1 = 200\n",
    "L2 = 100\n",
    "L3 = 50\n",
    "L4 = 30\n",
    "\n",
    "# build layers w/ dropout\n",
    "W1 = tf.Variable(tf.truncated_normal([784, L1], stddev=0.1)) # this is the weights matrix, of random numbers of normal dist , 28*28 = 784 and weights for the input to second layer\n",
    "B1 = tf.Variable(tf.ones([L1])/10) # bias for 200 neurons, initialize to non-zero small value\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([L1, L2], stddev=0.1)) # 2nd layer, input 200, output 100\n",
    "B2 = tf.Variable(tf.ones([L2])/10)\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([L2, L3], stddev=0.1)) # 3rd layer, input 100, output 60 \n",
    "B3 = tf.Variable(tf.ones([L3])/10)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([L3, L4], stddev=0.1)) # 4th layer, input 60, output 30\n",
    "B4 = tf.Variable(tf.ones([L4])/10)\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([L4, 10], stddev=0.1)) # 5th layer, input 30, output 10\n",
    "B5 = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# build the model\n",
    "XX = tf.reshape(X, [-1,784]) # -1 means tf will figure out dimensionality to make X flat to 784 columns\n",
    "Y1 = tf.nn.relu(tf.matmul(XX, W1) + B1) # relu activation between layers\n",
    "Y1d = tf.nn.dropout(Y1, pkeep)\n",
    "Y2 = tf.nn.relu(tf.matmul(Y1d, W2) + B2)\n",
    "Y2d = tf.nn.dropout(Y2, pkeep)\n",
    "Y3 = tf.nn.relu(tf.matmul(Y2d, W3) + B3)\n",
    "Y3d = tf.nn.dropout(Y3, pkeep)\n",
    "Y4 = tf.nn.relu(tf.matmul(Y3d, W4) + B4)\n",
    "Y4d = tf.nn.dropout(Y4, pkeep)\n",
    "YLogits = tf.matmul(Y4d, W5) + B5 # save logits for cross entropy loss function, some logs might be 0\n",
    "YPred = tf.nn.softmax(YLogits) # softmax activation for predict\n",
    "\n",
    "# loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=YLogits, labels = Y) # use tf cross entropy loss, deals with log(0) problems\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100 # get loss in percentage\n",
    "\n",
    "# learning rate, utlizing an exponential decay function\n",
    "lr = 0.0001 + tf.train.exponential_decay(0.003, step, 2000, 1/math.e)\n",
    "optimizer = tf.train.AdamOptimizer(lr) # variable learning rate\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# measures\n",
    "#is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(YPred,1))\n",
    "accuracy = tf.metrics.accuracy(tf.argmax(Y,1), tf.argmax(YPred,1))\n",
    "precision = tf.metrics.precision(tf.argmax(Y,1), tf.argmax(YPred,1))\n",
    "recall = tf.metrics.recall(tf.argmax(Y,1), tf.argmax(YPred,1))\n",
    "\n",
    "# initialize session\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# init = tf.global_variables_initializer()\n",
    "# sess = tf.Session() # begin and initialize tf session\n",
    "# sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dnn_training_acc = []\n",
    "dnn_testing_acc = []\n",
    "dnn_train_prec = []\n",
    "dnn_train_rec = []\n",
    "dnn_test_prec = []\n",
    "dnn_test_rec = []\n",
    "dnn_learn_rate = []\n",
    "dnn_train_cross_ent = []\n",
    "dnn_test_cross_ent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:(0.0, 0.09) loss: 230.6654 (lr:0.0031)\n",
      "0: ********* epoch 1 ********* test accuracy:(0.09, 0.10227723) test loss: 231.44208\n",
      "20: accuracy:(0.10227723, 0.10617647) loss: 141.05354 (lr:0.0030701496)\n",
      "40: accuracy:(0.10617647, 0.11242718) loss: 76.7388 (lr:0.0030405961)\n",
      "60: accuracy:(0.11242718, 0.118942305) loss: 59.685307 (lr:0.0030113366)\n",
      "80: accuracy:(0.118942305, 0.12609524) loss: 37.51021 (lr:0.0029823685)\n",
      "100: accuracy:(0.12609524, 0.13339622) loss: 39.072807 (lr:0.0029536884)\n",
      "100: ********* epoch 1 ********* test accuracy:(0.13339622, 0.50242716) test loss: 37.596245\n",
      "120: accuracy:(0.50242716, 0.5043478) loss: 33.35981 (lr:0.0029252938)\n",
      "140: accuracy:(0.5043478, 0.50629807) loss: 30.459986 (lr:0.0028971815)\n",
      "160: accuracy:(0.50629807, 0.5081818) loss: 33.42779 (lr:0.002869349)\n",
      "180: accuracy:(0.5081818, 0.51009524) loss: 28.39212 (lr:0.0028417937)\n",
      "200: accuracy:(0.51009524, 0.5120853) loss: 22.649748 (lr:0.0028145125)\n",
      "200: ********* epoch 1 ********* test accuracy:(0.5120853, 0.6444373) test loss: 28.312687\n",
      "220: accuracy:(0.6444373, 0.6455128) loss: 16.239594 (lr:0.0027875025)\n",
      "240: accuracy:(0.6455128, 0.6464537) loss: 26.303778 (lr:0.0027607614)\n",
      "260: accuracy:(0.6464537, 0.64713377) loss: 51.234715 (lr:0.0027342865)\n",
      "280: accuracy:(0.64713377, 0.648) loss: 26.573853 (lr:0.0027080749)\n",
      "300: accuracy:(0.648, 0.64892405) loss: 15.272895 (lr:0.002682124)\n",
      "300: ********* epoch 1 ********* test accuracy:(0.64892405, 0.7172596) test loss: 23.955322\n",
      "320: accuracy:(0.7172596, 0.7177698) loss: 23.500774 (lr:0.0026564314)\n",
      "340: accuracy:(0.7177698, 0.7182775) loss: 26.468472 (lr:0.0026309947)\n",
      "360: accuracy:(0.7182775, 0.7187351) loss: 28.786907 (lr:0.0026058108)\n",
      "380: accuracy:(0.7187351, 0.7192619) loss: 17.384636 (lr:0.0025808774)\n",
      "400: accuracy:(0.7192619, 0.7197862) loss: 15.676035 (lr:0.0025561925)\n",
      "400: ********* epoch 1 ********* test accuracy:(0.7197862, 0.76272553) test loss: 19.74694\n",
      "420: accuracy:(0.76272553, 0.7631034) loss: 12.93633 (lr:0.002531753)\n",
      "440: accuracy:(0.7631034, 0.7634608) loss: 15.484573 (lr:0.0025075565)\n",
      "460: accuracy:(0.7634608, 0.76375955) loss: 19.82775 (lr:0.002483601)\n",
      "480: accuracy:(0.76375955, 0.76411426) loss: 13.417869 (lr:0.0024598835)\n",
      "500: accuracy:(0.76411426, 0.7644296) loss: 39.555527 (lr:0.0024364025)\n",
      "500: ********* epoch 1 ********* test accuracy:(0.7644296, 0.7945048) test loss: 16.957272\n",
      "520: accuracy:(0.7945048, 0.79473686) loss: 17.796062 (lr:0.0024131548)\n",
      "540: accuracy:(0.79473686, 0.795) loss: 12.851134 (lr:0.0023901386)\n",
      "560: accuracy:(0.795, 0.7952305) loss: 26.57309 (lr:0.0023673512)\n",
      "580: accuracy:(0.7952305, 0.79549205) loss: 11.343746 (lr:0.0023447906)\n",
      "600: accuracy:(0.79549205, 0.7958003) loss: 3.6665998 (lr:0.0023224547)\n",
      "600: ********* epoch 2 ********* test accuracy:(0.7958003, 0.81745553) test loss: 16.191187\n",
      "620: accuracy:(0.81745553, 0.81758195) loss: 29.328588 (lr:0.002300341)\n",
      "640: accuracy:(0.81758195, 0.81774896) loss: 14.24555 (lr:0.0022784472)\n",
      "660: accuracy:(0.81774896, 0.81792915) loss: 18.637413 (lr:0.0022567713)\n",
      "680: accuracy:(0.81792915, 0.81813604) loss: 12.179989 (lr:0.002235311)\n",
      "700: accuracy:(0.81813604, 0.8183288) loss: 12.396392 (lr:0.0022140644)\n",
      "700: ********* epoch 2 ********* test accuracy:(0.8183288, 0.83465314) test loss: 15.802464\n",
      "720: accuracy:(0.83465314, 0.83477896) loss: 30.739191 (lr:0.002193029)\n",
      "740: accuracy:(0.83477896, 0.8349403) loss: 15.464478 (lr:0.002172203)\n",
      "760: accuracy:(0.8349403, 0.83506554) loss: 22.794136 (lr:0.0021515843)\n",
      "780: accuracy:(0.83506554, 0.8352262) loss: 11.140913 (lr:0.0021311706)\n",
      "800: accuracy:(0.8352262, 0.8353983) loss: 9.055561 (lr:0.0021109602)\n",
      "800: ********* epoch 2 ********* test accuracy:(0.8353983, 0.8486929) test loss: 14.355122\n",
      "820: accuracy:(0.8486929, 0.84881103) loss: 19.879091 (lr:0.0020909507)\n",
      "840: accuracy:(0.84881103, 0.8489289) loss: 24.568298 (lr:0.0020711406)\n",
      "860: accuracy:(0.8489289, 0.84903604) loss: 15.117447 (lr:0.0020515274)\n",
      "880: accuracy:(0.84903604, 0.8491746) loss: 13.702853 (lr:0.0020321093)\n",
      "900: accuracy:(0.8491746, 0.84927064) loss: 26.860035 (lr:0.0020128845)\n",
      "900: ********* epoch 2 ********* test accuracy:(0.84927064, 0.8597992) test loss: 15.121479\n",
      "920: accuracy:(0.8597992, 0.8599236) loss: 5.2489567 (lr:0.001993851)\n",
      "940: accuracy:(0.8599236, 0.86000955) loss: 24.345898 (lr:0.001975007)\n",
      "960: accuracy:(0.86000955, 0.86010486) loss: 16.319872 (lr:0.0019563502)\n",
      "980: accuracy:(0.86010486, 0.8602) loss: 18.248127 (lr:0.0019378791)\n",
      "1000: accuracy:(0.8602, 0.8603045) loss: 12.757579 (lr:0.0019195919)\n",
      "1000: ********* epoch 2 ********* test accuracy:(0.8603045, 0.86921805) test loss: 13.521196\n",
      "1020: accuracy:(0.86921805, 0.8692882) loss: 19.010271 (lr:0.0019014867)\n",
      "1040: accuracy:(0.8692882, 0.8693669) loss: 18.306738 (lr:0.0018835615)\n",
      "1060: accuracy:(0.8693669, 0.86947143) loss: 7.7926254 (lr:0.0018658149)\n",
      "1080: accuracy:(0.86947143, 0.86957574) loss: 7.2506933 (lr:0.0018482447)\n",
      "1100: accuracy:(0.86957574, 0.8696713) loss: 9.097423 (lr:0.0018308494)\n",
      "1100: ********* epoch 2 ********* test accuracy:(0.8696713, 0.8771019) test loss: 13.94994\n",
      "1120: accuracy:(0.8771019, 0.8771679) loss: 16.50645 (lr:0.0018136272)\n",
      "1140: accuracy:(0.8771679, 0.8772575) loss: 11.417313 (lr:0.0017965762)\n",
      "1160: accuracy:(0.8772575, 0.8773312) loss: 8.71456 (lr:0.001779695)\n",
      "1180: accuracy:(0.8773312, 0.87740475) loss: 8.2870245 (lr:0.0017629818)\n",
      "1200: accuracy:(0.87740475, 0.8774782) loss: 10.988012 (lr:0.0017464348)\n",
      "1200: ********* epoch 3 ********* test accuracy:(0.8774782, 0.88404113) test loss: 12.396896\n",
      "1220: accuracy:(0.88404113, 0.88410425) loss: 9.446151 (lr:0.0017300526)\n",
      "1240: accuracy:(0.88410425, 0.88414526) loss: 19.385786 (lr:0.0017138333)\n",
      "1260: accuracy:(0.88414526, 0.8842082) loss: 14.271273 (lr:0.0016977752)\n",
      "1280: accuracy:(0.8842082, 0.8842711) loss: 9.838895 (lr:0.0016818773)\n",
      "1300: accuracy:(0.8842711, 0.88431185) loss: 28.573353 (lr:0.0016661374)\n",
      "1300: ********* epoch 3 ********* test accuracy:(0.88431185, 0.8898772) test loss: 12.632975\n",
      "1320: accuracy:(0.8898772, 0.88993186) loss: 11.920149 (lr:0.0016505539)\n",
      "1340: accuracy:(0.88993186, 0.8899864) loss: 18.36114 (lr:0.0016351256)\n",
      "1360: accuracy:(0.8899864, 0.89004767) loss: 5.561489 (lr:0.001619851)\n",
      "1380: accuracy:(0.89004767, 0.89008164) loss: 27.424896 (lr:0.0016047282)\n",
      "1400: accuracy:(0.89008164, 0.8901563) loss: 2.5284956 (lr:0.0015897558)\n",
      "1400: ********* epoch 3 ********* test accuracy:(0.8901563, 0.8950987) test loss: 11.885731\n",
      "1420: accuracy:(0.8950987, 0.8951527) loss: 7.7898355 (lr:0.0015749325)\n",
      "1440: accuracy:(0.8951527, 0.89520663) loss: 6.354394 (lr:0.0015602567)\n",
      "1460: accuracy:(0.89520663, 0.89524776) loss: 11.1601715 (lr:0.0015457269)\n",
      "1480: accuracy:(0.89524776, 0.89528257) loss: 16.970509 (lr:0.0015313418)\n",
      "1500: accuracy:(0.89528257, 0.89532995) loss: 6.1844535 (lr:0.0015170996)\n",
      "1500: ********* epoch 3 ********* test accuracy:(0.89532995, 0.89968973) test loss: 12.0386715\n",
      "1520: accuracy:(0.89968973, 0.89973766) loss: 8.442347 (lr:0.0015029992)\n",
      "1540: accuracy:(0.89973766, 0.89978546) loss: 5.0942345 (lr:0.0014890392)\n",
      "1560: accuracy:(0.89978546, 0.8998273) loss: 9.376286 (lr:0.001475218)\n",
      "1580: accuracy:(0.8998273, 0.899875) loss: 5.688224 (lr:0.0014615343)\n",
      "1600: accuracy:(0.899875, 0.8999167) loss: 8.431627 (lr:0.0014479868)\n",
      "1600: ********* epoch 3 ********* test accuracy:(0.8999167, 0.90392476) test loss: 11.216082\n",
      "1620: accuracy:(0.90392476, 0.90396744) loss: 8.1965 (lr:0.0014345741)\n",
      "1640: accuracy:(0.90396744, 0.9040045) loss: 12.316337 (lr:0.0014212949)\n",
      "1660: accuracy:(0.9040045, 0.90404147) loss: 9.972225 (lr:0.0014081479)\n",
      "1680: accuracy:(0.90404147, 0.9040784) loss: 6.816174 (lr:0.0013951315)\n",
      "1700: accuracy:(0.9040784, 0.9041153) loss: 11.857866 (lr:0.0013822448)\n",
      "1700: ********* epoch 3 ********* test accuracy:(0.9041153, 0.9076882) test loss: 11.171538\n",
      "1720: accuracy:(0.9076882, 0.90773183) loss: 4.8109665 (lr:0.0013694862)\n",
      "1740: accuracy:(0.90773183, 0.90777016) loss: 4.9751086 (lr:0.0013568546)\n",
      "1760: accuracy:(0.90777016, 0.90779775) loss: 21.084291 (lr:0.0013443487)\n",
      "1780: accuracy:(0.90779775, 0.9078307) loss: 12.664466 (lr:0.0013319672)\n",
      "1800: accuracy:(0.9078307, 0.90786886) loss: 6.076131 (lr:0.0013197089)\n",
      "1800: ********* epoch 4 ********* test accuracy:(0.90786886, 0.9110648) test loss: 11.079141\n",
      "1820: accuracy:(0.9110648, 0.91110444) loss: 5.5773573 (lr:0.0013075727)\n",
      "1840: accuracy:(0.91110444, 0.911149) loss: 1.3643386 (lr:0.0012955571)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1860: accuracy:(0.911149, 0.91118354) loss: 2.9507742 (lr:0.001283661)\n",
      "1880: accuracy:(0.91118354, 0.91121304) loss: 11.107164 (lr:0.0012718835)\n",
      "1900: accuracy:(0.91121304, 0.9112325) loss: 25.165419 (lr:0.001260223)\n",
      "1900: ********* epoch 4 ********* test accuracy:(0.9112325, 0.9142032) test loss: 10.810454\n",
      "1920: accuracy:(0.9142032, 0.9142394) loss: 3.4137714 (lr:0.0012486785)\n",
      "1940: accuracy:(0.9142394, 0.9142612) loss: 10.874089 (lr:0.0012372491)\n",
      "1960: accuracy:(0.9142612, 0.9142782) loss: 16.151434 (lr:0.0012259333)\n",
      "1980: accuracy:(0.9142782, 0.9143095) loss: 15.636449 (lr:0.00121473)\n",
      "2000: accuracy:(0.9143095, 0.9143408) loss: 6.2279086 (lr:0.0012036383)\n",
      "2000: ********* epoch 4 ********* test accuracy:(0.9143408, 0.9169877) test loss: 10.781751\n",
      "2020: accuracy:(0.9169877, 0.9170163) loss: 5.104961 (lr:0.0011926569)\n",
      "2040: accuracy:(0.9170163, 0.9170359) loss: 17.350632 (lr:0.0011817847)\n",
      "2060: accuracy:(0.9170359, 0.9170599) loss: 13.446874 (lr:0.0011710208)\n",
      "2080: accuracy:(0.9170599, 0.9170794) loss: 14.452336 (lr:0.0011603639)\n",
      "2100: accuracy:(0.9170794, 0.9171079) loss: 12.121909 (lr:0.0011498132)\n",
      "2100: ********* epoch 4 ********* test accuracy:(0.9171079, 0.91955334) test loss: 10.700941\n",
      "2120: accuracy:(0.91955334, 0.9195752) loss: 9.128797 (lr:0.0011393673)\n",
      "2140: accuracy:(0.9195752, 0.919597) loss: 12.326753 (lr:0.0011290255)\n",
      "2160: accuracy:(0.919597, 0.9196189) loss: 7.72308 (lr:0.0011187865)\n",
      "2180: accuracy:(0.9196189, 0.919619) loss: 23.203125 (lr:0.0011086494)\n",
      "2200: accuracy:(0.919619, 0.91964084) loss: 14.174919 (lr:0.0010986131)\n",
      "2200: ********* epoch 4 ********* test accuracy:(0.91964084, 0.921825) test loss: 11.010537\n",
      "2220: accuracy:(0.921825, 0.9218491) loss: 11.397059 (lr:0.0010886769)\n",
      "2240: accuracy:(0.9218491, 0.9218649) loss: 11.289495 (lr:0.0010788393)\n",
      "2260: accuracy:(0.9218649, 0.92187655) loss: 12.919762 (lr:0.0010690998)\n",
      "2280: accuracy:(0.92187655, 0.92190474) loss: 8.702301 (lr:0.001059457)\n",
      "2300: accuracy:(0.92190474, 0.9219371) loss: 0.9334308 (lr:0.0010499102)\n",
      "2300: ********* epoch 4 ********* test accuracy:(0.9219371, 0.9239905) test loss: 10.662947\n",
      "2320: accuracy:(0.9239905, 0.9240127) loss: 5.5878077 (lr:0.0010404584)\n",
      "2340: accuracy:(0.9240127, 0.92402303) loss: 19.495985 (lr:0.0010311007)\n",
      "2360: accuracy:(0.92402303, 0.9240492) loss: 4.9504123 (lr:0.0010218362)\n",
      "2380: accuracy:(0.9240492, 0.9240754) loss: 5.271351 (lr:0.0010126637)\n",
      "2400: accuracy:(0.9240754, 0.9240976) loss: 6.9624534 (lr:0.0010035826)\n",
      "2400: ********* epoch 5 ********* test accuracy:(0.9240976, 0.92597103) test loss: 10.694799\n",
      "2420: accuracy:(0.92597103, 0.9259878) loss: 13.064155 (lr:0.0009945917)\n",
      "2440: accuracy:(0.9259878, 0.9260122) loss: 4.40778 (lr:0.0009856905)\n",
      "2460: accuracy:(0.9260122, 0.9260366) loss: 1.9080122 (lr:0.0009768778)\n",
      "2480: accuracy:(0.9260366, 0.92604953) loss: 13.4963455 (lr:0.00096815266)\n",
      "2500: accuracy:(0.92604953, 0.92607385) loss: 6.371771 (lr:0.0009595144)\n",
      "2500: ********* epoch 5 ********* test accuracy:(0.92607385, 0.92786133) test loss: 10.552916\n",
      "2520: accuracy:(0.92786133, 0.92788047) loss: 6.1232047 (lr:0.000950962)\n",
      "2540: accuracy:(0.92788047, 0.92790323) loss: 7.080254 (lr:0.00094249484)\n",
      "2560: accuracy:(0.92790323, 0.92792964) loss: 2.6117177 (lr:0.0009341118)\n",
      "2580: accuracy:(0.92792964, 0.9279487) loss: 6.1162353 (lr:0.0009258123)\n",
      "2600: accuracy:(0.9279487, 0.9279678) loss: 9.829484 (lr:0.00091759535)\n",
      "2600: ********* epoch 5 ********* test accuracy:(0.9279678, 0.9295902) test loss: 10.593338\n",
      "2620: accuracy:(0.9295902, 0.92961514) loss: 0.22494818 (lr:0.0009094601)\n",
      "2640: accuracy:(0.92961514, 0.92963994) loss: 0.7677892 (lr:0.0009014059)\n",
      "2660: accuracy:(0.92963994, 0.9296648) loss: 1.0719324 (lr:0.0008934318)\n",
      "2680: accuracy:(0.9296648, 0.92967194) loss: 17.227816 (lr:0.00088553707)\n",
      "2700: accuracy:(0.92967194, 0.9296897) loss: 9.587255 (lr:0.0008777208)\n",
      "2700: ********* epoch 5 ********* test accuracy:(0.9296897, 0.93122274) test loss: 10.235792\n",
      "2720: accuracy:(0.93122274, 0.93123937) loss: 7.8536777 (lr:0.00086998235)\n",
      "2740: accuracy:(0.93123937, 0.93125933) loss: 4.198233 (lr:0.0008623208)\n",
      "2760: accuracy:(0.93125933, 0.93127596) loss: 7.1592307 (lr:0.0008547356)\n",
      "2780: accuracy:(0.93127596, 0.9312993) loss: 2.4122448 (lr:0.0008472259)\n",
      "2800: accuracy:(0.9312993, 0.9313193) loss: 5.095958 (lr:0.00083979085)\n",
      "2800: ********* epoch 5 ********* test accuracy:(0.9313193, 0.93276554) test loss: 10.231449\n",
      "2820: accuracy:(0.93276554, 0.93277776) loss: 10.5853 (lr:0.00083242985)\n",
      "2840: accuracy:(0.93277776, 0.9327966) loss: 2.7654822 (lr:0.000825142)\n",
      "2860: accuracy:(0.9327966, 0.93280554) loss: 9.89633 (lr:0.0008179267)\n",
      "2880: accuracy:(0.93280554, 0.93281776) loss: 8.981177 (lr:0.00081078324)\n",
      "2900: accuracy:(0.93281776, 0.9328299) loss: 10.014379 (lr:0.00080371083)\n",
      "2900: ********* epoch 5 ********* test accuracy:(0.9328299, 0.93420213) test loss: 9.9508\n",
      "2920: accuracy:(0.93420213, 0.9342199) loss: 3.0778053 (lr:0.00079670886)\n",
      "2940: accuracy:(0.9342199, 0.9342376) loss: 3.4272776 (lr:0.00078977644)\n",
      "2960: accuracy:(0.9342376, 0.934249) loss: 8.998145 (lr:0.00078291306)\n",
      "2980: accuracy:(0.934249, 0.9342603) loss: 10.563746 (lr:0.00077611796)\n",
      "3000: accuracy:(0.9342603, 0.9342685) loss: 15.076925 (lr:0.00076939043)\n",
      "3000: ********* epoch 6 ********* test accuracy:(0.9342685, 0.9355737) test loss: 9.322928\n",
      "3020: accuracy:(0.9355737, 0.9355935) loss: 0.86808765 (lr:0.0007627299)\n",
      "3040: accuracy:(0.9355935, 0.93560714) loss: 6.179575 (lr:0.0007561356)\n",
      "3060: accuracy:(0.93560714, 0.9356208) loss: 9.444129 (lr:0.000749607)\n",
      "3080: accuracy:(0.9356208, 0.93562824) loss: 9.99595 (lr:0.0007431433)\n",
      "3100: accuracy:(0.93562824, 0.9356419) loss: 6.4730277 (lr:0.0007367439)\n",
      "3100: ********* epoch 6 ********* test accuracy:(0.9356419, 0.93684447) test loss: 10.068743\n",
      "3120: accuracy:(0.93684447, 0.93685436) loss: 8.527617 (lr:0.00073040824)\n",
      "3140: accuracy:(0.93685436, 0.9368642) loss: 11.021334 (lr:0.00072413555)\n",
      "3160: accuracy:(0.9368642, 0.93688005) loss: 8.642954 (lr:0.0007179253)\n",
      "3180: accuracy:(0.93688005, 0.93689287) loss: 4.601859 (lr:0.00071177684)\n",
      "3200: accuracy:(0.93689287, 0.93690866) loss: 3.1844504 (lr:0.00070568954)\n",
      "3200: ********* epoch 6 ********* test accuracy:(0.93690866, 0.9380526) test loss: 10.224859\n",
      "3220: accuracy:(0.9380526, 0.9380647) loss: 3.3774302 (lr:0.0006996628)\n",
      "3240: accuracy:(0.9380647, 0.93807393) loss: 8.633116 (lr:0.0006936961)\n",
      "3260: accuracy:(0.93807393, 0.93808025) loss: 9.976047 (lr:0.0006877887)\n",
      "3280: accuracy:(0.93808025, 0.9380952) loss: 3.3864863 (lr:0.00068194006)\n",
      "3300: accuracy:(0.9380952, 0.9381131) loss: 2.5475717 (lr:0.0006761497)\n",
      "3300: ********* epoch 6 ********* test accuracy:(0.9381131, 0.93915594) test loss: 9.966866\n",
      "3320: accuracy:(0.93915594, 0.9391702) loss: 2.8933868 (lr:0.0006704169)\n",
      "3340: accuracy:(0.9391702, 0.9391872) loss: 1.4218329 (lr:0.00066474115)\n",
      "3360: accuracy:(0.9391872, 0.9392015) loss: 2.7591002 (lr:0.0006591219)\n",
      "3380: accuracy:(0.9392015, 0.93921566) loss: 5.162551 (lr:0.0006535585)\n",
      "3400: accuracy:(0.93921566, 0.9392299) loss: 2.101535 (lr:0.00064805057)\n",
      "3400: ********* epoch 6 ********* test accuracy:(0.9392299, 0.9402179) test loss: 10.020269\n",
      "3420: accuracy:(0.9402179, 0.9402342) loss: 0.5564637 (lr:0.0006425974)\n",
      "3440: accuracy:(0.9402342, 0.94024503) loss: 11.988458 (lr:0.00063719845)\n",
      "3460: accuracy:(0.94024503, 0.94025856) loss: 5.501171 (lr:0.0006318532)\n",
      "3480: accuracy:(0.94025856, 0.9402721) loss: 2.5280142 (lr:0.0006265612)\n",
      "3500: accuracy:(0.9402721, 0.9402802) loss: 6.233598 (lr:0.0006213218)\n",
      "3500: ********* epoch 6 ********* test accuracy:(0.9402802, 0.9412209) test loss: 10.034309\n",
      "3520: accuracy:(0.9412209, 0.9412285) loss: 15.148406 (lr:0.0006161346)\n",
      "3540: accuracy:(0.9412285, 0.9412414) loss: 7.418969 (lr:0.00061099895)\n",
      "3560: accuracy:(0.9412414, 0.941249) loss: 7.044616 (lr:0.00060591445)\n",
      "3580: accuracy:(0.941249, 0.9412619) loss: 7.9211965 (lr:0.0006008805)\n",
      "3600: accuracy:(0.9412619, 0.9412748) loss: 1.9926593 (lr:0.0005958966)\n",
      "3600: ********* epoch 7 ********* test accuracy:(0.9412748, 0.942167) test loss: 9.787824\n",
      "3620: accuracy:(0.942167, 0.9421819) loss: 1.6801959 (lr:0.0005909624)\n",
      "3640: accuracy:(0.9421819, 0.94219416) loss: 3.5595226 (lr:0.0005860772)\n",
      "3660: accuracy:(0.94219416, 0.94220906) loss: 1.8572226 (lr:0.0005812407)\n",
      "3680: accuracy:(0.94220906, 0.94222134) loss: 2.3905218 (lr:0.0005764523)\n",
      "3700: accuracy:(0.94222134, 0.94223106) loss: 10.049008 (lr:0.00057171145)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3700: ********* epoch 7 ********* test accuracy:(0.94223106, 0.94312847) test loss: 9.495302\n",
      "3720: accuracy:(0.94312847, 0.9431327) loss: 10.655029 (lr:0.0005670178)\n",
      "3740: accuracy:(0.9431327, 0.94314194) loss: 12.48651 (lr:0.00056237093)\n",
      "3760: accuracy:(0.94314194, 0.9431537) loss: 12.21082 (lr:0.00055777025)\n",
      "3780: accuracy:(0.9431537, 0.9431654) loss: 4.6278186 (lr:0.0005532154)\n",
      "3800: accuracy:(0.9431654, 0.94317466) loss: 4.8205223 (lr:0.00054870587)\n",
      "3800: ********* epoch 7 ********* test accuracy:(0.94317466, 0.9440064) test loss: 9.879574\n",
      "3820: accuracy:(0.9440064, 0.9440176) loss: 6.99434 (lr:0.0005442411)\n",
      "3840: accuracy:(0.9440176, 0.94402885) loss: 3.0785139 (lr:0.0005398209)\n",
      "3860: accuracy:(0.94402885, 0.94404006) loss: 9.993935 (lr:0.0005354446)\n",
      "3880: accuracy:(0.94404006, 0.94405127) loss: 3.5670466 (lr:0.00053111184)\n",
      "3900: accuracy:(0.94405127, 0.94405764) loss: 6.5594287 (lr:0.0005268222)\n",
      "3900: ********* epoch 7 ********* test accuracy:(0.94405764, 0.94478554) test loss: 9.946131\n",
      "3920: accuracy:(0.94478554, 0.94479626) loss: 5.1185303 (lr:0.0005225753)\n",
      "3940: accuracy:(0.94479626, 0.94480705) loss: 5.3596797 (lr:0.00051837054)\n",
      "3960: accuracy:(0.94480705, 0.94481784) loss: 3.189983 (lr:0.0005142077)\n",
      "3980: accuracy:(0.94481784, 0.94483095) loss: 1.2742614 (lr:0.00051008625)\n",
      "4000: accuracy:(0.94483095, 0.94484407) loss: 0.76144046 (lr:0.0005060059)\n",
      "4000: ********* epoch 7 ********* test accuracy:(0.94484407, 0.9456034) test loss: 9.800985\n",
      "4020: accuracy:(0.9456034, 0.9456137) loss: 3.507296 (lr:0.00050196605)\n",
      "4040: accuracy:(0.9456137, 0.9456263) loss: 0.6400133 (lr:0.0004979664)\n",
      "4060: accuracy:(0.9456263, 0.94562966) loss: 8.991629 (lr:0.0004940065)\n",
      "4080: accuracy:(0.94562966, 0.9456423) loss: 0.20749639 (lr:0.0004900861)\n",
      "4100: accuracy:(0.9456423, 0.9456526) loss: 6.503684 (lr:0.00048620466)\n",
      "4100: ********* epoch 7 ********* test accuracy:(0.9456526, 0.9463527) test loss: 9.75963\n",
      "4120: accuracy:(0.9463527, 0.94636035) loss: 7.141184 (lr:0.00048236188)\n",
      "4140: accuracy:(0.94636035, 0.9463725) loss: 0.51280123 (lr:0.00047855728)\n",
      "4160: accuracy:(0.9463725, 0.9463824) loss: 1.915031 (lr:0.0004747906)\n",
      "4180: accuracy:(0.9463824, 0.9463923) loss: 2.400187 (lr:0.0004710614)\n",
      "4200: accuracy:(0.9463923, 0.9464022) loss: 4.1574965 (lr:0.00046736925)\n",
      "4200: ********* epoch 8 ********* test accuracy:(0.9464022, 0.94704944) test loss: 9.750882\n",
      "4220: accuracy:(0.94704944, 0.9470612) loss: 3.1966455 (lr:0.00046371386)\n",
      "4240: accuracy:(0.9470612, 0.94707066) loss: 7.759264 (lr:0.00046009486)\n",
      "4260: accuracy:(0.94707066, 0.9470802) loss: 3.788033 (lr:0.0004565119)\n",
      "4280: accuracy:(0.9470802, 0.9470897) loss: 3.365398 (lr:0.0004529645)\n",
      "4300: accuracy:(0.9470897, 0.9471014) loss: 0.40673372 (lr:0.00044945246)\n",
      "4300: ********* epoch 8 ********* test accuracy:(0.9471014, 0.9477535) test loss: 9.610172\n",
      "4320: accuracy:(0.9477535, 0.94775826) loss: 12.383133 (lr:0.00044597537)\n",
      "4340: accuracy:(0.94775826, 0.94776744) loss: 4.0525036 (lr:0.00044253285)\n",
      "4360: accuracy:(0.94776744, 0.9477744) loss: 3.4569316 (lr:0.0004391246)\n",
      "4380: accuracy:(0.9477744, 0.94777924) loss: 7.473771 (lr:0.00043575026)\n",
      "4400: accuracy:(0.94777924, 0.9477905) loss: 0.42996722 (lr:0.0004324095)\n",
      "4400: ********* epoch 8 ********* test accuracy:(0.9477905, 0.94843465) test loss: 9.576997\n",
      "4420: accuracy:(0.94843465, 0.9484413) loss: 3.3823252 (lr:0.00042910196)\n",
      "4440: accuracy:(0.9484413, 0.94845223) loss: 1.4594933 (lr:0.00042582734)\n",
      "4460: accuracy:(0.94845223, 0.94846314) loss: 0.83176124 (lr:0.00042258532)\n",
      "4480: accuracy:(0.94846314, 0.9484656) loss: 7.805525 (lr:0.00041937552)\n",
      "4500: accuracy:(0.9484656, 0.94847226) loss: 4.8622913 (lr:0.0004161977)\n",
      "4500: ********* epoch 8 ********* test accuracy:(0.94847226, 0.9490696) test loss: 9.513669\n",
      "4520: accuracy:(0.9490696, 0.9490781) loss: 3.5527506 (lr:0.0004130515)\n",
      "4540: accuracy:(0.9490781, 0.9490845) loss: 7.1304574 (lr:0.0004099365)\n",
      "4560: accuracy:(0.9490845, 0.949093) loss: 3.3177633 (lr:0.00040685257)\n",
      "4580: accuracy:(0.949093, 0.94910353) loss: 1.6567926 (lr:0.00040379935)\n",
      "4600: accuracy:(0.94910353, 0.9491099) loss: 11.209335 (lr:0.00040077648)\n",
      "4600: ********* epoch 8 ********* test accuracy:(0.9491099, 0.9496735) test loss: 9.908118\n",
      "4620: accuracy:(0.9496735, 0.9496797) loss: 4.767003 (lr:0.00039778373)\n",
      "4640: accuracy:(0.9496797, 0.94968987) loss: 0.64403087 (lr:0.00039482073)\n",
      "4660: accuracy:(0.94968987, 0.949694) loss: 9.95328 (lr:0.0003918872)\n",
      "4680: accuracy:(0.949694, 0.94970214) loss: 2.1003978 (lr:0.0003889829)\n",
      "4700: accuracy:(0.94970214, 0.94970626) loss: 10.853285 (lr:0.00038610745)\n",
      "4700: ********* epoch 8 ********* test accuracy:(0.94970626, 0.9502641) test loss: 9.744646\n",
      "4720: accuracy:(0.9502641, 0.950274) loss: 0.6250255 (lr:0.00038326066)\n",
      "4740: accuracy:(0.950274, 0.9502799) loss: 4.3275857 (lr:0.00038044216)\n",
      "4760: accuracy:(0.9502799, 0.95028776) loss: 3.6122098 (lr:0.00037765174)\n",
      "4780: accuracy:(0.95028776, 0.9502917) loss: 10.378451 (lr:0.00037488903)\n",
      "4800: accuracy:(0.9502917, 0.95029956) loss: 2.9011528 (lr:0.00037215385)\n",
      "4800: ********* epoch 9 ********* test accuracy:(0.95029956, 0.95084614) test loss: 9.513958\n",
      "4820: accuracy:(0.95084614, 0.9508499) loss: 8.580017 (lr:0.0003694459)\n",
      "4840: accuracy:(0.9508499, 0.95085746) loss: 1.358665 (lr:0.00036676484)\n",
      "4860: accuracy:(0.95085746, 0.95086706) loss: 0.91446054 (lr:0.0003641105)\n",
      "4880: accuracy:(0.95086706, 0.9508746) loss: 6.67194 (lr:0.00036148255)\n",
      "4900: accuracy:(0.9508746, 0.95088416) loss: 1.4125531 (lr:0.00035888076)\n",
      "4900: ********* epoch 9 ********* test accuracy:(0.95088416, 0.95140105) test loss: 10.020612\n",
      "4920: accuracy:(0.95140105, 0.95140845) loss: 2.1808815 (lr:0.00035630487)\n",
      "4940: accuracy:(0.95140845, 0.9514158) loss: 6.011799 (lr:0.00035375459)\n",
      "4960: accuracy:(0.9514158, 0.9514212) loss: 4.601927 (lr:0.00035122968)\n",
      "4980: accuracy:(0.9514212, 0.9514267) loss: 8.43317 (lr:0.00034872993)\n",
      "5000: accuracy:(0.9514267, 0.9514359) loss: 1.6691849 (lr:0.00034625502)\n",
      "5000: ********* epoch 9 ********* test accuracy:(0.9514359, 0.9519342) test loss: 9.775442\n",
      "5020: accuracy:(0.9519342, 0.9519413) loss: 8.508052 (lr:0.00034380468)\n",
      "5040: accuracy:(0.9519413, 0.95194656) loss: 12.737052 (lr:0.0003413788)\n",
      "5060: accuracy:(0.95194656, 0.95195556) loss: 0.301938 (lr:0.00033897703)\n",
      "5080: accuracy:(0.95195556, 0.95196265) loss: 4.9309964 (lr:0.0003365992)\n",
      "5100: accuracy:(0.95196265, 0.9519679) loss: 4.5996985 (lr:0.00033424495)\n",
      "5100: ********* epoch 9 ********* test accuracy:(0.9519679, 0.9524395) test loss: 9.469998\n",
      "5120: accuracy:(0.9524395, 0.95244825) loss: 1.2006888 (lr:0.0003319142)\n",
      "5140: accuracy:(0.95244825, 0.95245695) loss: 0.25020936 (lr:0.0003296066)\n",
      "5160: accuracy:(0.95245695, 0.952462) loss: 4.63921 (lr:0.000327322)\n",
      "5180: accuracy:(0.952462, 0.952467) loss: 6.022349 (lr:0.0003250601)\n",
      "5200: accuracy:(0.952467, 0.9524757) loss: 0.7393584 (lr:0.0003228207)\n",
      "5200: ********* epoch 9 ********* test accuracy:(0.9524757, 0.95293474) test loss: 9.560353\n",
      "5220: accuracy:(0.95293474, 0.9529414) loss: 4.4998 (lr:0.00032060363)\n",
      "5240: accuracy:(0.9529414, 0.95294803) loss: 3.2936876 (lr:0.00031840857)\n",
      "5260: accuracy:(0.95294803, 0.9529565) loss: 1.2507366 (lr:0.00031623538)\n",
      "5280: accuracy:(0.9529565, 0.9529632) loss: 7.96584 (lr:0.0003140838)\n",
      "5300: accuracy:(0.9529632, 0.9529698) loss: 2.1974008 (lr:0.00031195363)\n",
      "5300: ********* epoch 9 ********* test accuracy:(0.9529698, 0.9534151) test loss: 9.555538\n",
      "5320: accuracy:(0.9534151, 0.953418) loss: 9.717355 (lr:0.00030984465)\n",
      "5340: accuracy:(0.953418, 0.95342624) loss: 2.1031454 (lr:0.00030775668)\n",
      "5360: accuracy:(0.95342624, 0.95343447) loss: 1.9471235 (lr:0.00030568946)\n",
      "5380: accuracy:(0.95343447, 0.9534427) loss: 0.5589309 (lr:0.00030364282)\n",
      "5400: accuracy:(0.9534427, 0.9534509) loss: 0.8442433 (lr:0.00030161653)\n",
      "5400: ********* epoch 10 ********* test accuracy:(0.9534509, 0.95388323) test loss: 9.475026\n",
      "5420: accuracy:(0.95388323, 0.9538912) loss: 2.171082 (lr:0.00029961043)\n",
      "5440: accuracy:(0.9538912, 0.95389223) loss: 8.526489 (lr:0.00029762427)\n",
      "5460: accuracy:(0.95389223, 0.95389676) loss: 7.603498 (lr:0.00029565787)\n",
      "5480: accuracy:(0.95389676, 0.95390475) loss: 0.05130751 (lr:0.00029371103)\n",
      "5500: accuracy:(0.95390475, 0.953911) loss: 5.1875114 (lr:0.00029178357)\n",
      "5500: ********* epoch 10 ********* test accuracy:(0.953911, 0.9543329) test loss: 9.41777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5520: accuracy:(0.9543329, 0.95433897) loss: 2.5810363 (lr:0.0002898753)\n",
      "5540: accuracy:(0.95433897, 0.9543467) loss: 0.9372067 (lr:0.00028798598)\n",
      "5560: accuracy:(0.9543467, 0.9543528) loss: 2.1193616 (lr:0.0002861155)\n",
      "5580: accuracy:(0.9543528, 0.95436054) loss: 0.61088514 (lr:0.0002842636)\n",
      "5600: accuracy:(0.95436054, 0.9543666) loss: 3.510164 (lr:0.00028243018)\n",
      "5600: ********* epoch 10 ********* test accuracy:(0.9543666, 0.9547634) test loss: 9.763625\n",
      "5620: accuracy:(0.9547634, 0.9547693) loss: 4.3686733 (lr:0.00028061497)\n",
      "5640: accuracy:(0.9547693, 0.9547752) loss: 4.4228888 (lr:0.0002788178)\n",
      "5660: accuracy:(0.9547752, 0.95478106) loss: 4.240406 (lr:0.00027703855)\n",
      "5680: accuracy:(0.95478106, 0.9547886) loss: 0.103294015 (lr:0.00027527698)\n",
      "5700: accuracy:(0.9547886, 0.9547945) loss: 10.748441 (lr:0.00027353296)\n",
      "5700: ********* epoch 10 ********* test accuracy:(0.9547945, 0.95517415) test loss: 9.741418\n",
      "5720: accuracy:(0.95517415, 0.9551766) loss: 3.939032 (lr:0.00027180626)\n",
      "5740: accuracy:(0.9551766, 0.9551774) loss: 11.240205 (lr:0.00027009676)\n",
      "5760: accuracy:(0.9551774, 0.95518476) loss: 0.16373082 (lr:0.00026840428)\n",
      "5780: accuracy:(0.95518476, 0.9551921) loss: 1.0369624 (lr:0.00026672863)\n",
      "5800: accuracy:(0.9551921, 0.9551962) loss: 7.7341905 (lr:0.00026506966)\n",
      "5800: ********* epoch 10 ********* test accuracy:(0.9551962, 0.9555613) test loss: 9.86803\n",
      "5820: accuracy:(0.9555613, 0.9555669) loss: 4.027971 (lr:0.00026342718)\n",
      "5840: accuracy:(0.9555669, 0.9555692) loss: 9.717957 (lr:0.00026180106)\n",
      "5860: accuracy:(0.9555692, 0.95557475) loss: 1.7208233 (lr:0.0002601911)\n",
      "5880: accuracy:(0.95557475, 0.9555787) loss: 3.1523716 (lr:0.0002585972)\n",
      "5900: accuracy:(0.9555787, 0.9555842) loss: 2.946516 (lr:0.00025701913)\n",
      "5900: ********* epoch 10 ********* test accuracy:(0.9555842, 0.9559244) test loss: 9.787024\n",
      "5920: accuracy:(0.9559244, 0.9559314) loss: 0.9329231 (lr:0.00025545675)\n",
      "5940: accuracy:(0.9559314, 0.95593524) loss: 15.483739 (lr:0.00025390994)\n",
      "5960: accuracy:(0.95593524, 0.95593905) loss: 5.088376 (lr:0.00025237846)\n",
      "5980: accuracy:(0.95593905, 0.955946) loss: 0.107012466 (lr:0.0002508623)\n",
      "6000: accuracy:(0.955946, 0.9559451) loss: 13.131358 (lr:0.00024936118)\n",
      "6000: ********* epoch 11 ********* test accuracy:(0.9559451, 0.9562803) test loss: 9.82249\n",
      "6020: accuracy:(0.9562803, 0.956284) loss: 5.68653 (lr:0.000247875)\n",
      "6040: accuracy:(0.956284, 0.95628923) loss: 3.690026 (lr:0.00024640362)\n",
      "6060: accuracy:(0.95628923, 0.9562961) loss: 1.2049855 (lr:0.0002449469)\n",
      "6080: accuracy:(0.9562961, 0.9563029) loss: 0.7986549 (lr:0.00024350465)\n",
      "6100: accuracy:(0.9563029, 0.95630974) loss: 0.39552668 (lr:0.00024207676)\n",
      "6100: ********* epoch 11 ********* test accuracy:(0.95630974, 0.95663697) test loss: 9.875655\n",
      "6120: accuracy:(0.95663697, 0.95664054) loss: 5.047798 (lr:0.00024066307)\n",
      "6140: accuracy:(0.95664054, 0.95664567) loss: 3.805406 (lr:0.00023926346)\n",
      "6160: accuracy:(0.95664567, 0.9566508) loss: 2.5014076 (lr:0.00023787777)\n",
      "6180: accuracy:(0.9566508, 0.9566559) loss: 6.7148795 (lr:0.00023650585)\n",
      "6200: accuracy:(0.9566559, 0.95666105) loss: 1.6918255 (lr:0.0002351476)\n",
      "6200: ********* epoch 11 ********* test accuracy:(0.95666105, 0.95698684) test loss: 9.937802\n",
      "6220: accuracy:(0.95698684, 0.95699334) loss: 0.859077 (lr:0.00023380286)\n",
      "6240: accuracy:(0.95699334, 0.95699984) loss: 1.1853732 (lr:0.0002324715)\n",
      "6260: accuracy:(0.95699984, 0.95700634) loss: 0.41672507 (lr:0.00023115339)\n",
      "6280: accuracy:(0.95700634, 0.9570083) loss: 9.098619 (lr:0.00022984837)\n",
      "6300: accuracy:(0.9570083, 0.9570148) loss: 0.37674603 (lr:0.00022855637)\n",
      "6300: ********* epoch 11 ********* test accuracy:(0.9570148, 0.9573302) test loss: 9.910905\n",
      "6320: accuracy:(0.9573302, 0.9573366) loss: 0.3166731 (lr:0.00022727723)\n",
      "6340: accuracy:(0.9573366, 0.957343) loss: 0.6132793 (lr:0.0002260108)\n",
      "6360: accuracy:(0.957343, 0.95734483) loss: 11.049461 (lr:0.00022475695)\n",
      "6380: accuracy:(0.95734483, 0.9573497) loss: 1.541162 (lr:0.00022351561)\n",
      "6400: accuracy:(0.9573497, 0.95735455) loss: 7.057707 (lr:0.00022228662)\n",
      "6400: ********* epoch 11 ********* test accuracy:(0.95735455, 0.9576543) test loss: 9.8686\n",
      "6420: accuracy:(0.9576543, 0.95765907) loss: 3.3603384 (lr:0.00022106984)\n",
      "6440: accuracy:(0.95765907, 0.95766526) loss: 0.06533855 (lr:0.00021986515)\n",
      "6460: accuracy:(0.95766526, 0.95767) loss: 1.7316761 (lr:0.00021867247)\n",
      "6480: accuracy:(0.95767, 0.9576762) loss: 1.6901121 (lr:0.00021749166)\n",
      "6500: accuracy:(0.9576762, 0.95768094) loss: 5.9868507 (lr:0.0002163226)\n",
      "6500: ********* epoch 11 ********* test accuracy:(0.95768094, 0.9579772) test loss: 9.627584\n",
      "6520: accuracy:(0.9579772, 0.9579818) loss: 5.162053 (lr:0.00021516517)\n",
      "6540: accuracy:(0.9579818, 0.9579879) loss: 0.72543967 (lr:0.00021401927)\n",
      "6560: accuracy:(0.9579879, 0.9579939) loss: 1.4556804 (lr:0.00021288476)\n",
      "6580: accuracy:(0.9579939, 0.95799714) loss: 4.160178 (lr:0.00021176154)\n",
      "6600: accuracy:(0.95799714, 0.9580003) loss: 6.864062 (lr:0.0002106495)\n",
      "6600: ********* epoch 12 ********* test accuracy:(0.9580003, 0.958289) test loss: 9.49172\n",
      "6620: accuracy:(0.958289, 0.9582935) loss: 4.3798633 (lr:0.00020954851)\n",
      "6640: accuracy:(0.9582935, 0.958298) loss: 5.530082 (lr:0.00020845848)\n",
      "6660: accuracy:(0.958298, 0.9583039) loss: 0.25200683 (lr:0.00020737931)\n",
      "6680: accuracy:(0.9583039, 0.9583099) loss: 1.0922664 (lr:0.00020631085)\n",
      "6700: accuracy:(0.9583099, 0.95831436) loss: 3.3544977 (lr:0.00020525305)\n",
      "6700: ********* epoch 12 ********* test accuracy:(0.95831436, 0.9586015) test loss: 9.829611\n",
      "6720: accuracy:(0.9586015, 0.95860445) loss: 3.4522164 (lr:0.00020420577)\n",
      "6740: accuracy:(0.95860445, 0.95860887) loss: 1.7373303 (lr:0.00020316891)\n",
      "6760: accuracy:(0.95860887, 0.9586133) loss: 3.6808794 (lr:0.00020214237)\n",
      "6780: accuracy:(0.9586133, 0.95861626) loss: 9.916327 (lr:0.00020112604)\n",
      "6800: accuracy:(0.95861626, 0.95862204) loss: 0.8237119 (lr:0.0002001198)\n",
      "6800: ********* epoch 12 ********* test accuracy:(0.95862204, 0.95888966) test loss: 9.673227\n",
      "6820: accuracy:(0.95888966, 0.9588926) loss: 8.043498 (lr:0.0001991236)\n",
      "6840: accuracy:(0.9588926, 0.9588955) loss: 3.558906 (lr:0.0001981373)\n",
      "6860: accuracy:(0.9588955, 0.95890117) loss: 1.0391974 (lr:0.00019716081)\n",
      "6880: accuracy:(0.95890117, 0.95890546) loss: 1.2149377 (lr:0.00019619406)\n",
      "6900: accuracy:(0.95890546, 0.95890975) loss: 2.2676094 (lr:0.0001952369)\n",
      "6900: ********* epoch 12 ********* test accuracy:(0.95890975, 0.95916414) test loss: 9.685626\n",
      "6920: accuracy:(0.95916414, 0.95916975) loss: 0.76299125 (lr:0.00019428927)\n",
      "6940: accuracy:(0.95916975, 0.9591753) loss: 0.07662889 (lr:0.00019335106)\n",
      "6960: accuracy:(0.9591753, 0.9591781) loss: 3.36088 (lr:0.00019242222)\n",
      "6980: accuracy:(0.9591781, 0.9591796) loss: 7.2377563 (lr:0.0001915026)\n",
      "7000: accuracy:(0.9591796, 0.9591838) loss: 3.0439892 (lr:0.00019059214)\n",
      "7000: ********* epoch 12 ********* test accuracy:(0.9591838, 0.9594363) test loss: 9.688829\n",
      "7020: accuracy:(0.9594363, 0.9594418) loss: 0.46504718 (lr:0.00018969073)\n",
      "7040: accuracy:(0.9594418, 0.95944583) loss: 4.2158318 (lr:0.0001887983)\n",
      "7060: accuracy:(0.95944583, 0.95944864) loss: 7.636401 (lr:0.00018791473)\n",
      "7080: accuracy:(0.95944864, 0.9594514) loss: 7.527718 (lr:0.00018703997)\n",
      "7100: accuracy:(0.9594514, 0.9594555) loss: 1.4015913 (lr:0.00018617391)\n",
      "7100: ********* epoch 12 ********* test accuracy:(0.9594555, 0.95969826) test loss: 9.894509\n",
      "7120: accuracy:(0.95969826, 0.95970225) loss: 5.850126 (lr:0.00018531646)\n",
      "7140: accuracy:(0.95970225, 0.95970625) loss: 3.2124348 (lr:0.00018446756)\n",
      "7160: accuracy:(0.95970625, 0.9597103) loss: 1.1231346 (lr:0.00018362708)\n",
      "7180: accuracy:(0.9597103, 0.9597143) loss: 4.2094655 (lr:0.00018279499)\n",
      "7200: accuracy:(0.9597143, 0.9597183) loss: 3.2801907 (lr:0.00018197116)\n",
      "7200: ********* epoch 13 ********* test accuracy:(0.9597183, 0.9599595) test loss: 9.650685\n",
      "7220: accuracy:(0.9599595, 0.9599621) loss: 2.5320559 (lr:0.00018115554)\n",
      "7240: accuracy:(0.9599621, 0.9599674) loss: 1.4104843 (lr:0.00018034803)\n",
      "7260: accuracy:(0.9599674, 0.95997) loss: 11.172764 (lr:0.00017954854)\n",
      "7280: accuracy:(0.95997, 0.9599752) loss: 0.8868576 (lr:0.00017875704)\n",
      "7300: accuracy:(0.9599752, 0.9599765) loss: 11.685532 (lr:0.00017797339)\n",
      "7300: ********* epoch 13 ********* test accuracy:(0.9599765, 0.96021634) test loss: 9.535175\n",
      "7320: accuracy:(0.96021634, 0.96022016) loss: 5.6798186 (lr:0.00017719754)\n",
      "7340: accuracy:(0.96022016, 0.9602227) loss: 6.5896354 (lr:0.0001764294)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7360: accuracy:(0.9602227, 0.9602252) loss: 7.2706604 (lr:0.00017566892)\n",
      "7380: accuracy:(0.9602252, 0.96023035) loss: 0.61111057 (lr:0.000174916)\n",
      "7400: accuracy:(0.96023035, 0.9602342) loss: 2.1653776 (lr:0.00017417056)\n",
      "7400: ********* epoch 13 ********* test accuracy:(0.9602342, 0.9604688) test loss: 9.537003\n",
      "7420: accuracy:(0.9604688, 0.9604713) loss: 3.692554 (lr:0.00017343255)\n",
      "7440: accuracy:(0.9604713, 0.960475) loss: 5.29715 (lr:0.0001727019)\n",
      "7460: accuracy:(0.960475, 0.96047753) loss: 3.2689388 (lr:0.00017197849)\n",
      "7480: accuracy:(0.96047753, 0.96048254) loss: 0.9743998 (lr:0.0001712623)\n",
      "7500: accuracy:(0.96048254, 0.9604863) loss: 4.8110027 (lr:0.00017055322)\n",
      "7500: ********* epoch 13 ********* test accuracy:(0.9604863, 0.96071714) test loss: 9.633533\n",
      "7520: accuracy:(0.96071714, 0.96072084) loss: 2.961846 (lr:0.0001698512)\n",
      "7540: accuracy:(0.96072084, 0.96072197) loss: 6.1313996 (lr:0.00016915618)\n",
      "7560: accuracy:(0.96072197, 0.96072567) loss: 1.2850261 (lr:0.00016846808)\n",
      "7580: accuracy:(0.96072567, 0.96072805) loss: 5.296251 (lr:0.0001677868)\n",
      "7600: accuracy:(0.96072805, 0.96073174) loss: 3.3027313 (lr:0.0001671123)\n",
      "7600: ********* epoch 13 ********* test accuracy:(0.96073174, 0.9609578) test loss: 9.636129\n",
      "7620: accuracy:(0.9609578, 0.96096015) loss: 11.461924 (lr:0.00016644453)\n",
      "7640: accuracy:(0.96096015, 0.960965) loss: 0.5760558 (lr:0.0001657834)\n",
      "7660: accuracy:(0.960965, 0.9609698) loss: 0.55131567 (lr:0.00016512885)\n",
      "7680: accuracy:(0.9609698, 0.96097463) loss: 1.0085412 (lr:0.00016448079)\n",
      "7700: accuracy:(0.96097463, 0.9609782) loss: 2.6545534 (lr:0.0001638392)\n",
      "7700: ********* epoch 13 ********* test accuracy:(0.9609782, 0.9611935) test loss: 9.851536\n",
      "7720: accuracy:(0.9611935, 0.9611983) loss: 0.19079 (lr:0.000163204)\n",
      "7740: accuracy:(0.9611983, 0.96120054) loss: 3.1475377 (lr:0.0001625751)\n",
      "7760: accuracy:(0.96120054, 0.96120405) loss: 1.9725678 (lr:0.00016195247)\n",
      "7780: accuracy:(0.96120405, 0.9612064) loss: 11.521159 (lr:0.00016133604)\n",
      "7800: accuracy:(0.9612064, 0.96120864) loss: 5.4447308 (lr:0.00016072573)\n",
      "7800: ********* epoch 14 ********* test accuracy:(0.96120864, 0.96140635) test loss: 9.856202\n",
      "7820: accuracy:(0.96140635, 0.961411) loss: 0.82850754 (lr:0.0001601215)\n",
      "7840: accuracy:(0.961411, 0.96141565) loss: 0.9847995 (lr:0.00015952328)\n",
      "7860: accuracy:(0.96141565, 0.9614203) loss: 0.6217075 (lr:0.000158931)\n",
      "7880: accuracy:(0.9614203, 0.96142375) loss: 2.9548745 (lr:0.00015834463)\n",
      "7900: accuracy:(0.96142375, 0.9614272) loss: 2.1430614 (lr:0.0001577641)\n",
      "7900: ********* epoch 14 ********* test accuracy:(0.9614272, 0.9616198) test loss: 9.861621\n",
      "7920: accuracy:(0.9616198, 0.9616244) loss: 1.1982616 (lr:0.00015718934)\n",
      "7940: accuracy:(0.9616244, 0.961629) loss: 1.1473922 (lr:0.00015662028)\n",
      "7960: accuracy:(0.961629, 0.9616323) loss: 1.7753156 (lr:0.0001560569)\n",
      "7980: accuracy:(0.9616323, 0.9616369) loss: 0.5199969 (lr:0.00015549913)\n",
      "8000: accuracy:(0.9616369, 0.9616415) loss: 0.12015537 (lr:0.00015494692)\n",
      "8000: ********* epoch 14 ********* test accuracy:(0.9616415, 0.9618374) test loss: 9.954188\n",
      "8020: accuracy:(0.9618374, 0.96184194) loss: 0.56296915 (lr:0.00015440017)\n",
      "8040: accuracy:(0.96184194, 0.9618452) loss: 3.2497334 (lr:0.0001538589)\n",
      "8060: accuracy:(0.9618452, 0.96184736) loss: 5.5568867 (lr:0.00015332298)\n",
      "8080: accuracy:(0.96184736, 0.96185184) loss: 0.6336877 (lr:0.00015279242)\n",
      "8100: accuracy:(0.96185184, 0.961854) loss: 7.267382 (lr:0.00015226711)\n",
      "8100: ********* epoch 14 ********* test accuracy:(0.961854, 0.9620416) test loss: 9.919897\n",
      "8120: accuracy:(0.9620416, 0.962046) loss: 0.22652689 (lr:0.00015174704)\n",
      "8140: accuracy:(0.962046, 0.96204925) loss: 12.948604 (lr:0.00015123216)\n",
      "8160: accuracy:(0.96204925, 0.9620502) loss: 8.408714 (lr:0.00015072238)\n",
      "8180: accuracy:(0.9620502, 0.9620546) loss: 0.22958806 (lr:0.0001502177)\n",
      "8200: accuracy:(0.9620546, 0.962059) loss: 0.9612989 (lr:0.000149718)\n",
      "8200: ********* epoch 14 ********* test accuracy:(0.962059, 0.9622466) test loss: 9.868355\n",
      "8220: accuracy:(0.9622466, 0.9622486) loss: 2.7895486 (lr:0.00014922331)\n",
      "8240: accuracy:(0.9622486, 0.9622518) loss: 2.8758457 (lr:0.00014873352)\n",
      "8260: accuracy:(0.9622518, 0.96225613) loss: 1.2378287 (lr:0.00014824864)\n",
      "8280: accuracy:(0.96225613, 0.96225816) loss: 8.070223 (lr:0.00014776854)\n",
      "8300: accuracy:(0.96225816, 0.9622602) loss: 6.802045 (lr:0.00014729325)\n",
      "8300: ********* epoch 14 ********* test accuracy:(0.9622602, 0.9624444) test loss: 9.847136\n",
      "8320: accuracy:(0.9624444, 0.96244866) loss: 0.75568277 (lr:0.00014682265)\n",
      "8340: accuracy:(0.96244866, 0.9624518) loss: 4.448863 (lr:0.00014635678)\n",
      "8360: accuracy:(0.9624518, 0.96245265) loss: 10.0617285 (lr:0.00014589551)\n",
      "8380: accuracy:(0.96245265, 0.96245694) loss: 0.738562 (lr:0.00014543885)\n",
      "8400: accuracy:(0.96245694, 0.9624612) loss: 1.6130173 (lr:0.00014498673)\n",
      "8400: ********* epoch 15 ********* test accuracy:(0.9624612, 0.9626376) test loss: 9.634909\n",
      "8420: accuracy:(0.9626376, 0.96264064) loss: 1.9277878 (lr:0.00014453911)\n",
      "8440: accuracy:(0.96264064, 0.9626426) loss: 5.2132874 (lr:0.00014409592)\n",
      "8460: accuracy:(0.9626426, 0.9626468) loss: 0.87511784 (lr:0.00014365718)\n",
      "8480: accuracy:(0.9626468, 0.96264875) loss: 3.261382 (lr:0.00014322277)\n",
      "8500: accuracy:(0.96264875, 0.9626518) loss: 5.2237816 (lr:0.0001427927)\n",
      "8500: ********* epoch 15 ********* test accuracy:(0.9626518, 0.96281964) test loss: 9.738376\n",
      "8520: accuracy:(0.96281964, 0.96282375) loss: 1.079915 (lr:0.0001423669)\n",
      "8540: accuracy:(0.96282375, 0.9628268) loss: 5.058614 (lr:0.00014194535)\n",
      "8560: accuracy:(0.9628268, 0.9628309) loss: 1.7264555 (lr:0.00014152798)\n",
      "8580: accuracy:(0.9628309, 0.962835) loss: 0.24428998 (lr:0.00014111475)\n",
      "8600: accuracy:(0.962835, 0.9628391) loss: 0.48892456 (lr:0.00014070567)\n",
      "8600: ********* epoch 15 ********* test accuracy:(0.9628391, 0.963003) test loss: 9.873119\n",
      "8620: accuracy:(0.963003, 0.9630059) loss: 3.5746078 (lr:0.00014030063)\n",
      "8640: accuracy:(0.9630059, 0.9630089) loss: 3.0549924 (lr:0.00013989964)\n",
      "8660: accuracy:(0.9630089, 0.9630118) loss: 8.229309 (lr:0.00013950263)\n",
      "8680: accuracy:(0.9630118, 0.96301585) loss: 0.19585547 (lr:0.00013910959)\n",
      "8700: accuracy:(0.96301585, 0.96301883) loss: 3.1960652 (lr:0.00013872042)\n",
      "8700: ********* epoch 15 ********* test accuracy:(0.96301883, 0.9631821) test loss: 9.945397\n",
      "8720: accuracy:(0.9631821, 0.963185) loss: 1.8338231 (lr:0.00013833516)\n",
      "8740: accuracy:(0.963185, 0.963189) loss: 0.55328673 (lr:0.00013795371)\n",
      "8760: accuracy:(0.963189, 0.9631919) loss: 1.1097555 (lr:0.00013757608)\n",
      "8780: accuracy:(0.9631919, 0.9631937) loss: 3.9120321 (lr:0.00013720218)\n",
      "8800: accuracy:(0.9631937, 0.96319556) loss: 11.097197 (lr:0.00013683202)\n",
      "8800: ********* epoch 15 ********* test accuracy:(0.96319556, 0.96335083) test loss: 9.992166\n",
      "8820: accuracy:(0.96335083, 0.96335477) loss: 0.71786875 (lr:0.00013646553)\n",
      "8840: accuracy:(0.96335477, 0.96335864) loss: 0.60177684 (lr:0.0001361027)\n",
      "8860: accuracy:(0.96335864, 0.9633615) loss: 2.5412478 (lr:0.00013574347)\n",
      "8880: accuracy:(0.9633615, 0.96336436) loss: 2.0300057 (lr:0.00013538782)\n",
      "8900: accuracy:(0.96336436, 0.9633672) loss: 1.9869384 (lr:0.00013503569)\n",
      "8900: ********* epoch 15 ********* test accuracy:(0.9633672, 0.96352214) test loss: 10.044545\n",
      "8920: accuracy:(0.96352214, 0.963526) loss: 0.7614573 (lr:0.00013468708)\n",
      "8940: accuracy:(0.963526, 0.9635288) loss: 3.4067984 (lr:0.00013434194)\n",
      "8960: accuracy:(0.9635288, 0.9635316) loss: 3.0607238 (lr:0.00013400023)\n",
      "8980: accuracy:(0.9635316, 0.96353227) loss: 14.668711 (lr:0.00013366193)\n",
      "9000: accuracy:(0.96353227, 0.963534) loss: 3.6460485 (lr:0.00013332699)\n",
      "9000: ********* epoch 16 ********* test accuracy:(0.963534, 0.9636886) test loss: 9.808685\n",
      "9020: accuracy:(0.9636886, 0.9636924) loss: 0.2862729 (lr:0.00013299537)\n",
      "9040: accuracy:(0.9636924, 0.96369624) loss: 0.45769528 (lr:0.00013266708)\n",
      "9060: accuracy:(0.96369624, 0.9637) loss: 0.34300253 (lr:0.00013234201)\n",
      "9080: accuracy:(0.9637, 0.9637028) loss: 3.0799725 (lr:0.00013202021)\n",
      "9100: accuracy:(0.9637028, 0.96370554) loss: 3.1272109 (lr:0.00013170161)\n",
      "9100: ********* epoch 16 ********* test accuracy:(0.96370554, 0.9638525) test loss: 9.927677\n",
      "9120: accuracy:(0.9638525, 0.9638552) loss: 1.7320868 (lr:0.00013138616)\n",
      "9140: accuracy:(0.9638552, 0.96385795) loss: 1.518176 (lr:0.00013107387)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9160: accuracy:(0.96385795, 0.96386063) loss: 2.5289493 (lr:0.00013076467)\n",
      "9180: accuracy:(0.96386063, 0.9638644) loss: 0.46597832 (lr:0.00013045857)\n",
      "9200: accuracy:(0.9638644, 0.96386814) loss: 0.38888627 (lr:0.0001301555)\n",
      "9200: ********* epoch 16 ********* test accuracy:(0.96386814, 0.96401906) test loss: 9.793943\n",
      "9220: accuracy:(0.96401906, 0.96402276) loss: 0.63825536 (lr:0.00012985546)\n",
      "9240: accuracy:(0.96402276, 0.9640254) loss: 5.481367 (lr:0.00012955838)\n",
      "9260: accuracy:(0.9640254, 0.96402806) loss: 2.4253297 (lr:0.00012926427)\n",
      "9280: accuracy:(0.96402806, 0.96403074) loss: 2.0686178 (lr:0.00012897309)\n",
      "9300: accuracy:(0.96403074, 0.9640344) loss: 0.64344764 (lr:0.0001286848)\n",
      "9300: ********* epoch 16 ********* test accuracy:(0.9640344, 0.96417594) test loss: 10.051674\n",
      "9320: accuracy:(0.96417594, 0.9641796) loss: 0.55547744 (lr:0.00012839938)\n",
      "9340: accuracy:(0.9641796, 0.9641832) loss: 0.56903267 (lr:0.00012811681)\n",
      "9360: accuracy:(0.9641832, 0.96418583) loss: 10.489156 (lr:0.00012783703)\n",
      "9380: accuracy:(0.96418583, 0.96418744) loss: 4.8257585 (lr:0.00012756005)\n",
      "9400: accuracy:(0.96418744, 0.9641911) loss: 1.0137757 (lr:0.00012728582)\n",
      "9400: ********* epoch 16 ********* test accuracy:(0.9641911, 0.96433055) test loss: 9.991765\n",
      "9420: accuracy:(0.96433055, 0.9643331) loss: 9.581256 (lr:0.00012701433)\n",
      "9440: accuracy:(0.9643331, 0.9643347) loss: 6.8753185 (lr:0.00012674552)\n",
      "9460: accuracy:(0.9643347, 0.9643363) loss: 6.323859 (lr:0.00012647941)\n",
      "9480: accuracy:(0.9643363, 0.96433884) loss: 4.07597 (lr:0.00012621593)\n",
      "9500: accuracy:(0.96433884, 0.9643414) loss: 2.1542246 (lr:0.00012595509)\n",
      "9500: ********* epoch 16 ********* test accuracy:(0.9643414, 0.96448094) test loss: 10.032411\n",
      "9520: accuracy:(0.96448094, 0.96448445) loss: 3.0262897 (lr:0.00012569682)\n",
      "9540: accuracy:(0.96448445, 0.96448797) loss: 1.715336 (lr:0.00012544113)\n",
      "9560: accuracy:(0.96448797, 0.9644895) loss: 7.3084464 (lr:0.00012518799)\n",
      "9580: accuracy:(0.9644895, 0.96449304) loss: 0.18324766 (lr:0.00012493736)\n",
      "9600: accuracy:(0.96449304, 0.96449655) loss: 0.25374886 (lr:0.00012468924)\n",
      "9600: ********* epoch 17 ********* test accuracy:(0.96449655, 0.964639) test loss: 9.942983\n",
      "9620: accuracy:(0.964639, 0.9646425) loss: 0.36258927 (lr:0.00012444357)\n",
      "9640: accuracy:(0.9646425, 0.964646) loss: 0.5288005 (lr:0.00012420036)\n",
      "9660: accuracy:(0.964646, 0.96464944) loss: 0.36555576 (lr:0.00012395956)\n",
      "9680: accuracy:(0.96464944, 0.96465194) loss: 3.0703757 (lr:0.00012372115)\n",
      "9700: accuracy:(0.96465194, 0.96465445) loss: 6.1830955 (lr:0.00012348512)\n",
      "9700: ********* epoch 17 ********* test accuracy:(0.96465445, 0.9647929) test loss: 9.945456\n",
      "9720: accuracy:(0.9647929, 0.96479535) loss: 1.6695298 (lr:0.00012325145)\n",
      "9740: accuracy:(0.96479535, 0.9647978) loss: 2.6739597 (lr:0.00012302009)\n",
      "9760: accuracy:(0.9647978, 0.96480125) loss: 0.077628314 (lr:0.00012279104)\n",
      "9780: accuracy:(0.96480125, 0.96480465) loss: 1.0797101 (lr:0.00012256426)\n",
      "9800: accuracy:(0.96480465, 0.9648071) loss: 2.9148054 (lr:0.00012233976)\n",
      "9800: ********* epoch 17 ********* test accuracy:(0.9648071, 0.9649456) test loss: 9.859761\n",
      "9820: accuracy:(0.9649456, 0.964949) loss: 2.264419 (lr:0.00012211746)\n",
      "9840: accuracy:(0.964949, 0.96495235) loss: 0.12472599 (lr:0.00012189739)\n",
      "9860: accuracy:(0.96495235, 0.96495384) loss: 4.4700265 (lr:0.000121679506)\n",
      "9880: accuracy:(0.96495384, 0.9649553) loss: 4.7774 (lr:0.000121463796)\n",
      "9900: accuracy:(0.9649553, 0.9649567) loss: 4.4648695 (lr:0.000121250225)\n",
      "9900: ********* epoch 17 ********* test accuracy:(0.9649567, 0.96508765) test loss: 9.894467\n",
      "9920: accuracy:(0.96508765, 0.965091) loss: 0.04837087 (lr:0.000121038785)\n",
      "9940: accuracy:(0.965091, 0.9650943) loss: 0.22806576 (lr:0.00012082944)\n",
      "9960: accuracy:(0.9650943, 0.9650976) loss: 0.79673946 (lr:0.000120622186)\n",
      "9980: accuracy:(0.9650976, 0.96510094) loss: 0.7157033 (lr:0.00012041699)\n",
      "10000: accuracy:(0.96510094, 0.9651033) loss: 3.1337495 (lr:0.00012021384)\n",
      "10000: ********* epoch 17 ********* test accuracy:(0.9651033, 0.9652297) test loss: 9.79174\n",
      "10020: accuracy:(0.9652297, 0.96523297) loss: 0.18678352 (lr:0.0001200127)\n",
      "10040: accuracy:(0.96523297, 0.96523434) loss: 4.0095825 (lr:0.00011981357)\n",
      "10060: accuracy:(0.96523434, 0.9652367) loss: 2.6324375 (lr:0.00011961643)\n",
      "10080: accuracy:(0.9652367, 0.96523905) loss: 2.011889 (lr:0.00011942124)\n",
      "10100: accuracy:(0.96523905, 0.9652414) loss: 8.535306 (lr:0.000119227996)\n",
      "10100: ********* epoch 17 ********* test accuracy:(0.9652414, 0.9653717) test loss: 9.805656\n",
      "10120: accuracy:(0.9653717, 0.965375) loss: 0.5757604 (lr:0.00011903667)\n",
      "10140: accuracy:(0.965375, 0.9653782) loss: 0.35824212 (lr:0.00011884726)\n",
      "10160: accuracy:(0.9653782, 0.96538144) loss: 0.59263206 (lr:0.00011865972)\n",
      "10180: accuracy:(0.96538144, 0.9653828) loss: 8.393237 (lr:0.000118474054)\n",
      "10200: accuracy:(0.9653828, 0.96538603) loss: 1.1110538 (lr:0.000118290234)\n",
      "10200: ********* epoch 18 ********* test accuracy:(0.96538603, 0.965511) test loss: 9.823548\n",
      "10220: accuracy:(0.965511, 0.96551424) loss: 0.1624506 (lr:0.00011810825)\n",
      "10240: accuracy:(0.96551424, 0.9655165) loss: 8.9098015 (lr:0.00011792806)\n",
      "10260: accuracy:(0.9655165, 0.9655197) loss: 0.17539617 (lr:0.00011774968)\n",
      "10280: accuracy:(0.9655197, 0.9655229) loss: 0.7342328 (lr:0.000117573065)\n",
      "10300: accuracy:(0.9655229, 0.96552604) loss: 0.74896216 (lr:0.00011739822)\n",
      "10300: ********* epoch 18 ********* test accuracy:(0.96552604, 0.9656449) test loss: 9.983762\n",
      "10320: accuracy:(0.9656449, 0.96564806) loss: 0.03793031 (lr:0.00011722509)\n",
      "10340: accuracy:(0.96564806, 0.9656512) loss: 0.99696404 (lr:0.00011705371)\n",
      "10360: accuracy:(0.9656512, 0.9656544) loss: 0.7685125 (lr:0.00011688401)\n",
      "10380: accuracy:(0.9656544, 0.96565753) loss: 0.19917396 (lr:0.00011671602)\n",
      "10400: accuracy:(0.96565753, 0.96566063) loss: 0.5423316 (lr:0.00011654969)\n",
      "10400: ********* epoch 18 ********* test accuracy:(0.96566063, 0.9657799) test loss: 10.03621\n",
      "10420: accuracy:(0.9657799, 0.96578026) loss: 5.2218194 (lr:0.00011638502)\n",
      "10440: accuracy:(0.96578026, 0.96578336) loss: 0.07471074 (lr:0.000116221985)\n",
      "10460: accuracy:(0.96578336, 0.96578467) loss: 11.602871 (lr:0.000116060575)\n",
      "10480: accuracy:(0.96578467, 0.96578777) loss: 0.6149345 (lr:0.000115900766)\n",
      "10500: accuracy:(0.96578777, 0.96579087) loss: 0.8733399 (lr:0.00011574254)\n",
      "10500: ********* epoch 18 ********* test accuracy:(0.96579087, 0.9659051) test loss: 10.157896\n",
      "10520: accuracy:(0.9659051, 0.9659064) loss: 5.8834324 (lr:0.000115585906)\n",
      "10540: accuracy:(0.9659064, 0.9659085) loss: 2.8554106 (lr:0.00011543083)\n",
      "10560: accuracy:(0.9659085, 0.9659098) loss: 2.271074 (lr:0.00011527729)\n",
      "10580: accuracy:(0.9659098, 0.9659119) loss: 1.8118299 (lr:0.00011512527)\n",
      "10600: accuracy:(0.9659119, 0.96591413) loss: 2.6316798 (lr:0.00011497478)\n",
      "10600: ********* epoch 18 ********* test accuracy:(0.96591413, 0.96602887) test loss: 10.067734\n",
      "10620: accuracy:(0.96602887, 0.9660301) loss: 3.7184093 (lr:0.00011482577)\n",
      "10640: accuracy:(0.9660301, 0.9660331) loss: 1.549085 (lr:0.00011467826)\n",
      "10660: accuracy:(0.9660331, 0.96603525) loss: 4.905877 (lr:0.0001145322)\n",
      "10680: accuracy:(0.96603525, 0.9660383) loss: 0.88203746 (lr:0.00011438761)\n",
      "10700: accuracy:(0.9660383, 0.96604127) loss: 0.877465 (lr:0.000114244445)\n",
      "10700: ********* epoch 18 ********* test accuracy:(0.96604127, 0.9661565) test loss: 9.781167\n",
      "10720: accuracy:(0.9661565, 0.96615857) loss: 9.485268 (lr:0.00011410272)\n",
      "10740: accuracy:(0.96615857, 0.9661598) loss: 5.403405 (lr:0.00011396239)\n",
      "10760: accuracy:(0.9661598, 0.9661628) loss: 0.22426939 (lr:0.000113823466)\n",
      "10780: accuracy:(0.9661628, 0.9661649) loss: 3.275841 (lr:0.000113685914)\n",
      "10800: accuracy:(0.9661649, 0.96616787) loss: 0.8540465 (lr:0.000113549744)\n",
      "10800: ********* epoch 19 ********* test accuracy:(0.96616787, 0.96628267) test loss: 9.6740675\n",
      "10820: accuracy:(0.96628267, 0.9662856) loss: 0.35402298 (lr:0.00011341491)\n",
      "10840: accuracy:(0.9662856, 0.96628684) loss: 4.5146465 (lr:0.000113281436)\n",
      "10860: accuracy:(0.96628684, 0.96628886) loss: 2.4809506 (lr:0.00011314928)\n",
      "10880: accuracy:(0.96628886, 0.96629095) loss: 1.8846734 (lr:0.00011301845)\n",
      "10900: accuracy:(0.96629095, 0.96629393) loss: 0.26849407 (lr:0.00011288891)\n",
      "10900: ********* epoch 19 ********* test accuracy:(0.96629393, 0.9664048) test loss: 9.662203\n",
      "10920: accuracy:(0.9664048, 0.966406) loss: 8.869442 (lr:0.00011276067)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10940: accuracy:(0.966406, 0.966408) loss: 2.629033 (lr:0.000112633694)\n",
      "10960: accuracy:(0.966408, 0.96641093) loss: 0.11405177 (lr:0.00011250798)\n",
      "10980: accuracy:(0.96641093, 0.96641386) loss: 0.38176182 (lr:0.00011238353)\n",
      "11000: accuracy:(0.96641386, 0.9664168) loss: 0.3649497 (lr:0.000112260306)\n",
      "11000: ********* epoch 19 ********* test accuracy:(0.9664168, 0.96652645) test loss: 9.84769\n",
      "11020: accuracy:(0.96652645, 0.96652937) loss: 0.43446916 (lr:0.00011213832)\n",
      "11040: accuracy:(0.96652937, 0.96653223) loss: 0.44089413 (lr:0.00011201754)\n",
      "11060: accuracy:(0.96653223, 0.96653426) loss: 3.5246785 (lr:0.00011189796)\n",
      "11080: accuracy:(0.96653426, 0.9665354) loss: 6.7614026 (lr:0.00011177958)\n",
      "11100: accuracy:(0.9665354, 0.9665374) loss: 2.3528087 (lr:0.00011166237)\n",
      "11100: ********* epoch 19 ********* test accuracy:(0.9665374, 0.96664) test loss: 9.80851\n",
      "11120: accuracy:(0.96664, 0.9666411) loss: 6.208888 (lr:0.000111546324)\n",
      "11140: accuracy:(0.9666411, 0.96664226) loss: 6.6372066 (lr:0.00011143144)\n",
      "11160: accuracy:(0.96664226, 0.9666434) loss: 7.0876546 (lr:0.00011131769)\n",
      "11180: accuracy:(0.9666434, 0.9666446) loss: 9.515758 (lr:0.00011120508)\n",
      "11200: accuracy:(0.9666446, 0.96664655) loss: 1.6096148 (lr:0.000111093585)\n",
      "11200: ********* epoch 19 ********* test accuracy:(0.96664655, 0.9667482) test loss: 9.800703\n",
      "11220: accuracy:(0.9667482, 0.9667493) loss: 14.000721 (lr:0.00011098321)\n",
      "11240: accuracy:(0.9667493, 0.9667521) loss: 1.0347407 (lr:0.000110873916)\n",
      "11260: accuracy:(0.9667521, 0.9667549) loss: 0.43020064 (lr:0.00011076572)\n",
      "11280: accuracy:(0.9667549, 0.9667568) loss: 2.118335 (lr:0.0001106586)\n",
      "11300: accuracy:(0.9667568, 0.9667588) loss: 3.8816516 (lr:0.000110552544)\n",
      "11300: ********* epoch 19 ********* test accuracy:(0.9667588, 0.9668628) test loss: 9.89389\n",
      "11320: accuracy:(0.9668628, 0.96686554) loss: 0.26781636 (lr:0.000110447545)\n",
      "11340: accuracy:(0.96686554, 0.96686834) loss: 0.17661892 (lr:0.00011034359)\n",
      "11360: accuracy:(0.96686834, 0.9668711) loss: 0.7960017 (lr:0.00011024067)\n",
      "11380: accuracy:(0.9668711, 0.966873) loss: 5.6500106 (lr:0.000110138775)\n",
      "11400: accuracy:(0.966873, 0.96687496) loss: 1.1284909 (lr:0.000110037894)\n",
      "11400: ********* epoch 20 ********* test accuracy:(0.96687496, 0.9669729) test loss: 9.943049\n",
      "11420: accuracy:(0.9669729, 0.9669756) loss: 0.9396614 (lr:0.00010993802)\n",
      "11440: accuracy:(0.9669756, 0.9669784) loss: 0.52347255 (lr:0.00010983913)\n",
      "11460: accuracy:(0.9669784, 0.9669811) loss: 1.3916115 (lr:0.000109741224)\n",
      "11480: accuracy:(0.9669811, 0.96698385) loss: 0.14140952 (lr:0.0001096443)\n",
      "11500: accuracy:(0.96698385, 0.9669866) loss: 1.5346692 (lr:0.00010954834)\n",
      "11500: ********* epoch 20 ********* test accuracy:(0.9669866, 0.9670811) test loss: 9.996905\n",
      "11520: accuracy:(0.9670811, 0.96708304) loss: 3.7840452 (lr:0.00010945333)\n",
      "11540: accuracy:(0.96708304, 0.9670849) loss: 1.8305202 (lr:0.000109359265)\n",
      "11560: accuracy:(0.9670849, 0.96708596) loss: 4.935928 (lr:0.00010926614)\n",
      "11580: accuracy:(0.96708596, 0.96708786) loss: 10.7603245 (lr:0.00010917394)\n",
      "11600: accuracy:(0.96708786, 0.96708894) loss: 3.378354 (lr:0.00010908266)\n",
      "11600: ********* epoch 20 ********* test accuracy:(0.96708894, 0.96717936) test loss: 10.035369\n",
      "11620: accuracy:(0.96717936, 0.96718204) loss: 0.678986 (lr:0.00010899229)\n",
      "11640: accuracy:(0.96718204, 0.9671847) loss: 0.74148107 (lr:0.000108902816)\n",
      "11660: accuracy:(0.9671847, 0.9671874) loss: 0.5811807 (lr:0.00010881422)\n",
      "11680: accuracy:(0.9671874, 0.96718925) loss: 3.622007 (lr:0.00010872653)\n",
      "11700: accuracy:(0.96718925, 0.96719193) loss: 0.6785697 (lr:0.000108639695)\n",
      "11700: ********* epoch 20 ********* test accuracy:(0.96719193, 0.9672792) test loss: 9.992905\n",
      "11720: accuracy:(0.9672792, 0.96728104) loss: 1.922285 (lr:0.00010855373)\n",
      "11740: accuracy:(0.96728104, 0.96728283) loss: 2.1189694 (lr:0.000108468616)\n",
      "11760: accuracy:(0.96728283, 0.9672855) loss: 3.1480954 (lr:0.00010838435)\n",
      "11780: accuracy:(0.9672855, 0.96728814) loss: 0.1383597 (lr:0.00010830093)\n",
      "11800: accuracy:(0.96728814, 0.96728915) loss: 3.6224775 (lr:0.00010821833)\n",
      "11800: ********* epoch 20 ********* test accuracy:(0.96728915, 0.9673789) test loss: 9.972403\n",
      "11820: accuracy:(0.9673789, 0.9673807) loss: 1.0961453 (lr:0.000108136555)\n",
      "11840: accuracy:(0.9673807, 0.9673817) loss: 4.93805 (lr:0.000108055596)\n",
      "11860: accuracy:(0.9673817, 0.9673827) loss: 3.0257962 (lr:0.000107975444)\n",
      "11880: accuracy:(0.9673827, 0.9673846) loss: 1.7649254 (lr:0.000107896085)\n",
      "11900: accuracy:(0.9673846, 0.96738714) loss: 0.5816188 (lr:0.00010781752)\n",
      "11900: ********* epoch 20 ********* test accuracy:(0.96738714, 0.9674738) test loss: 9.962761\n",
      "11920: accuracy:(0.9674738, 0.967474) loss: 7.8832397 (lr:0.00010773973)\n",
      "11940: accuracy:(0.967474, 0.9674766) loss: 0.1333218 (lr:0.00010766272)\n",
      "11960: accuracy:(0.9674766, 0.96747917) loss: 0.371789 (lr:0.00010758647)\n",
      "11980: accuracy:(0.96747917, 0.96748173) loss: 0.570434 (lr:0.00010751099)\n",
      "12000: accuracy:(0.96748173, 0.96748275) loss: 8.441319 (lr:0.00010743625)\n",
      "12000: ********* epoch 21 ********* test accuracy:(0.96748275, 0.9675679) test loss: 9.914977\n"
     ]
    }
   ],
   "source": [
    "def training_step(i, batch_x, batch_y, x_test, y_test, update_test_data, update_train_data, x_train):\n",
    "\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c, l, p, r = sess.run([accuracy, cross_entropy, lr, precision, recall], feed_dict={X: batch_X, Y: batch_Y, pkeep: 1.0, step: i})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(l) + \")\")\n",
    "        dnn_training_acc.append(a)\n",
    "        dnn_learn_rate.append(l)\n",
    "        dnn_train_cross_ent.append(c)\n",
    "        dnn_train_prec.append(p)\n",
    "        dnn_train_rec.append(r)\n",
    "        \n",
    "    # compute test values for visualisation\n",
    "    if update_test_data:\n",
    "        a, c, p, r = sess.run([accuracy, cross_entropy, precision, recall], feed_dict={X: x_test, Y: y_test, pkeep: 1.0})\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//x_train.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        dnn_testing_acc.append(a)\n",
    "        dnn_test_cross_ent.append(c)\n",
    "        dnn_test_prec.append(p)\n",
    "        dnn_test_rec.append(r)\n",
    "        \n",
    "\n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, {X: batch_X, Y: batch_Y, pkeep: 0.7, step: i}) \n",
    "\n",
    "epochs = 20\n",
    "iterations = epochs * 600 # an epoch will complete every 600 iterations\n",
    "\n",
    "for i in range(iterations+1): \n",
    "    if (i % 600 == 0):\n",
    "        k = 0\n",
    "        shf = np.random.choice(x_train.shape[0], 60000, replace=False)\n",
    "        n = 100\n",
    "        idx_batches = [shf[j:j + n] for j in range(0, len(shf), n)] \n",
    "    batch_X, batch_Y = x_train[idx_batches[k]], y_train[idx_batches[k]]\n",
    "    k += 1\n",
    "    training_step(i, batch_X, batch_Y, x_test, y_test, i % 100 == 0, i % 20 == 0, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set tf placeholders for X and Y data\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1]) # placerholder, images are 28,28,1, None refers to batch size, will be filled during training\n",
    "Y = tf.placeholder(tf.float32, [None, 10]) # placeholder, output is a 1 hot vector for the 10 possible classes\n",
    "step = tf.placeholder(tf.float32) # place holder for the learning rate function\n",
    "#pkeep = tf.placeholder(tf.float32) # placeholder, dropout of neurons \n",
    "\n",
    "L1 = 4  # convolutional layer output depth\n",
    "L2 = 8 \n",
    "L3 = 12 \n",
    "L4 = 200  # fully connected layer\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, L1], stddev=0.1))  # 5x5 filter, L1 output channels\n",
    "B1 = tf.Variable(tf.ones([L1])/10)\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, L1, L2], stddev=0.1))\n",
    "B2 = tf.Variable(tf.ones([L2])/10)\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, L2, L3], stddev=0.1))\n",
    "B3 = tf.Variable(tf.ones([L3])/10)\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7 * 7 * L3, L4], stddev=0.1))\n",
    "B4 = tf.Variable(tf.ones([L4])/10)\n",
    "W5 = tf.Variable(tf.truncated_normal([L4, 10], stddev=0.1))\n",
    "B5 = tf.Variable(tf.ones([10])/10)\n",
    "\n",
    "# The model\n",
    "stride = 1 \n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "stride = 2 \n",
    "Y2 = tf.nn.relu(tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "stride = 2 \n",
    "Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME') + B3)\n",
    "\n",
    "# reshape for fully connected\n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * L3])\n",
    "\n",
    "Y4 = tf.nn.relu(tf.matmul(YY, W4) + B4)\n",
    "YLogits = tf.matmul(Y4, W5) + B5\n",
    "YPred = tf.nn.softmax(YLogits)\n",
    "\n",
    "# loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=YLogits, labels = Y) # use tf cross entropy loss, deals with log(0) problems\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100 # get loss in percentage\n",
    "\n",
    "# learning rate, utlizing an exponential decay function\n",
    "lr = 0.0001 + tf.train.exponential_decay(0.003, step, 2000, 1/math.e)\n",
    "optimizer = tf.train.AdamOptimizer(lr) # variable learning rate\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# measures\n",
    "#is_correct = tf.equal(tf.argmax(Y,1), tf.argmax(YPred,1))\n",
    "accuracy = tf.metrics.accuracy(tf.argmax(Y,1), tf.argmax(YPred,1))\n",
    "precision = tf.metrics.precision(tf.argmax(Y,1), tf.argmax(YPred,1))\n",
    "recall = tf.metrics.recall(tf.argmax(Y,1), tf.argmax(YPred,1))\n",
    "\n",
    "# initialize session\n",
    "init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "# init = tf.global_variables_initializer()\n",
    "# sess = tf.Session() # begin and initialize tf session\n",
    "# sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_acc = []\n",
    "testing_acc = []\n",
    "train_prec = []\n",
    "train_rec = []\n",
    "test_prec = []\n",
    "test_rec = []\n",
    "learn_rate = []\n",
    "train_cross_ent = []\n",
    "test_cross_ent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: accuracy:(0.0, 0.08) loss: 236.42603 (lr:0.0031)\n",
      "0: ********* epoch 1 ********* test accuracy:(0.08, 0.097524755) test loss: 234.96297\n",
      "20: accuracy:(0.097524755, 0.1032353) loss: 92.31827 (lr:0.0030701496)\n",
      "40: accuracy:(0.1032353, 0.110291265) loss: 52.38128 (lr:0.0030405961)\n",
      "60: accuracy:(0.110291265, 0.11740384) loss: 39.76779 (lr:0.0030113366)\n",
      "80: accuracy:(0.11740384, 0.12504762) loss: 24.894522 (lr:0.0029823685)\n",
      "100: accuracy:(0.12504762, 0.13254717) loss: 29.266777 (lr:0.0029536884)\n",
      "100: ********* epoch 1 ********* test accuracy:(0.13254717, 0.52470875) test loss: 19.884758\n",
      "120: accuracy:(0.52470875, 0.526715) loss: 21.322514 (lr:0.0029252938)\n",
      "140: accuracy:(0.526715, 0.5285577) loss: 26.8513 (lr:0.0028971815)\n",
      "160: accuracy:(0.5285577, 0.530622) loss: 23.427244 (lr:0.002869349)\n",
      "180: accuracy:(0.530622, 0.5326667) loss: 14.314585 (lr:0.0028417937)\n",
      "200: accuracy:(0.5326667, 0.5347393) loss: 7.350736 (lr:0.0028145125)\n",
      "200: ********* epoch 1 ********* test accuracy:(0.5347393, 0.67173636) test loss: 11.721484\n",
      "220: accuracy:(0.67173636, 0.67259616) loss: 24.015213 (lr:0.0027875025)\n",
      "240: accuracy:(0.67259616, 0.6734185) loss: 19.902187 (lr:0.0027607614)\n",
      "260: accuracy:(0.6734185, 0.6743312) loss: 12.470659 (lr:0.0027342865)\n",
      "280: accuracy:(0.6743312, 0.6751746) loss: 17.961748 (lr:0.0027080749)\n",
      "300: accuracy:(0.6751746, 0.67613924) loss: 4.670962 (lr:0.002682124)\n",
      "300: ********* epoch 1 ********* test accuracy:(0.67613924, 0.7471875) test loss: 9.06515\n",
      "320: accuracy:(0.7471875, 0.74769783) loss: 13.4930725 (lr:0.0026564314)\n",
      "340: accuracy:(0.74769783, 0.74813396) loss: 15.490296 (lr:0.0026309947)\n",
      "360: accuracy:(0.74813396, 0.74861574) loss: 18.843346 (lr:0.0026058108)\n",
      "380: accuracy:(0.74861574, 0.7490714) loss: 9.809704 (lr:0.0025808774)\n",
      "400: accuracy:(0.7490714, 0.7496437) loss: 4.4395394 (lr:0.0025561925)\n",
      "400: ********* epoch 1 ********* test accuracy:(0.7496437, 0.79170823) test loss: 9.335381\n",
      "420: accuracy:(0.79170823, 0.7920115) loss: 16.25879 (lr:0.002531753)\n",
      "440: accuracy:(0.7920115, 0.79235184) loss: 5.6962585 (lr:0.0025075565)\n",
      "460: accuracy:(0.79235184, 0.79267174) loss: 7.628131 (lr:0.002483601)\n",
      "480: accuracy:(0.79267174, 0.7930095) loss: 11.32747 (lr:0.0024598835)\n",
      "500: accuracy:(0.7930095, 0.793346) loss: 12.288197 (lr:0.0024364025)\n",
      "500: ********* epoch 1 ********* test accuracy:(0.793346, 0.8225399) test loss: 7.755474\n",
      "520: accuracy:(0.8225399, 0.82274324) loss: 20.771097 (lr:0.0024131548)\n",
      "540: accuracy:(0.82274324, 0.8229777) loss: 7.2441964 (lr:0.0023901386)\n",
      "560: accuracy:(0.8229777, 0.8231956) loss: 12.458098 (lr:0.0023673512)\n",
      "580: accuracy:(0.8231956, 0.8234444) loss: 8.084571 (lr:0.0023447906)\n",
      "600: accuracy:(0.8234444, 0.8237084) loss: 2.6436682 (lr:0.0023224547)\n",
      "600: ********* epoch 2 ********* test accuracy:(0.8237084, 0.8450889) test loss: 6.4955287\n",
      "620: accuracy:(0.8450889, 0.84525955) loss: 11.83888 (lr:0.002300341)\n",
      "640: accuracy:(0.84525955, 0.845457) loss: 2.0449996 (lr:0.0022784472)\n",
      "660: accuracy:(0.845457, 0.8456676) loss: 1.7889013 (lr:0.0022567713)\n",
      "680: accuracy:(0.8456676, 0.8458231) loss: 7.4536657 (lr:0.002235311)\n",
      "700: accuracy:(0.8458231, 0.84600544) loss: 4.280467 (lr:0.0022140644)\n",
      "700: ********* epoch 2 ********* test accuracy:(0.84600544, 0.86155504) test loss: 7.1325054\n",
      "720: accuracy:(0.86155504, 0.86170846) loss: 4.578603 (lr:0.002193029)\n",
      "740: accuracy:(0.86170846, 0.86184967) loss: 5.1134367 (lr:0.002172203)\n",
      "760: accuracy:(0.86184967, 0.8620024) loss: 2.24648 (lr:0.0021515843)\n",
      "780: accuracy:(0.8620024, 0.86214286) loss: 5.6616316 (lr:0.0021311706)\n",
      "800: accuracy:(0.86214286, 0.8622949) loss: 3.438451 (lr:0.0021109602)\n",
      "800: ********* epoch 2 ********* test accuracy:(0.8622949, 0.87486714) test loss: 5.642141\n",
      "820: accuracy:(0.87486714, 0.875) loss: 0.89514863 (lr:0.0020909507)\n",
      "840: accuracy:(0.875, 0.87510073) loss: 6.6905117 (lr:0.0020711406)\n",
      "860: accuracy:(0.87510073, 0.87521183) loss: 4.328013 (lr:0.0020515274)\n",
      "880: accuracy:(0.87521183, 0.87531215) loss: 4.725351 (lr:0.0020321093)\n",
      "900: accuracy:(0.87531215, 0.8754334) loss: 3.4887984 (lr:0.0020128845)\n",
      "900: ********* epoch 2 ********* test accuracy:(0.8754334, 0.88564056) test loss: 5.6163116\n",
      "920: accuracy:(0.88564056, 0.8857402) loss: 2.3755598 (lr:0.001993851)\n",
      "940: accuracy:(0.8857402, 0.88583016) loss: 7.413521 (lr:0.001975007)\n",
      "960: accuracy:(0.88583016, 0.8859199) loss: 13.033642 (lr:0.0019563502)\n",
      "980: accuracy:(0.8859199, 0.88601905) loss: 3.1168718 (lr:0.0019378791)\n",
      "1000: accuracy:(0.88601905, 0.886118) loss: 4.3246055 (lr:0.0019195919)\n",
      "1000: ********* epoch 2 ********* test accuracy:(0.886118, 0.8944657) test loss: 5.3558044\n",
      "1020: accuracy:(0.8944657, 0.8945486) loss: 3.3456867 (lr:0.0019014867)\n",
      "1040: accuracy:(0.8945486, 0.8946314) loss: 4.360574 (lr:0.0018835615)\n",
      "1060: accuracy:(0.8946314, 0.89471406) loss: 3.8122318 (lr:0.0018658149)\n",
      "1080: accuracy:(0.89471406, 0.8947879) loss: 4.2336802 (lr:0.0018482447)\n",
      "1100: accuracy:(0.8947879, 0.8948616) loss: 5.9890065 (lr:0.0018308494)\n",
      "1100: ********* epoch 2 ********* test accuracy:(0.8948616, 0.9018073) test loss: 5.40808\n",
      "1120: accuracy:(0.9018073, 0.90186155) loss: 13.102041 (lr:0.0018136272)\n",
      "1140: accuracy:(0.90186155, 0.90193164) loss: 3.7741318 (lr:0.0017965762)\n",
      "1160: accuracy:(0.90193164, 0.9019857) loss: 5.6585007 (lr:0.001779695)\n",
      "1180: accuracy:(0.9019857, 0.90205556) loss: 3.0198905 (lr:0.0017629818)\n",
      "1200: accuracy:(0.90205556, 0.9021174) loss: 3.7549484 (lr:0.0017464348)\n",
      "1200: ********* epoch 3 ********* test accuracy:(0.9021174, 0.90792066) test loss: 5.246624\n",
      "1220: accuracy:(0.90792066, 0.9079736) loss: 4.122278 (lr:0.0017300526)\n",
      "1240: accuracy:(0.9079736, 0.9080337) loss: 1.9886427 (lr:0.0017138333)\n",
      "1260: accuracy:(0.9080337, 0.9081012) loss: 0.56473285 (lr:0.0016977752)\n",
      "1280: accuracy:(0.9081012, 0.9081685) loss: 3.0305696 (lr:0.0016818773)\n",
      "1300: accuracy:(0.9081685, 0.9082284) loss: 2.927503 (lr:0.0016661374)\n",
      "1300: ********* epoch 3 ********* test accuracy:(0.9082284, 0.91341066) test loss: 4.409895\n",
      "1320: accuracy:(0.91341066, 0.913456) loss: 5.3245335 (lr:0.0016505539)\n",
      "1340: accuracy:(0.913456, 0.913515) loss: 0.59805125 (lr:0.0016351256)\n",
      "1360: accuracy:(0.913515, 0.91356707) loss: 2.505116 (lr:0.001619851)\n",
      "1380: accuracy:(0.91356707, 0.91361225) loss: 6.143861 (lr:0.0016047282)\n",
      "1400: accuracy:(0.91361225, 0.91366416) loss: 2.0871785 (lr:0.0015897558)\n",
      "1400: ********* epoch 3 ********* test accuracy:(0.91366416, 0.9182368) test loss: 4.2389717\n",
      "1420: accuracy:(0.9182368, 0.9182888) loss: 2.5584598 (lr:0.0015749325)\n",
      "1440: accuracy:(0.9182888, 0.91832167) loss: 10.038661 (lr:0.0015602567)\n",
      "1460: accuracy:(0.91832167, 0.9183672) loss: 2.9065404 (lr:0.0015457269)\n",
      "1480: accuracy:(0.9183672, 0.9184127) loss: 1.5383008 (lr:0.0015313418)\n",
      "1500: accuracy:(0.9184127, 0.9184518) loss: 2.4028215 (lr:0.0015170996)\n",
      "1500: ********* epoch 3 ********* test accuracy:(0.9184518, 0.9224224) test loss: 4.373109\n",
      "1520: accuracy:(0.9224224, 0.9224687) loss: 0.60259926 (lr:0.0015029992)\n",
      "1540: accuracy:(0.9224687, 0.922503) loss: 3.4853678 (lr:0.0014890392)\n",
      "1560: accuracy:(0.922503, 0.9225372) loss: 8.4835 (lr:0.001475218)\n",
      "1580: accuracy:(0.9225372, 0.92258334) loss: 0.7483915 (lr:0.0014615343)\n",
      "1600: accuracy:(0.92258334, 0.9226294) loss: 0.59260094 (lr:0.0014479868)\n",
      "1600: ********* epoch 3 ********* test accuracy:(0.9226294, 0.9260977) test loss: 4.8345513\n",
      "1620: accuracy:(0.9260977, 0.9261392) loss: 0.922052 (lr:0.0014345741)\n",
      "1640: accuracy:(0.9261392, 0.926175) loss: 1.8996444 (lr:0.0014212949)\n",
      "1660: accuracy:(0.926175, 0.92620516) loss: 4.2200723 (lr:0.0014081479)\n",
      "1680: accuracy:(0.92620516, 0.9262465) loss: 1.6037713 (lr:0.0013951315)\n",
      "1700: accuracy:(0.9262465, 0.9262766) loss: 3.04521 (lr:0.0013822448)\n",
      "1700: ********* epoch 3 ********* test accuracy:(0.9262766, 0.9294804) test loss: 4.006986\n",
      "1720: accuracy:(0.9294804, 0.92950714) loss: 6.545949 (lr:0.0013694862)\n",
      "1740: accuracy:(0.92950714, 0.9295392) loss: 1.7859491 (lr:0.0013568546)\n",
      "1760: accuracy:(0.9295392, 0.9295712) loss: 2.67804 (lr:0.0013443487)\n",
      "1780: accuracy:(0.9295712, 0.92959785) loss: 6.0821953 (lr:0.0013319672)\n",
      "1800: accuracy:(0.92959785, 0.9296298) loss: 2.3396027 (lr:0.0013197089)\n",
      "1800: ********* epoch 4 ********* test accuracy:(0.9296298, 0.93248117) test loss: 3.895218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1820: accuracy:(0.93248117, 0.93251) loss: 1.1125699 (lr:0.0013075727)\n",
      "1840: accuracy:(0.93251, 0.93254393) loss: 0.6728476 (lr:0.0012955571)\n",
      "1860: accuracy:(0.93254393, 0.9325777) loss: 1.57957 (lr:0.001283661)\n",
      "1880: accuracy:(0.9325777, 0.9326115) loss: 0.6576721 (lr:0.0012718835)\n",
      "1900: accuracy:(0.9326115, 0.93264025) loss: 3.8997517 (lr:0.001260223)\n",
      "1900: ********* epoch 4 ********* test accuracy:(0.93264025, 0.9351479) test loss: 4.2383137\n",
      "1920: accuracy:(0.9351479, 0.93517405) loss: 3.976879 (lr:0.0012486785)\n",
      "1940: accuracy:(0.93517405, 0.9352002) loss: 4.423499 (lr:0.0012372491)\n",
      "1960: accuracy:(0.9352002, 0.93522155) loss: 2.5064974 (lr:0.0012259333)\n",
      "1980: accuracy:(0.93522155, 0.9352476) loss: 2.4214478 (lr:0.00121473)\n",
      "2000: accuracy:(0.9352476, 0.9352784) loss: 0.36795628 (lr:0.0012036383)\n",
      "2000: ********* epoch 4 ********* test accuracy:(0.9352784, 0.93763745) test loss: 4.0298743\n",
      "2020: accuracy:(0.93763745, 0.93766123) loss: 1.3690547 (lr:0.0011926569)\n",
      "2040: accuracy:(0.93766123, 0.93768954) loss: 0.5643698 (lr:0.0011817847)\n",
      "2060: accuracy:(0.93768954, 0.9377178) loss: 0.2578338 (lr:0.0011710208)\n",
      "2080: accuracy:(0.9377178, 0.9377415) loss: 1.7735479 (lr:0.0011603639)\n",
      "2100: accuracy:(0.9377415, 0.9377697) loss: 0.43330762 (lr:0.0011498132)\n",
      "2100: ********* epoch 4 ********* test accuracy:(0.9377697, 0.9398872) test loss: 3.904281\n",
      "2120: accuracy:(0.9398872, 0.939909) loss: 4.0696354 (lr:0.0011393673)\n",
      "2140: accuracy:(0.939909, 0.939935) loss: 0.12508276 (lr:0.0011290255)\n",
      "2160: accuracy:(0.939935, 0.939961) loss: 0.41669825 (lr:0.0011187865)\n",
      "2180: accuracy:(0.939961, 0.93997836) loss: 4.4109774 (lr:0.0011086494)\n",
      "2200: accuracy:(0.93997836, 0.94) loss: 1.8842691 (lr:0.0010986131)\n",
      "2200: ********* epoch 4 ********* test accuracy:(0.94, 0.94191206) test loss: 4.0997086\n",
      "2220: accuracy:(0.94191206, 0.94193614) loss: 2.0562751 (lr:0.0010886769)\n",
      "2240: accuracy:(0.94193614, 0.9419602) loss: 0.36336938 (lr:0.0010788393)\n",
      "2260: accuracy:(0.9419602, 0.94198424) loss: 0.5195557 (lr:0.0010690998)\n",
      "2280: accuracy:(0.94198424, 0.94200414) loss: 3.5711472 (lr:0.001059457)\n",
      "2300: accuracy:(0.94200414, 0.9420199) loss: 5.8231273 (lr:0.0010499102)\n",
      "2300: ********* epoch 4 ********* test accuracy:(0.9420199, 0.94378376) test loss: 3.8940825\n",
      "2320: accuracy:(0.94378376, 0.9437982) loss: 4.0654426 (lr:0.0010404584)\n",
      "2340: accuracy:(0.9437982, 0.94381654) loss: 2.558369 (lr:0.0010311007)\n",
      "2360: accuracy:(0.94381654, 0.94383883) loss: 1.0892965 (lr:0.0010218362)\n",
      "2380: accuracy:(0.94383883, 0.9438532) loss: 8.132328 (lr:0.0010126637)\n",
      "2400: accuracy:(0.9438532, 0.9438715) loss: 1.3681942 (lr:0.0010035826)\n",
      "2400: ********* epoch 5 ********* test accuracy:(0.9438715, 0.94550174) test loss: 3.7323613\n",
      "2420: accuracy:(0.94550174, 0.9455187) loss: 1.2692868 (lr:0.0009945917)\n",
      "2440: accuracy:(0.9455187, 0.9455395) loss: 0.6976911 (lr:0.0009856905)\n",
      "2460: accuracy:(0.9455395, 0.9455602) loss: 0.8671529 (lr:0.0009768778)\n",
      "2480: accuracy:(0.9455602, 0.94558096) loss: 0.06417735 (lr:0.00096815266)\n",
      "2500: accuracy:(0.94558096, 0.9455979) loss: 2.0275567 (lr:0.0009595144)\n",
      "2500: ********* epoch 5 ********* test accuracy:(0.9455979, 0.94715333) test loss: 4.070599\n",
      "2520: accuracy:(0.94715333, 0.94716907) loss: 1.0180118 (lr:0.000950962)\n",
      "2540: accuracy:(0.94716907, 0.94718474) loss: 2.2354941 (lr:0.00094249484)\n",
      "2560: accuracy:(0.94718474, 0.9472041) loss: 1.0491917 (lr:0.0009341118)\n",
      "2580: accuracy:(0.9472041, 0.9472234) loss: 0.12995702 (lr:0.0009258123)\n",
      "2600: accuracy:(0.9472234, 0.9472428) loss: 0.5442358 (lr:0.00091759535)\n",
      "2600: ********* epoch 5 ********* test accuracy:(0.9472428, 0.9486577) test loss: 3.7018683\n",
      "2620: accuracy:(0.9486577, 0.9486723) loss: 1.1725913 (lr:0.0009094601)\n",
      "2640: accuracy:(0.9486723, 0.9486904) loss: 0.09197676 (lr:0.0009014059)\n",
      "2660: accuracy:(0.9486904, 0.94870853) loss: 0.9547043 (lr:0.0008934318)\n",
      "2680: accuracy:(0.94870853, 0.9487231) loss: 1.1574079 (lr:0.00088553707)\n",
      "2700: accuracy:(0.9487231, 0.9487412) loss: 0.5548441 (lr:0.0008777208)\n",
      "2700: ********* epoch 5 ********* test accuracy:(0.9487412, 0.9500341) test loss: 3.7867506\n",
      "2720: accuracy:(0.9500341, 0.95005107) loss: 0.36596698 (lr:0.00086998235)\n",
      "2740: accuracy:(0.95005107, 0.95006466) loss: 3.9525096 (lr:0.0008623208)\n",
      "2760: accuracy:(0.95006466, 0.95008165) loss: 1.0877303 (lr:0.0008547356)\n",
      "2780: accuracy:(0.95008165, 0.95009184) loss: 4.2013707 (lr:0.0008472259)\n",
      "2800: accuracy:(0.95009184, 0.9501054) loss: 1.4141501 (lr:0.00083979085)\n",
      "2800: ********* epoch 5 ********* test accuracy:(0.9501054, 0.9513318) test loss: 3.9212835\n",
      "2820: accuracy:(0.9513318, 0.9513445) loss: 1.2883689 (lr:0.00083242985)\n",
      "2840: accuracy:(0.9513445, 0.9513572) loss: 4.7189054 (lr:0.000825142)\n",
      "2860: accuracy:(0.9513572, 0.9513732) loss: 0.5853601 (lr:0.0008179267)\n",
      "2880: accuracy:(0.9513732, 0.95138913) loss: 0.3396254 (lr:0.00081078324)\n",
      "2900: accuracy:(0.95138913, 0.9514051) loss: 0.65678334 (lr:0.00080371083)\n",
      "2900: ********* epoch 5 ********* test accuracy:(0.9514051, 0.95255244) test loss: 3.987572\n",
      "2920: accuracy:(0.95255244, 0.9525675) loss: 1.2140715 (lr:0.00079670886)\n",
      "2940: accuracy:(0.9525675, 0.95257944) loss: 5.9484835 (lr:0.00078977644)\n",
      "2960: accuracy:(0.95257944, 0.95259446) loss: 0.09304256 (lr:0.00078291306)\n",
      "2980: accuracy:(0.95259446, 0.95260954) loss: 0.8350005 (lr:0.00077611796)\n",
      "3000: accuracy:(0.95260954, 0.95262456) loss: 0.692695 (lr:0.00076939043)\n",
      "3000: ********* epoch 6 ********* test accuracy:(0.95262456, 0.9536758) test loss: 4.016024\n",
      "3020: accuracy:(0.9536758, 0.95369005) loss: 1.418664 (lr:0.0007627299)\n",
      "3040: accuracy:(0.95369005, 0.9537043) loss: 0.39456758 (lr:0.0007561356)\n",
      "3060: accuracy:(0.9537043, 0.9537185) loss: 0.37547058 (lr:0.000749607)\n",
      "3080: accuracy:(0.9537185, 0.9537327) loss: 0.12803094 (lr:0.0007431433)\n",
      "3100: accuracy:(0.9537327, 0.9537469) loss: 0.06799918 (lr:0.0007367439)\n",
      "3100: ********* epoch 6 ********* test accuracy:(0.9537469, 0.95474374) test loss: 3.9301212\n",
      "3120: accuracy:(0.95474374, 0.95475423) loss: 1.4834348 (lr:0.00073040824)\n",
      "3140: accuracy:(0.95475423, 0.9547677) loss: 0.16747126 (lr:0.00072413555)\n",
      "3160: accuracy:(0.9547677, 0.9547782) loss: 1.2115568 (lr:0.0007179253)\n",
      "3180: accuracy:(0.9547782, 0.95479167) loss: 0.7162077 (lr:0.00071177684)\n",
      "3200: accuracy:(0.95479167, 0.95480514) loss: 0.6680546 (lr:0.00070568954)\n",
      "3200: ********* epoch 6 ********* test accuracy:(0.95480514, 0.95573246) test loss: 4.018622\n",
      "3220: accuracy:(0.95573246, 0.95574236) loss: 2.7112284 (lr:0.0006996628)\n",
      "3240: accuracy:(0.95574236, 0.95575225) loss: 1.2696447 (lr:0.0006936961)\n",
      "3260: accuracy:(0.95575225, 0.955765) loss: 0.13814834 (lr:0.0006877887)\n",
      "3280: accuracy:(0.955765, 0.95577776) loss: 0.02610833 (lr:0.00068194006)\n",
      "3300: accuracy:(0.95577776, 0.95578474) loss: 3.5775192 (lr:0.0006761497)\n",
      "3300: ********* epoch 6 ********* test accuracy:(0.95578474, 0.95668256) test loss: 4.054069\n",
      "3320: accuracy:(0.95668256, 0.9566947) loss: 1.0624894 (lr:0.0006704169)\n",
      "3340: accuracy:(0.9566947, 0.9567068) loss: 0.1152064 (lr:0.00066474115)\n",
      "3360: accuracy:(0.9567068, 0.956719) loss: 0.32405448 (lr:0.0006591219)\n",
      "3380: accuracy:(0.956719, 0.9567283) loss: 3.2309783 (lr:0.0006535585)\n",
      "3400: accuracy:(0.9567283, 0.9567404) loss: 0.122561224 (lr:0.00064805057)\n",
      "3400: ********* epoch 6 ********* test accuracy:(0.9567404, 0.9575756) test loss: 3.995939\n",
      "3420: accuracy:(0.9575756, 0.95758444) loss: 2.009399 (lr:0.0006425974)\n",
      "3440: accuracy:(0.95758444, 0.95759594) loss: 0.2835983 (lr:0.00063719845)\n",
      "3460: accuracy:(0.95759594, 0.95760477) loss: 1.3028247 (lr:0.0006318532)\n",
      "3480: accuracy:(0.95760477, 0.9576163) loss: 0.23499048 (lr:0.0006265612)\n",
      "3500: accuracy:(0.9576163, 0.95762783) loss: 0.8458429 (lr:0.0006213218)\n",
      "3500: ********* epoch 6 ********* test accuracy:(0.95762783, 0.95837396) test loss: 4.3045626\n",
      "3520: accuracy:(0.95837396, 0.9583797) loss: 2.750806 (lr:0.0006161346)\n",
      "3540: accuracy:(0.9583797, 0.95839065) loss: 1.4393333 (lr:0.00061099895)\n",
      "3560: accuracy:(0.95839065, 0.95839906) loss: 3.7988224 (lr:0.00060591445)\n",
      "3580: accuracy:(0.95839906, 0.9584048) loss: 2.4079149 (lr:0.0006008805)\n",
      "3600: accuracy:(0.9584048, 0.95841575) loss: 0.5559373 (lr:0.0005958966)\n",
      "3600: ********* epoch 7 ********* test accuracy:(0.95841575, 0.9591497) test loss: 4.061068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3620: accuracy:(0.9591497, 0.9591602) loss: 0.22776885 (lr:0.0005909624)\n",
      "3640: accuracy:(0.9591602, 0.95917076) loss: 0.06537431 (lr:0.0005860772)\n",
      "3660: accuracy:(0.95917076, 0.95918125) loss: 0.38247415 (lr:0.0005812407)\n",
      "3680: accuracy:(0.95918125, 0.95919174) loss: 0.3754046 (lr:0.0005764523)\n",
      "3700: accuracy:(0.95919174, 0.95919967) loss: 1.8316833 (lr:0.00057171145)\n",
      "3700: ********* epoch 7 ********* test accuracy:(0.95919967, 0.9599197) test loss: 3.9309587\n",
      "3720: accuracy:(0.9599197, 0.95992726) loss: 0.8979051 (lr:0.0005670178)\n",
      "3740: accuracy:(0.95992726, 0.95993733) loss: 0.21522659 (lr:0.00056237093)\n",
      "3760: accuracy:(0.95993733, 0.95994735) loss: 0.07129696 (lr:0.00055777025)\n",
      "3780: accuracy:(0.95994735, 0.9599574) loss: 0.7868906 (lr:0.0005532154)\n",
      "3800: accuracy:(0.9599574, 0.95996493) loss: 2.2922375 (lr:0.00054870587)\n",
      "3800: ********* epoch 7 ********* test accuracy:(0.95996493, 0.9606429) test loss: 3.945542\n",
      "3820: accuracy:(0.9606429, 0.9606525) loss: 0.23917565 (lr:0.0005442411)\n",
      "3840: accuracy:(0.9606525, 0.9606597) loss: 8.336016 (lr:0.0005398209)\n",
      "3860: accuracy:(0.9606597, 0.9606693) loss: 0.056891568 (lr:0.0005354446)\n",
      "3880: accuracy:(0.9606693, 0.9606789) loss: 1.1366053 (lr:0.00053111184)\n",
      "3900: accuracy:(0.9606789, 0.9606885) loss: 0.17192322 (lr:0.0005268222)\n",
      "3900: ********* epoch 7 ********* test accuracy:(0.9606885, 0.96130127) test loss: 4.382771\n",
      "3920: accuracy:(0.96130127, 0.96131045) loss: 0.028463464 (lr:0.0005225753)\n",
      "3940: accuracy:(0.96131045, 0.9613197) loss: 0.4343512 (lr:0.00051837054)\n",
      "3960: accuracy:(0.9613197, 0.96132886) loss: 0.9150108 (lr:0.0005142077)\n",
      "3980: accuracy:(0.96132886, 0.9613381) loss: 0.06250256 (lr:0.00051008625)\n",
      "4000: accuracy:(0.9613381, 0.9613473) loss: 0.16301146 (lr:0.0005060059)\n",
      "4000: ********* epoch 7 ********* test accuracy:(0.9613473, 0.9619414) test loss: 4.599693\n",
      "4020: accuracy:(0.9619414, 0.96195024) loss: 0.39265886 (lr:0.00050196605)\n",
      "4040: accuracy:(0.96195024, 0.9619591) loss: 0.22819328 (lr:0.0004979664)\n",
      "4060: accuracy:(0.9619591, 0.9619656) loss: 1.5958188 (lr:0.0004940065)\n",
      "4080: accuracy:(0.9619656, 0.9619721) loss: 1.5391408 (lr:0.0004900861)\n",
      "4100: accuracy:(0.9619721, 0.96198094) loss: 0.2503283 (lr:0.00048620466)\n",
      "4100: ********* epoch 7 ********* test accuracy:(0.96198094, 0.962542) test loss: 4.341602\n",
      "4120: accuracy:(0.962542, 0.96255046) loss: 0.099783786 (lr:0.00048236188)\n",
      "4140: accuracy:(0.96255046, 0.962559) loss: 0.1929686 (lr:0.00047855728)\n",
      "4160: accuracy:(0.962559, 0.9625652) loss: 1.9397273 (lr:0.0004747906)\n",
      "4180: accuracy:(0.9625652, 0.9625737) loss: 0.21765521 (lr:0.0004710614)\n",
      "4200: accuracy:(0.9625737, 0.9625822) loss: 0.05226805 (lr:0.00046736925)\n",
      "4200: ********* epoch 8 ********* test accuracy:(0.9625822, 0.9631257) test loss: 4.663007\n",
      "4220: accuracy:(0.9631257, 0.9631339) loss: 0.120245524 (lr:0.00046371386)\n",
      "4240: accuracy:(0.9631339, 0.96314204) loss: 0.45648968 (lr:0.00046009486)\n",
      "4260: accuracy:(0.96314204, 0.9631502) loss: 0.025459487 (lr:0.0004565119)\n",
      "4280: accuracy:(0.9631502, 0.96315837) loss: 0.4177624 (lr:0.0004529645)\n",
      "4300: accuracy:(0.96315837, 0.96316653) loss: 0.3855673 (lr:0.00044945246)\n",
      "4300: ********* epoch 8 ********* test accuracy:(0.96316653, 0.96368283) test loss: 4.4864073\n",
      "4320: accuracy:(0.96368283, 0.96368855) loss: 1.1578382 (lr:0.00044597537)\n",
      "4340: accuracy:(0.96368855, 0.9636964) loss: 0.033008467 (lr:0.00044253285)\n",
      "4360: accuracy:(0.9636964, 0.9637043) loss: 0.008018169 (lr:0.0004391246)\n",
      "4380: accuracy:(0.9637043, 0.9637121) loss: 1.0656258 (lr:0.00043575026)\n",
      "4400: accuracy:(0.9637121, 0.96371996) loss: 0.010171357 (lr:0.0004324095)\n",
      "4400: ********* epoch 8 ********* test accuracy:(0.96371996, 0.9642258) test loss: 4.414342\n",
      "4420: accuracy:(0.9642258, 0.9642334) loss: 0.018721843 (lr:0.00042910196)\n",
      "4440: accuracy:(0.9642334, 0.96424097) loss: 0.09851952 (lr:0.00042582734)\n",
      "4460: accuracy:(0.96424097, 0.96424854) loss: 0.18361844 (lr:0.00042258532)\n",
      "4480: accuracy:(0.96424854, 0.9642561) loss: 0.18321136 (lr:0.00041937552)\n",
      "4500: accuracy:(0.9642561, 0.9642637) loss: 0.05822014 (lr:0.0004161977)\n",
      "4500: ********* epoch 8 ********* test accuracy:(0.9642637, 0.964741) test loss: 4.284451\n",
      "4520: accuracy:(0.964741, 0.96474826) loss: 0.09710937 (lr:0.0004130515)\n",
      "4540: accuracy:(0.96474826, 0.9647556) loss: 0.098345965 (lr:0.0004099365)\n",
      "4560: accuracy:(0.9647556, 0.96476287) loss: 0.37061056 (lr:0.00040685257)\n",
      "4580: accuracy:(0.96476287, 0.9647702) loss: 0.11696013 (lr:0.00040379935)\n",
      "4600: accuracy:(0.9647702, 0.96477747) loss: 0.16196135 (lr:0.00040077648)\n",
      "4600: ********* epoch 8 ********* test accuracy:(0.96477747, 0.96523017) test loss: 4.4179535\n",
      "4620: accuracy:(0.96523017, 0.9652372) loss: 0.09822948 (lr:0.00039778373)\n",
      "4640: accuracy:(0.9652372, 0.96524227) loss: 1.0675752 (lr:0.00039482073)\n",
      "4660: accuracy:(0.96524227, 0.9652493) loss: 0.07521225 (lr:0.0003918872)\n",
      "4680: accuracy:(0.9652493, 0.9652543) loss: 1.938187 (lr:0.0003889829)\n",
      "4700: accuracy:(0.9652543, 0.96526134) loss: 0.16544053 (lr:0.00038610745)\n",
      "4700: ********* epoch 8 ********* test accuracy:(0.96526134, 0.96568906) test loss: 4.3494577\n",
      "4720: accuracy:(0.96568906, 0.96569586) loss: 0.005127902 (lr:0.00038326066)\n",
      "4740: accuracy:(0.96569586, 0.96570265) loss: 0.034472723 (lr:0.00038044216)\n",
      "4760: accuracy:(0.96570265, 0.96570945) loss: 0.09605894 (lr:0.00037765174)\n",
      "4780: accuracy:(0.96570945, 0.96571624) loss: 0.020215224 (lr:0.00037488903)\n",
      "4800: accuracy:(0.96571624, 0.9657231) loss: 0.50538546 (lr:0.00037215385)\n",
      "4800: ********* epoch 9 ********* test accuracy:(0.9657231, 0.9661525) test loss: 4.4312744\n",
      "4820: accuracy:(0.9661525, 0.9661591) loss: 0.0029886996 (lr:0.0003694459)\n",
      "4840: accuracy:(0.9661591, 0.96616566) loss: 0.24958874 (lr:0.00036676484)\n",
      "4860: accuracy:(0.96616566, 0.9661722) loss: 0.03655534 (lr:0.0003641105)\n",
      "4880: accuracy:(0.9661722, 0.96617883) loss: 0.06272205 (lr:0.00036148255)\n",
      "4900: accuracy:(0.96617883, 0.9661854) loss: 0.87337494 (lr:0.00035888076)\n",
      "4900: ********* epoch 9 ********* test accuracy:(0.9661854, 0.9665879) test loss: 4.37593\n",
      "4920: accuracy:(0.9665879, 0.9665942) loss: 0.009242093 (lr:0.00035630487)\n",
      "4940: accuracy:(0.9665942, 0.9665987) loss: 0.8466481 (lr:0.00035375459)\n",
      "4960: accuracy:(0.9665987, 0.96660507) loss: 0.1625658 (lr:0.00035122968)\n",
      "4980: accuracy:(0.96660507, 0.96661144) loss: 0.0678383 (lr:0.00034872993)\n",
      "5000: accuracy:(0.96661144, 0.96661776) loss: 0.0646873 (lr:0.00034625502)\n",
      "5000: ********* epoch 9 ********* test accuracy:(0.96661776, 0.967023) test loss: 4.3113527\n",
      "5020: accuracy:(0.967023, 0.96702915) loss: 0.13942961 (lr:0.00034380468)\n",
      "5040: accuracy:(0.96702915, 0.9670353) loss: 0.5490398 (lr:0.0003413788)\n",
      "5060: accuracy:(0.9670353, 0.9670415) loss: 0.0061499695 (lr:0.00033897703)\n",
      "5080: accuracy:(0.9670415, 0.96704763) loss: 0.1121358 (lr:0.0003365992)\n",
      "5100: accuracy:(0.96704763, 0.9670538) loss: 0.03974994 (lr:0.00033424495)\n",
      "5100: ********* epoch 9 ********* test accuracy:(0.9670538, 0.9674193) test loss: 4.58409\n",
      "5120: accuracy:(0.9674193, 0.96742535) loss: 0.022313027 (lr:0.0003319142)\n",
      "5140: accuracy:(0.96742535, 0.9674313) loss: 0.010670748 (lr:0.0003296066)\n",
      "5160: accuracy:(0.9674313, 0.96743727) loss: 0.09168507 (lr:0.000327322)\n",
      "5180: accuracy:(0.96743727, 0.9674432) loss: 0.02378636 (lr:0.0003250601)\n",
      "5200: accuracy:(0.9674432, 0.9674492) loss: 0.007256461 (lr:0.0003228207)\n",
      "5200: ********* epoch 9 ********* test accuracy:(0.9674492, 0.9678134) test loss: 4.6608515\n",
      "5220: accuracy:(0.9678134, 0.96781915) loss: 0.08276521 (lr:0.00032060363)\n",
      "5240: accuracy:(0.96781915, 0.96782494) loss: 0.10847799 (lr:0.00031840857)\n",
      "5260: accuracy:(0.96782494, 0.9678307) loss: 0.1100041 (lr:0.00031623538)\n",
      "5280: accuracy:(0.9678307, 0.9678365) loss: 0.698695 (lr:0.0003140838)\n",
      "5300: accuracy:(0.9678365, 0.9678423) loss: 0.020669237 (lr:0.00031195363)\n",
      "5300: ********* epoch 9 ********* test accuracy:(0.9678423, 0.96819276) test loss: 4.650152\n",
      "5320: accuracy:(0.96819276, 0.96819836) loss: 0.14945272 (lr:0.00030984465)\n",
      "5340: accuracy:(0.96819836, 0.96820396) loss: 0.015675437 (lr:0.00030775668)\n",
      "5360: accuracy:(0.96820396, 0.96820956) loss: 0.025046611 (lr:0.00030568946)\n",
      "5380: accuracy:(0.96820956, 0.96821517) loss: 0.030625978 (lr:0.00030364282)\n",
      "5400: accuracy:(0.96821517, 0.9682208) loss: 0.011113505 (lr:0.00030161653)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400: ********* epoch 10 ********* test accuracy:(0.9682208, 0.96855485) test loss: 4.7480826\n",
      "5420: accuracy:(0.96855485, 0.9685603) loss: 0.021101372 (lr:0.00029961043)\n",
      "5440: accuracy:(0.9685603, 0.96856576) loss: 0.006162687 (lr:0.00029762427)\n",
      "5460: accuracy:(0.96856576, 0.96856946) loss: 0.97665787 (lr:0.00029565787)\n",
      "5480: accuracy:(0.96856946, 0.9685749) loss: 0.05856354 (lr:0.00029371103)\n",
      "5500: accuracy:(0.9685749, 0.9685803) loss: 0.0029510765 (lr:0.00029178357)\n",
      "5500: ********* epoch 10 ********* test accuracy:(0.9685803, 0.9688887) test loss: 4.718582\n",
      "5520: accuracy:(0.9688887, 0.968894) loss: 0.012943205 (lr:0.0002898753)\n",
      "5540: accuracy:(0.968894, 0.9688993) loss: 0.04663483 (lr:0.00028798598)\n",
      "5560: accuracy:(0.9688993, 0.96890455) loss: 0.068122946 (lr:0.0002861155)\n",
      "5580: accuracy:(0.96890455, 0.96890986) loss: 0.13654473 (lr:0.0002842636)\n",
      "5600: accuracy:(0.96890986, 0.96891516) loss: 0.0065639256 (lr:0.00028243018)\n",
      "5600: ********* epoch 10 ********* test accuracy:(0.96891516, 0.9692242) test loss: 4.715334\n",
      "5620: accuracy:(0.9692242, 0.96922934) loss: 0.06140226 (lr:0.00028061497)\n",
      "5640: accuracy:(0.96922934, 0.9692345) loss: 0.0046202377 (lr:0.0002788178)\n",
      "5660: accuracy:(0.9692345, 0.96923965) loss: 0.5057217 (lr:0.00027703855)\n",
      "5680: accuracy:(0.96923965, 0.9692448) loss: 0.22516492 (lr:0.00027527698)\n",
      "5700: accuracy:(0.9692448, 0.9692499) loss: 0.25620967 (lr:0.00027353296)\n",
      "5700: ********* epoch 10 ********* test accuracy:(0.9692499, 0.9695465) test loss: 4.7635164\n",
      "5720: accuracy:(0.9695465, 0.9695515) loss: 0.059592158 (lr:0.00027180626)\n",
      "5740: accuracy:(0.9695515, 0.9695565) loss: 0.53916055 (lr:0.00027009676)\n",
      "5760: accuracy:(0.9695565, 0.9695615) loss: 0.0029876446 (lr:0.00026840428)\n",
      "5780: accuracy:(0.9695615, 0.9695665) loss: 0.047169432 (lr:0.00026672863)\n",
      "5800: accuracy:(0.9695665, 0.9695715) loss: 0.081161804 (lr:0.00026506966)\n",
      "5800: ********* epoch 10 ********* test accuracy:(0.9695715, 0.9698627) test loss: 4.829499\n",
      "5820: accuracy:(0.9698627, 0.9698676) loss: 0.009357229 (lr:0.00026342718)\n",
      "5840: accuracy:(0.9698676, 0.9698724) loss: 0.06994156 (lr:0.00026180106)\n",
      "5860: accuracy:(0.9698724, 0.9698773) loss: 0.012588206 (lr:0.0002601911)\n",
      "5880: accuracy:(0.9698773, 0.9698822) loss: 0.04658477 (lr:0.0002585972)\n",
      "5900: accuracy:(0.9698822, 0.969887) loss: 0.0224387 (lr:0.00025701913)\n",
      "5900: ********* epoch 10 ********* test accuracy:(0.969887, 0.9701779) test loss: 4.8079653\n",
      "5920: accuracy:(0.9701779, 0.9701826) loss: 0.10911329 (lr:0.00025545675)\n",
      "5940: accuracy:(0.9701826, 0.97018737) loss: 0.06806914 (lr:0.00025390994)\n",
      "5960: accuracy:(0.97018737, 0.9701921) loss: 0.0089813275 (lr:0.00025237846)\n",
      "5980: accuracy:(0.9701921, 0.97019684) loss: 0.05815606 (lr:0.0002508623)\n",
      "6000: accuracy:(0.97019684, 0.97020155) loss: 0.054893613 (lr:0.00024936118)\n",
      "6000: ********* epoch 11 ********* test accuracy:(0.97020155, 0.9704843) test loss: 4.7365875\n",
      "6020: accuracy:(0.9704843, 0.9704889) loss: 0.008827221 (lr:0.000247875)\n",
      "6040: accuracy:(0.9704889, 0.9704935) loss: 0.08467112 (lr:0.00024640362)\n",
      "6060: accuracy:(0.9704935, 0.97049814) loss: 0.057221577 (lr:0.0002449469)\n",
      "6080: accuracy:(0.97049814, 0.97050273) loss: 0.019041035 (lr:0.00024350465)\n",
      "6100: accuracy:(0.97050273, 0.9705058) loss: 1.8420912 (lr:0.00024207676)\n",
      "6100: ********* epoch 11 ********* test accuracy:(0.9705058, 0.97076696) test loss: 4.788902\n",
      "6120: accuracy:(0.97076696, 0.9707715) loss: 1.0721612 (lr:0.00024066307)\n",
      "6140: accuracy:(0.9707715, 0.97077596) loss: 0.022623215 (lr:0.00023926346)\n",
      "6160: accuracy:(0.97077596, 0.97078043) loss: 0.04467075 (lr:0.00023787777)\n",
      "6180: accuracy:(0.97078043, 0.97078496) loss: 0.05892858 (lr:0.00023650585)\n",
      "6200: accuracy:(0.97078496, 0.97078943) loss: 0.16840903 (lr:0.0002351476)\n",
      "6200: ********* epoch 11 ********* test accuracy:(0.97078943, 0.97104067) test loss: 5.0394254\n",
      "6220: accuracy:(0.97104067, 0.9710451) loss: 0.015539308 (lr:0.00023380286)\n",
      "6240: accuracy:(0.9710451, 0.9710494) loss: 0.09183596 (lr:0.0002324715)\n",
      "6260: accuracy:(0.9710494, 0.97105384) loss: 0.03454304 (lr:0.00023115339)\n",
      "6280: accuracy:(0.97105384, 0.9710582) loss: 0.07153063 (lr:0.00022984837)\n",
      "6300: accuracy:(0.9710582, 0.9710626) loss: 0.092132054 (lr:0.00022855637)\n",
      "6300: ********* epoch 11 ********* test accuracy:(0.9710626, 0.9713088) test loss: 5.039104\n",
      "6320: accuracy:(0.9713088, 0.97131306) loss: 0.21392886 (lr:0.00022727723)\n",
      "6340: accuracy:(0.97131306, 0.97131735) loss: 0.117612384 (lr:0.0002260108)\n",
      "6360: accuracy:(0.97131735, 0.97132164) loss: 0.34531936 (lr:0.00022475695)\n",
      "6380: accuracy:(0.97132164, 0.9713259) loss: 0.006855415 (lr:0.00022351561)\n",
      "6400: accuracy:(0.9713259, 0.97133017) loss: 0.12776478 (lr:0.00022228662)\n",
      "6400: ********* epoch 11 ********* test accuracy:(0.97133017, 0.9715687) test loss: 4.915721\n",
      "6420: accuracy:(0.9715687, 0.9715729) loss: 0.16414209 (lr:0.00022106984)\n",
      "6440: accuracy:(0.9715729, 0.97157705) loss: 0.023628574 (lr:0.00021986515)\n",
      "6460: accuracy:(0.97157705, 0.97158116) loss: 0.0118792225 (lr:0.00021867247)\n",
      "6480: accuracy:(0.97158116, 0.97158533) loss: 0.10217145 (lr:0.00021749166)\n",
      "6500: accuracy:(0.97158533, 0.9715895) loss: 0.00889005 (lr:0.0002163226)\n",
      "6500: ********* epoch 11 ********* test accuracy:(0.9715895, 0.97181344) test loss: 4.8483396\n",
      "6520: accuracy:(0.97181344, 0.97181755) loss: 0.48242193 (lr:0.00021516517)\n",
      "6540: accuracy:(0.97181755, 0.9718216) loss: 0.44789767 (lr:0.00021401927)\n",
      "6560: accuracy:(0.9718216, 0.97182566) loss: 0.14861509 (lr:0.00021288476)\n",
      "6580: accuracy:(0.97182566, 0.9718297) loss: 0.01271661 (lr:0.00021176154)\n",
      "6600: accuracy:(0.9718297, 0.97183377) loss: 0.053781822 (lr:0.0002106495)\n",
      "6600: ********* epoch 12 ********* test accuracy:(0.97183377, 0.9720509) test loss: 5.0308247\n",
      "6620: accuracy:(0.9720509, 0.9720549) loss: 0.0019804072 (lr:0.00020954851)\n",
      "6640: accuracy:(0.9720549, 0.9720589) loss: 0.01727402 (lr:0.00020845848)\n",
      "6660: accuracy:(0.9720589, 0.9720628) loss: 0.01625344 (lr:0.00020737931)\n",
      "6680: accuracy:(0.9720628, 0.9720668) loss: 0.0037224912 (lr:0.00020631085)\n",
      "6700: accuracy:(0.9720668, 0.97207075) loss: 0.031419758 (lr:0.00020525305)\n",
      "6700: ********* epoch 12 ********* test accuracy:(0.97207075, 0.9722786) test loss: 5.010028\n",
      "6720: accuracy:(0.9722786, 0.97228247) loss: 0.0027067917 (lr:0.00020420577)\n",
      "6740: accuracy:(0.97228247, 0.97228634) loss: 0.04571218 (lr:0.00020316891)\n",
      "6760: accuracy:(0.97228634, 0.9722902) loss: 0.014718175 (lr:0.00020214237)\n",
      "6780: accuracy:(0.9722902, 0.9722941) loss: 0.002930857 (lr:0.00020112604)\n",
      "6800: accuracy:(0.9722941, 0.972298) loss: 0.049478978 (lr:0.0002001198)\n",
      "6800: ********* epoch 12 ********* test accuracy:(0.972298, 0.97250515) test loss: 5.243482\n",
      "6820: accuracy:(0.97250515, 0.97250897) loss: 0.009313524 (lr:0.0001991236)\n",
      "6840: accuracy:(0.97250897, 0.9725128) loss: 0.03924365 (lr:0.0001981373)\n",
      "6860: accuracy:(0.9725128, 0.97251654) loss: 0.027042266 (lr:0.00019716081)\n",
      "6880: accuracy:(0.97251654, 0.97252035) loss: 0.0040008165 (lr:0.00019619406)\n",
      "6900: accuracy:(0.97252035, 0.97252417) loss: 0.0007745293 (lr:0.0001952369)\n",
      "6900: ********* epoch 12 ********* test accuracy:(0.97252417, 0.9727253) test loss: 5.067009\n",
      "6920: accuracy:(0.9727253, 0.972729) loss: 0.017491473 (lr:0.00019428927)\n",
      "6940: accuracy:(0.972729, 0.9727327) loss: 0.007633716 (lr:0.00019335106)\n",
      "6960: accuracy:(0.9727327, 0.9727364) loss: 0.10526776 (lr:0.00019242222)\n",
      "6980: accuracy:(0.9727364, 0.9727401) loss: 0.017280316 (lr:0.0001915026)\n",
      "7000: accuracy:(0.9727401, 0.97274387) loss: 0.13644329 (lr:0.00019059214)\n",
      "7000: ********* epoch 12 ********* test accuracy:(0.97274387, 0.9729352) test loss: 5.076837\n",
      "7020: accuracy:(0.9729352, 0.97293884) loss: 0.07279831 (lr:0.00018969073)\n",
      "7040: accuracy:(0.97293884, 0.9729424) loss: 0.0050451565 (lr:0.0001887983)\n",
      "7060: accuracy:(0.9729424, 0.97294605) loss: 0.05524241 (lr:0.00018791473)\n",
      "7080: accuracy:(0.97294605, 0.9729497) loss: 0.024120202 (lr:0.00018703997)\n",
      "7100: accuracy:(0.9729497, 0.9729533) loss: 0.015551998 (lr:0.00018617391)\n",
      "7100: ********* epoch 12 ********* test accuracy:(0.9729533, 0.97314185) test loss: 5.294259\n",
      "7120: accuracy:(0.97314185, 0.9731454) loss: 0.11140921 (lr:0.00018531646)\n",
      "7140: accuracy:(0.9731454, 0.973149) loss: 0.017140817 (lr:0.00018446756)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7160: accuracy:(0.973149, 0.9731525) loss: 0.00028620637 (lr:0.00018362708)\n",
      "7180: accuracy:(0.9731525, 0.9731561) loss: 0.08530471 (lr:0.00018279499)\n",
      "7200: accuracy:(0.9731561, 0.9731596) loss: 0.048534483 (lr:0.00018197116)\n",
      "7200: ********* epoch 13 ********* test accuracy:(0.9731596, 0.9733416) test loss: 5.3859105\n",
      "7220: accuracy:(0.9733416, 0.9733451) loss: 0.032326713 (lr:0.00018115554)\n",
      "7240: accuracy:(0.9733451, 0.97334856) loss: 0.01581268 (lr:0.00018034803)\n",
      "7260: accuracy:(0.97334856, 0.973352) loss: 0.04368916 (lr:0.00017954854)\n",
      "7280: accuracy:(0.973352, 0.97335553) loss: 0.053258155 (lr:0.00017875704)\n",
      "7300: accuracy:(0.97335553, 0.973359) loss: 0.038921315 (lr:0.00017797339)\n",
      "7300: ********* epoch 13 ********* test accuracy:(0.973359, 0.97353595) test loss: 5.3678565\n",
      "7320: accuracy:(0.97353595, 0.97353935) loss: 0.00420483 (lr:0.00017719754)\n",
      "7340: accuracy:(0.97353935, 0.97354275) loss: 0.012855206 (lr:0.0001764294)\n",
      "7360: accuracy:(0.97354275, 0.97354615) loss: 0.0019459623 (lr:0.00017566892)\n",
      "7380: accuracy:(0.97354615, 0.97354954) loss: 0.0071662636 (lr:0.000174916)\n",
      "7400: accuracy:(0.97354954, 0.97355294) loss: 0.02388606 (lr:0.00017417056)\n",
      "7400: ********* epoch 13 ********* test accuracy:(0.97355294, 0.9737251) test loss: 5.38421\n",
      "7420: accuracy:(0.9737251, 0.9737284) loss: 0.0039441823 (lr:0.00017343255)\n",
      "7440: accuracy:(0.9737284, 0.97373176) loss: 0.006819723 (lr:0.0001727019)\n",
      "7460: accuracy:(0.97373176, 0.9737351) loss: 0.08517012 (lr:0.00017197849)\n",
      "7480: accuracy:(0.9737351, 0.97373843) loss: 0.011453965 (lr:0.0001712623)\n",
      "7500: accuracy:(0.97373843, 0.97374177) loss: 0.24030243 (lr:0.00017055322)\n",
      "7500: ********* epoch 13 ********* test accuracy:(0.97374177, 0.9739092) test loss: 5.356301\n",
      "7520: accuracy:(0.9739092, 0.9739125) loss: 0.011854501 (lr:0.0001698512)\n",
      "7540: accuracy:(0.9739125, 0.97391576) loss: 0.0060093994 (lr:0.00016915618)\n",
      "7560: accuracy:(0.97391576, 0.97391903) loss: 0.0035127678 (lr:0.00016846808)\n",
      "7580: accuracy:(0.97391903, 0.9739223) loss: 0.036774393 (lr:0.0001677868)\n",
      "7600: accuracy:(0.9739223, 0.9739256) loss: 0.00925775 (lr:0.0001671123)\n",
      "7600: ********* epoch 13 ********* test accuracy:(0.9739256, 0.9740849) test loss: 5.506309\n",
      "7620: accuracy:(0.9740849, 0.9740881) loss: 0.15514223 (lr:0.00016644453)\n",
      "7640: accuracy:(0.9740881, 0.9740913) loss: 0.054811254 (lr:0.0001657834)\n",
      "7660: accuracy:(0.9740913, 0.9740945) loss: 0.0048540113 (lr:0.00016512885)\n",
      "7680: accuracy:(0.9740945, 0.9740977) loss: 0.028677523 (lr:0.00016448079)\n",
      "7700: accuracy:(0.9740977, 0.9741009) loss: 0.043352336 (lr:0.0001638392)\n",
      "7700: ********* epoch 13 ********* test accuracy:(0.9741009, 0.97425604) test loss: 5.4971404\n",
      "7720: accuracy:(0.97425604, 0.9742592) loss: 0.0021724212 (lr:0.000163204)\n",
      "7740: accuracy:(0.9742592, 0.97426236) loss: 0.0010029621 (lr:0.0001625751)\n",
      "7760: accuracy:(0.97426236, 0.97426546) loss: 0.013498003 (lr:0.00016195247)\n",
      "7780: accuracy:(0.97426546, 0.9742686) loss: 0.04729522 (lr:0.00016133604)\n",
      "7800: accuracy:(0.9742686, 0.9742718) loss: 0.026102794 (lr:0.00016072573)\n",
      "7800: ********* epoch 14 ********* test accuracy:(0.9742718, 0.9744337) test loss: 5.460681\n",
      "7820: accuracy:(0.9744337, 0.9744368) loss: 0.0061880923 (lr:0.0001601215)\n",
      "7840: accuracy:(0.9744368, 0.97443986) loss: 0.018123882 (lr:0.00015952328)\n",
      "7860: accuracy:(0.97443986, 0.97444296) loss: 0.007335305 (lr:0.000158931)\n",
      "7880: accuracy:(0.97444296, 0.97444606) loss: 0.12506098 (lr:0.00015834463)\n",
      "7900: accuracy:(0.97444606, 0.97444916) loss: 0.038073827 (lr:0.0001577641)\n",
      "7900: ********* epoch 14 ********* test accuracy:(0.97444916, 0.97460335) test loss: 5.4406385\n",
      "7920: accuracy:(0.97460335, 0.9746064) loss: 0.0076563773 (lr:0.00015718934)\n",
      "7940: accuracy:(0.9746064, 0.97460943) loss: 0.057410967 (lr:0.00015662028)\n",
      "7960: accuracy:(0.97460943, 0.9746125) loss: 0.05221728 (lr:0.0001560569)\n",
      "7980: accuracy:(0.9746125, 0.97461545) loss: 0.012896045 (lr:0.00015549913)\n",
      "8000: accuracy:(0.97461545, 0.9746185) loss: 0.010255709 (lr:0.00015494692)\n",
      "8000: ********* epoch 14 ********* test accuracy:(0.9746185, 0.9747712) test loss: 5.443268\n",
      "8020: accuracy:(0.9747712, 0.9747742) loss: 0.026731236 (lr:0.00015440017)\n",
      "8040: accuracy:(0.9747742, 0.97477716) loss: 0.0016906044 (lr:0.0001538589)\n",
      "8060: accuracy:(0.97477716, 0.9747801) loss: 0.002307514 (lr:0.00015332298)\n",
      "8080: accuracy:(0.9747801, 0.97478306) loss: 0.001514286 (lr:0.00015279242)\n",
      "8100: accuracy:(0.97478306, 0.97478604) loss: 0.037727058 (lr:0.00015226711)\n",
      "8100: ********* epoch 14 ********* test accuracy:(0.97478604, 0.97493374) test loss: 5.4631467\n",
      "8120: accuracy:(0.97493374, 0.97493666) loss: 0.0028446636 (lr:0.00015174704)\n",
      "8140: accuracy:(0.97493666, 0.9749396) loss: 0.019051166 (lr:0.00015123216)\n",
      "8160: accuracy:(0.9749396, 0.9749425) loss: 0.023961421 (lr:0.00015072238)\n",
      "8180: accuracy:(0.9749425, 0.9749454) loss: 0.068244785 (lr:0.0001502177)\n",
      "8200: accuracy:(0.9749454, 0.97494835) loss: 0.01382814 (lr:0.000149718)\n",
      "8200: ********* epoch 14 ********* test accuracy:(0.97494835, 0.9750855) test loss: 5.6301665\n",
      "8220: accuracy:(0.9750855, 0.97508836) loss: 0.013566012 (lr:0.00014922331)\n",
      "8240: accuracy:(0.97508836, 0.9750912) loss: 0.013121712 (lr:0.00014873352)\n",
      "8260: accuracy:(0.9750912, 0.9750941) loss: 0.0065308507 (lr:0.00014824864)\n",
      "8280: accuracy:(0.9750941, 0.97509694) loss: 0.014025474 (lr:0.00014776854)\n",
      "8300: accuracy:(0.97509694, 0.9750998) loss: 0.014545499 (lr:0.00014729325)\n",
      "8300: ********* epoch 14 ********* test accuracy:(0.9750998, 0.9752337) test loss: 5.729745\n",
      "8320: accuracy:(0.9752337, 0.9752365) loss: 0.003904738 (lr:0.00014682265)\n",
      "8340: accuracy:(0.9752365, 0.9752393) loss: 0.009905038 (lr:0.00014635678)\n",
      "8360: accuracy:(0.9752393, 0.9752421) loss: 0.014104255 (lr:0.00014589551)\n",
      "8380: accuracy:(0.9752421, 0.9752449) loss: 0.0064672045 (lr:0.00014543885)\n",
      "8400: accuracy:(0.9752449, 0.9752477) loss: 0.021757077 (lr:0.00014498673)\n",
      "8400: ********* epoch 15 ********* test accuracy:(0.9752477, 0.97538054) test loss: 5.666577\n",
      "8420: accuracy:(0.97538054, 0.97538334) loss: 0.0006679897 (lr:0.00014453911)\n",
      "8440: accuracy:(0.97538334, 0.9753861) loss: 0.09008085 (lr:0.00014409592)\n",
      "8460: accuracy:(0.9753861, 0.9753888) loss: 0.015982794 (lr:0.00014365718)\n",
      "8480: accuracy:(0.9753888, 0.9753916) loss: 0.011975834 (lr:0.00014322277)\n",
      "8500: accuracy:(0.9753916, 0.97539437) loss: 0.0031278424 (lr:0.0001427927)\n",
      "8500: ********* epoch 15 ********* test accuracy:(0.97539437, 0.97552407) test loss: 5.6120934\n",
      "8520: accuracy:(0.97552407, 0.97552675) loss: 0.07109568 (lr:0.0001423669)\n",
      "8540: accuracy:(0.97552675, 0.9755295) loss: 0.01599887 (lr:0.00014194535)\n",
      "8560: accuracy:(0.9755295, 0.9755322) loss: 0.0027438486 (lr:0.00014152798)\n",
      "8580: accuracy:(0.9755322, 0.97553486) loss: 0.0063508023 (lr:0.00014111475)\n",
      "8600: accuracy:(0.97553486, 0.9755376) loss: 0.0018777353 (lr:0.00014070567)\n",
      "8600: ********* epoch 15 ********* test accuracy:(0.9755376, 0.9756697) test loss: 5.6308055\n",
      "8620: accuracy:(0.9756697, 0.97567236) loss: 0.00037226916 (lr:0.00014030063)\n",
      "8640: accuracy:(0.97567236, 0.97567505) loss: 0.002696373 (lr:0.00013989964)\n",
      "8660: accuracy:(0.97567505, 0.97567767) loss: 0.005765027 (lr:0.00013950263)\n",
      "8680: accuracy:(0.97567767, 0.97568035) loss: 0.07026705 (lr:0.00013910959)\n",
      "8700: accuracy:(0.97568035, 0.97568303) loss: 0.006827645 (lr:0.00013872042)\n",
      "8700: ********* epoch 15 ********* test accuracy:(0.97568303, 0.9758099) test loss: 5.716382\n",
      "8720: accuracy:(0.9758099, 0.9758125) loss: 0.0587824 (lr:0.00013833516)\n",
      "8740: accuracy:(0.9758125, 0.9758151) loss: 0.005936428 (lr:0.00013795371)\n",
      "8760: accuracy:(0.9758151, 0.97581774) loss: 0.0065987892 (lr:0.00013757608)\n",
      "8780: accuracy:(0.97581774, 0.97582036) loss: 0.002776816 (lr:0.00013720218)\n",
      "8800: accuracy:(0.97582036, 0.975823) loss: 0.0032030807 (lr:0.00013683202)\n",
      "8800: ********* epoch 15 ********* test accuracy:(0.975823, 0.9759469) test loss: 5.8033967\n",
      "8820: accuracy:(0.9759469, 0.97594947) loss: 0.077129185 (lr:0.00013646553)\n",
      "8840: accuracy:(0.97594947, 0.975952) loss: 0.005526287 (lr:0.0001361027)\n",
      "8860: accuracy:(0.975952, 0.97595465) loss: 0.0045270445 (lr:0.00013574347)\n",
      "8880: accuracy:(0.97595465, 0.9759572) loss: 2.5391515e-05 (lr:0.00013538782)\n",
      "8900: accuracy:(0.9759572, 0.9759598) loss: 0.013639504 (lr:0.00013503569)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8900: ********* epoch 15 ********* test accuracy:(0.9759598, 0.97608197) test loss: 5.7638903\n",
      "8920: accuracy:(0.97608197, 0.9760845) loss: 0.0077791386 (lr:0.00013468708)\n",
      "8940: accuracy:(0.9760845, 0.976087) loss: 0.0079773 (lr:0.00013434194)\n",
      "8960: accuracy:(0.976087, 0.97608954) loss: 0.0072230045 (lr:0.00013400023)\n",
      "8980: accuracy:(0.97608954, 0.97609204) loss: 0.003506273 (lr:0.00013366193)\n",
      "9000: accuracy:(0.97609204, 0.9760946) loss: 0.019465128 (lr:0.00013332699)\n",
      "9000: ********* epoch 16 ********* test accuracy:(0.9760946, 0.97621506) test loss: 5.7978725\n",
      "9020: accuracy:(0.97621506, 0.97621757) loss: 0.007309431 (lr:0.00013299537)\n",
      "9040: accuracy:(0.97621757, 0.97622) loss: 0.017166056 (lr:0.00013266708)\n",
      "9060: accuracy:(0.97622, 0.9762225) loss: 0.00024174746 (lr:0.00013234201)\n",
      "9080: accuracy:(0.9762225, 0.976225) loss: 0.0006107696 (lr:0.00013202021)\n",
      "9100: accuracy:(0.976225, 0.9762275) loss: 0.0110621005 (lr:0.00013170161)\n",
      "9100: ********* epoch 16 ********* test accuracy:(0.9762275, 0.97634214) test loss: 5.7970886\n",
      "9120: accuracy:(0.97634214, 0.97634465) loss: 0.014753516 (lr:0.00013138616)\n",
      "9140: accuracy:(0.97634465, 0.9763471) loss: 0.0050121467 (lr:0.00013107387)\n",
      "9160: accuracy:(0.9763471, 0.97634953) loss: 0.018075824 (lr:0.00013076467)\n",
      "9180: accuracy:(0.97634953, 0.976352) loss: 0.010628993 (lr:0.00013045857)\n",
      "9200: accuracy:(0.976352, 0.9763544) loss: 0.01437721 (lr:0.0001301555)\n",
      "9200: ********* epoch 16 ********* test accuracy:(0.9763544, 0.9764614) test loss: 5.861213\n",
      "9220: accuracy:(0.9764614, 0.97646385) loss: 0.0068254326 (lr:0.00012985546)\n",
      "9240: accuracy:(0.97646385, 0.97646624) loss: 0.016861873 (lr:0.00012955838)\n",
      "9260: accuracy:(0.97646624, 0.9764687) loss: 0.010383008 (lr:0.00012926427)\n",
      "9280: accuracy:(0.9764687, 0.97647107) loss: 0.010713229 (lr:0.00012897309)\n",
      "9300: accuracy:(0.97647107, 0.97647345) loss: 0.00516687 (lr:0.0001286848)\n",
      "9300: ********* epoch 16 ********* test accuracy:(0.97647345, 0.97658324) test loss: 5.800699\n",
      "9320: accuracy:(0.97658324, 0.97658557) loss: 0.031555895 (lr:0.00012839938)\n",
      "9340: accuracy:(0.97658557, 0.97658795) loss: 0.004272081 (lr:0.00012811681)\n",
      "9360: accuracy:(0.97658795, 0.97659034) loss: 0.0045302687 (lr:0.00012783703)\n",
      "9380: accuracy:(0.97659034, 0.9765927) loss: 0.0096157845 (lr:0.00012756005)\n",
      "9400: accuracy:(0.9765927, 0.9765951) loss: 0.0019148377 (lr:0.00012728582)\n",
      "9400: ********* epoch 16 ********* test accuracy:(0.9765951, 0.97670543) test loss: 5.972611\n",
      "9420: accuracy:(0.97670543, 0.97670776) loss: 0.01629099 (lr:0.00012701433)\n",
      "9440: accuracy:(0.97670776, 0.97671014) loss: 0.002278795 (lr:0.00012674552)\n",
      "9460: accuracy:(0.97671014, 0.97671247) loss: 0.0022258775 (lr:0.00012647941)\n",
      "9480: accuracy:(0.97671247, 0.9767148) loss: 0.0070836768 (lr:0.00012621593)\n",
      "9500: accuracy:(0.9767148, 0.9767171) loss: 0.01656436 (lr:0.00012595509)\n",
      "9500: ********* epoch 16 ********* test accuracy:(0.9767171, 0.97682315) test loss: 5.952942\n",
      "9520: accuracy:(0.97682315, 0.9768254) loss: 0.0029319387 (lr:0.00012569682)\n",
      "9540: accuracy:(0.9768254, 0.97682774) loss: 0.0027425187 (lr:0.00012544113)\n",
      "9560: accuracy:(0.97682774, 0.97683007) loss: 0.0048244293 (lr:0.00012518799)\n",
      "9580: accuracy:(0.97683007, 0.97683233) loss: 0.07534991 (lr:0.00012493736)\n",
      "9600: accuracy:(0.97683233, 0.97683465) loss: 0.0024884364 (lr:0.00012468924)\n",
      "9600: ********* epoch 17 ********* test accuracy:(0.97683465, 0.97693545) test loss: 5.999447\n",
      "9620: accuracy:(0.97693545, 0.9769377) loss: 0.011712998 (lr:0.00012444357)\n",
      "9640: accuracy:(0.9769377, 0.97694) loss: 0.007361823 (lr:0.00012420036)\n",
      "9660: accuracy:(0.97694, 0.97694224) loss: 0.004893462 (lr:0.00012395956)\n",
      "9680: accuracy:(0.97694224, 0.9769445) loss: 0.017512081 (lr:0.00012372115)\n",
      "9700: accuracy:(0.9769445, 0.9769468) loss: 0.0036351539 (lr:0.00012348512)\n",
      "9700: ********* epoch 17 ********* test accuracy:(0.9769468, 0.97704357) test loss: 6.2197595\n",
      "9720: accuracy:(0.97704357, 0.9770458) loss: 0.021216135 (lr:0.00012325145)\n",
      "9740: accuracy:(0.9770458, 0.97704804) loss: 0.0050497274 (lr:0.00012302009)\n",
      "9760: accuracy:(0.97704804, 0.97705024) loss: 0.03930962 (lr:0.00012279104)\n",
      "9780: accuracy:(0.97705024, 0.97705245) loss: 0.0064499867 (lr:0.00012256426)\n",
      "9800: accuracy:(0.97705245, 0.9770547) loss: 0.01536793 (lr:0.00012233976)\n",
      "9800: ********* epoch 17 ********* test accuracy:(0.9770547, 0.9771533) test loss: 5.9993153\n",
      "9820: accuracy:(0.9771533, 0.9771555) loss: 0.016895743 (lr:0.00012211746)\n",
      "9840: accuracy:(0.9771555, 0.9771577) loss: 0.1749938 (lr:0.00012189739)\n",
      "9860: accuracy:(0.9771577, 0.9771599) loss: 0.0012991508 (lr:0.000121679506)\n",
      "9880: accuracy:(0.9771599, 0.9771621) loss: 0.007817691 (lr:0.000121463796)\n",
      "9900: accuracy:(0.9771621, 0.97716427) loss: 0.0074414397 (lr:0.000121250225)\n",
      "9900: ********* epoch 17 ********* test accuracy:(0.97716427, 0.97726375) test loss: 5.943681\n",
      "9920: accuracy:(0.97726375, 0.9772659) loss: 0.011892034 (lr:0.000121038785)\n",
      "9940: accuracy:(0.9772659, 0.97726804) loss: 0.00041052778 (lr:0.00012082944)\n",
      "9960: accuracy:(0.97726804, 0.97727025) loss: 0.02574154 (lr:0.000120622186)\n",
      "9980: accuracy:(0.97727025, 0.9772724) loss: 0.004524554 (lr:0.00012041699)\n",
      "10000: accuracy:(0.9772724, 0.97727454) loss: 0.0048471997 (lr:0.00012021384)\n",
      "10000: ********* epoch 17 ********* test accuracy:(0.97727454, 0.9773701) test loss: 6.076609\n",
      "10020: accuracy:(0.9773701, 0.97737217) loss: 0.00036500127 (lr:0.0001200127)\n",
      "10040: accuracy:(0.97737217, 0.9773743) loss: 0.001308311 (lr:0.00011981357)\n",
      "10060: accuracy:(0.9773743, 0.97737646) loss: 0.0026751603 (lr:0.00011961643)\n",
      "10080: accuracy:(0.97737646, 0.9773786) loss: 0.0012673129 (lr:0.00011942124)\n",
      "10100: accuracy:(0.9773786, 0.97738075) loss: 0.0005888446 (lr:0.000119227996)\n",
      "10100: ********* epoch 17 ********* test accuracy:(0.97738075, 0.9774734) test loss: 6.138761\n",
      "10120: accuracy:(0.9774734, 0.97747546) loss: 0.021803483 (lr:0.00011903667)\n",
      "10140: accuracy:(0.97747546, 0.9774776) loss: 0.00066832645 (lr:0.00011884726)\n",
      "10160: accuracy:(0.9774776, 0.9774797) loss: 0.036746576 (lr:0.00011865972)\n",
      "10180: accuracy:(0.9774797, 0.9774818) loss: 0.006501469 (lr:0.000118474054)\n",
      "10200: accuracy:(0.9774818, 0.97748387) loss: 0.008800196 (lr:0.000118290234)\n",
      "10200: ********* epoch 18 ********* test accuracy:(0.97748387, 0.9775784) test loss: 6.1934924\n",
      "10220: accuracy:(0.9775784, 0.9775805) loss: 0.059186872 (lr:0.00011810825)\n",
      "10240: accuracy:(0.9775805, 0.9775825) loss: 0.0044755093 (lr:0.00011792806)\n",
      "10260: accuracy:(0.9775825, 0.9775846) loss: 0.01163236 (lr:0.00011774968)\n",
      "10280: accuracy:(0.9775846, 0.9775867) loss: 0.00087989547 (lr:0.000117573065)\n",
      "10300: accuracy:(0.9775867, 0.9775888) loss: 0.005425263 (lr:0.00011739822)\n",
      "10300: ********* epoch 18 ********* test accuracy:(0.9775888, 0.97768044) test loss: 6.191307\n",
      "10320: accuracy:(0.97768044, 0.97768253) loss: 0.03699933 (lr:0.00011722509)\n",
      "10340: accuracy:(0.97768253, 0.97768456) loss: 0.008262212 (lr:0.00011705371)\n",
      "10360: accuracy:(0.97768456, 0.9776866) loss: 0.0053428696 (lr:0.00011688401)\n",
      "10380: accuracy:(0.9776866, 0.97768867) loss: 0.000333294 (lr:0.00011671602)\n",
      "10400: accuracy:(0.97768867, 0.9776907) loss: 0.01688102 (lr:0.00011654969)\n",
      "10400: ********* epoch 18 ********* test accuracy:(0.9776907, 0.9777788) test loss: 6.2277393\n",
      "10420: accuracy:(0.9777788, 0.9777808) loss: 0.0106309615 (lr:0.00011638502)\n",
      "10440: accuracy:(0.9777808, 0.97778285) loss: 0.008067693 (lr:0.000116221985)\n",
      "10460: accuracy:(0.97778285, 0.9777848) loss: 0.0019147529 (lr:0.000116060575)\n",
      "10480: accuracy:(0.9777848, 0.97778684) loss: 0.0010808124 (lr:0.000115900766)\n",
      "10500: accuracy:(0.97778684, 0.97778887) loss: 0.009005973 (lr:0.00011574254)\n",
      "10500: ********* epoch 18 ********* test accuracy:(0.97778887, 0.9778761) test loss: 6.21042\n",
      "10520: accuracy:(0.9778761, 0.97787815) loss: 0.00083930773 (lr:0.000115585906)\n",
      "10540: accuracy:(0.97787815, 0.9778801) loss: 0.0064899474 (lr:0.00011543083)\n",
      "10560: accuracy:(0.9778801, 0.9778821) loss: 0.023325633 (lr:0.00011527729)\n",
      "10580: accuracy:(0.9778821, 0.9778841) loss: 0.0017928206 (lr:0.00011512527)\n",
      "10600: accuracy:(0.9778841, 0.9778861) loss: 0.024747921 (lr:0.00011497478)\n",
      "10600: ********* epoch 18 ********* test accuracy:(0.9778861, 0.9779708) test loss: 6.333127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10620: accuracy:(0.9779708, 0.97797275) loss: 0.0071526146 (lr:0.00011482577)\n",
      "10640: accuracy:(0.97797275, 0.9779747) loss: 0.021574652 (lr:0.00011467826)\n",
      "10660: accuracy:(0.9779747, 0.9779767) loss: 0.0047498047 (lr:0.0001145322)\n",
      "10680: accuracy:(0.9779767, 0.97797865) loss: 0.008174587 (lr:0.00011438761)\n",
      "10700: accuracy:(0.97797865, 0.9779806) loss: 0.0022932512 (lr:0.000114244445)\n",
      "10700: ********* epoch 18 ********* test accuracy:(0.9779806, 0.97806543) test loss: 6.299391\n",
      "10720: accuracy:(0.97806543, 0.9780674) loss: 0.0031863502 (lr:0.00011410272)\n",
      "10740: accuracy:(0.9780674, 0.9780693) loss: 0.015051252 (lr:0.00011396239)\n",
      "10760: accuracy:(0.9780693, 0.9780713) loss: 0.0018596286 (lr:0.000113823466)\n",
      "10780: accuracy:(0.9780713, 0.9780732) loss: 0.007048491 (lr:0.000113685914)\n",
      "10800: accuracy:(0.9780732, 0.97807515) loss: 0.0026834467 (lr:0.000113549744)\n",
      "10800: ********* epoch 19 ********* test accuracy:(0.97807515, 0.97816277) test loss: 6.3684874\n",
      "10820: accuracy:(0.97816277, 0.9781647) loss: 0.044399224 (lr:0.00011341491)\n",
      "10840: accuracy:(0.9781647, 0.9781666) loss: 0.0017242043 (lr:0.000113281436)\n",
      "10860: accuracy:(0.9781666, 0.9781685) loss: 0.004984292 (lr:0.00011314928)\n",
      "10880: accuracy:(0.9781685, 0.9781704) loss: 0.0052347807 (lr:0.00011301845)\n",
      "10900: accuracy:(0.9781704, 0.9781723) loss: 0.058483355 (lr:0.00011288891)\n",
      "10900: ********* epoch 19 ********* test accuracy:(0.9781723, 0.97825223) test loss: 6.479519\n",
      "10920: accuracy:(0.97825223, 0.9782541) loss: 0.00015472923 (lr:0.00011276067)\n",
      "10940: accuracy:(0.9782541, 0.978256) loss: 0.0042665787 (lr:0.000112633694)\n",
      "10960: accuracy:(0.978256, 0.97825783) loss: 0.00021885423 (lr:0.00011250798)\n",
      "10980: accuracy:(0.97825783, 0.97825974) loss: 0.013895868 (lr:0.00011238353)\n",
      "11000: accuracy:(0.97825974, 0.97826165) loss: 0.008452438 (lr:0.000112260306)\n",
      "11000: ********* epoch 19 ********* test accuracy:(0.97826165, 0.97834265) test loss: 6.410995\n",
      "11020: accuracy:(0.97834265, 0.9783445) loss: 0.025115386 (lr:0.00011213832)\n",
      "11040: accuracy:(0.9783445, 0.97834635) loss: 0.00078462565 (lr:0.00011201754)\n",
      "11060: accuracy:(0.97834635, 0.9783482) loss: 0.0008454658 (lr:0.00011189796)\n",
      "11080: accuracy:(0.9783482, 0.97835004) loss: 0.0014905396 (lr:0.00011177958)\n",
      "11100: accuracy:(0.97835004, 0.97835195) loss: 0.005281227 (lr:0.00011166237)\n",
      "11100: ********* epoch 19 ********* test accuracy:(0.97835195, 0.97842973) test loss: 6.462922\n",
      "11120: accuracy:(0.97842973, 0.9784316) loss: 0.007890805 (lr:0.000111546324)\n",
      "11140: accuracy:(0.9784316, 0.97843343) loss: 0.012606827 (lr:0.00011143144)\n",
      "11160: accuracy:(0.97843343, 0.9784352) loss: 0.045965858 (lr:0.00011131769)\n",
      "11180: accuracy:(0.9784352, 0.97843707) loss: 0.0007637964 (lr:0.00011120508)\n",
      "11200: accuracy:(0.97843707, 0.9784389) loss: 0.006933782 (lr:0.000111093585)\n",
      "11200: ********* epoch 19 ********* test accuracy:(0.9784389, 0.97851443) test loss: 6.4626293\n",
      "11220: accuracy:(0.97851443, 0.9785163) loss: 0.07872992 (lr:0.00011098321)\n",
      "11240: accuracy:(0.9785163, 0.97851807) loss: 0.004219082 (lr:0.000110873916)\n",
      "11260: accuracy:(0.97851807, 0.9785199) loss: 0.0020255144 (lr:0.00011076572)\n",
      "11280: accuracy:(0.9785199, 0.9785217) loss: 0.00016080929 (lr:0.0001106586)\n",
      "11300: accuracy:(0.9785217, 0.9785235) loss: 0.011044932 (lr:0.000110552544)\n",
      "11300: ********* epoch 19 ********* test accuracy:(0.9785235, 0.978591) test loss: 6.5879846\n",
      "11320: accuracy:(0.978591, 0.9785928) loss: 0.005262645 (lr:0.000110447545)\n",
      "11340: accuracy:(0.9785928, 0.9785946) loss: 0.005687167 (lr:0.00011034359)\n",
      "11360: accuracy:(0.9785946, 0.9785964) loss: 0.0027042637 (lr:0.00011024067)\n",
      "11380: accuracy:(0.9785964, 0.9785982) loss: 0.004780604 (lr:0.000110138775)\n",
      "11400: accuracy:(0.9785982, 0.97859997) loss: 0.0005851444 (lr:0.000110037894)\n",
      "11400: ********* epoch 20 ********* test accuracy:(0.97859997, 0.97867286) test loss: 6.540061\n",
      "11420: accuracy:(0.97867286, 0.9786746) loss: 0.012166582 (lr:0.00010993802)\n",
      "11440: accuracy:(0.9786746, 0.9786764) loss: 0.0011381668 (lr:0.00010983913)\n",
      "11460: accuracy:(0.9786764, 0.97867817) loss: 0.0029893972 (lr:0.000109741224)\n",
      "11480: accuracy:(0.97867817, 0.9786799) loss: 0.025549822 (lr:0.0001096443)\n",
      "11500: accuracy:(0.9786799, 0.9786817) loss: 0.007703153 (lr:0.00010954834)\n",
      "11500: ********* epoch 20 ********* test accuracy:(0.9786817, 0.97875494) test loss: 6.5604024\n",
      "11520: accuracy:(0.97875494, 0.97875667) loss: 0.00041504067 (lr:0.00010945333)\n",
      "11540: accuracy:(0.97875667, 0.9787584) loss: 0.0006387252 (lr:0.000109359265)\n",
      "11560: accuracy:(0.9787584, 0.9787602) loss: 0.002014431 (lr:0.00010926614)\n",
      "11580: accuracy:(0.9787602, 0.9787619) loss: 0.00914829 (lr:0.00010917394)\n",
      "11600: accuracy:(0.9787619, 0.97876364) loss: 0.0008469692 (lr:0.00010908266)\n",
      "11600: ********* epoch 20 ********* test accuracy:(0.97876364, 0.97883314) test loss: 6.5593047\n",
      "11620: accuracy:(0.97883314, 0.97883487) loss: 0.00023255948 (lr:0.00010899229)\n",
      "11640: accuracy:(0.97883487, 0.9788366) loss: 0.0034049947 (lr:0.000108902816)\n",
      "11660: accuracy:(0.9788366, 0.9788383) loss: 0.0068257554 (lr:0.00010881422)\n",
      "11680: accuracy:(0.9788383, 0.97884005) loss: 0.00051561394 (lr:0.00010872653)\n",
      "11700: accuracy:(0.97884005, 0.9788418) loss: 0.012886017 (lr:0.000108639695)\n",
      "11700: ********* epoch 20 ********* test accuracy:(0.9788418, 0.9789117) test loss: 6.563764\n",
      "11720: accuracy:(0.9789117, 0.97891337) loss: 0.003730987 (lr:0.00010855373)\n",
      "11740: accuracy:(0.97891337, 0.9789151) loss: 7.986797e-05 (lr:0.000108468616)\n",
      "11760: accuracy:(0.9789151, 0.97891676) loss: 0.0016387475 (lr:0.00010838435)\n",
      "11780: accuracy:(0.97891676, 0.9789185) loss: 0.0007779764 (lr:0.00010830093)\n",
      "11800: accuracy:(0.9789185, 0.97892016) loss: 0.10141958 (lr:0.00010821833)\n",
      "11800: ********* epoch 20 ********* test accuracy:(0.97892016, 0.9789889) test loss: 6.637119\n",
      "11820: accuracy:(0.9789889, 0.97899055) loss: 0.0026579837 (lr:0.000108136555)\n",
      "11840: accuracy:(0.97899055, 0.9789922) loss: 0.010093677 (lr:0.000108055596)\n",
      "11860: accuracy:(0.9789922, 0.9789939) loss: 0.002658898 (lr:0.000107975444)\n",
      "11880: accuracy:(0.9789939, 0.9789956) loss: 0.0032905468 (lr:0.000107896085)\n",
      "11900: accuracy:(0.9789956, 0.9789973) loss: 0.003443751 (lr:0.00010781752)\n",
      "11900: ********* epoch 20 ********* test accuracy:(0.9789973, 0.9790656) test loss: 6.612016\n",
      "11920: accuracy:(0.9790656, 0.97906727) loss: 0.0020922115 (lr:0.00010773973)\n",
      "11940: accuracy:(0.97906727, 0.9790689) loss: 0.0051713157 (lr:0.00010766272)\n",
      "11960: accuracy:(0.9790689, 0.97907054) loss: 0.011827748 (lr:0.00010758647)\n",
      "11980: accuracy:(0.97907054, 0.9790722) loss: 0.027683796 (lr:0.00010751099)\n",
      "12000: accuracy:(0.9790722, 0.9790739) loss: 0.0009830471 (lr:0.00010743625)\n",
      "12000: ********* epoch 21 ********* test accuracy:(0.9790739, 0.97913706) test loss: 6.719651\n"
     ]
    }
   ],
   "source": [
    "def training_step(i, batch_x, batch_y, x_test, y_test, update_test_data, update_train_data, x_train):\n",
    "\n",
    "    # compute training values for visualisation\n",
    "    if update_train_data:\n",
    "        a, c, l, p, r = sess.run([accuracy, cross_entropy, lr, precision, recall], feed_dict={X: batch_X, Y: batch_Y, step: i})\n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(l) + \")\")\n",
    "        training_acc.append(a)\n",
    "        learn_rate.append(l)\n",
    "        train_cross_ent.append(c)\n",
    "        train_prec.append(p)\n",
    "        train_rec.append(r)\n",
    "        \n",
    "    # compute test values for visualisation\n",
    "    if update_test_data:\n",
    "        a, c, p, r = sess.run([accuracy, cross_entropy, precision, recall], feed_dict={X: x_test, Y: y_test})\n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//x_train.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))\n",
    "        testing_acc.append(a)\n",
    "        test_cross_ent.append(c)\n",
    "        test_prec.append(p)\n",
    "        test_rec.append(r)\n",
    "        \n",
    "\n",
    "    # the backpropagation training step\n",
    "    sess.run(train_step, {X: batch_X, Y: batch_Y, step: i}) \n",
    "\n",
    "epochs = 20\n",
    "iterations = epochs * 600 # an epoch will complete every 600 iterations\n",
    "\n",
    "for i in range(iterations+1): \n",
    "    if (i % 600 == 0):\n",
    "        k = 0\n",
    "        shf = np.random.choice(x_train.shape[0], 60000, replace=False)\n",
    "        n = 100\n",
    "        idx_batches = [shf[j:j + n] for j in range(0, len(shf), n)] \n",
    "    batch_X, batch_Y = x_train[idx_batches[k]], y_train[idx_batches[k]]\n",
    "    k += 1\n",
    "    training_step(i, batch_X, batch_Y, x_test, y_test, i % 100 == 0, i % 20 == 0, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_epoch_test(list_of_accs):\n",
    "    new_list = []\n",
    "    for i in range(0,len(list_of_accs)-1,6):\n",
    "        new_list.append(sum([i for i in list_of_accs][i:i+6])/6)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_epoch_train(list_of_accs):\n",
    "    new_list = []\n",
    "    for i in range(0,len(list_of_accs)-1,30):\n",
    "        new_list.append(sum([i for i in list_of_accs][i:i+30])/30)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-79290feee789>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mget_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn_train_cross_ent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'DNN_Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mget_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cross_ent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'CNN_Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DNN and CNN Cross Entropy Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-686126c77825>\u001b[0m in \u001b[0;36mget_epoch_train\u001b[0;34m(list_of_accs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnew_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnew_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_accs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-686126c77825>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnew_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnew_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_accs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "plt.plot(get_epoch_train(dnn_train_cross_ent), label = 'DNN_Test')\n",
    "plt.plot(get_epoch_train(train_cross_ent), label = 'CNN_Test')\n",
    "plt.legend()\n",
    "plt.title('DNN and CNN Cross Entropy Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(range(0, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.90999997],\n",
       "       [0.8977228 , 0.8938235 ],\n",
       "       [0.8938235 , 0.8875728 ],\n",
       "       ...,\n",
       "       [0.03252339, 0.03252083],\n",
       "       [0.03252083, 0.03251827],\n",
       "       [0.03251827, 0.03251725]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.subtract(1,dnn_training_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x182b244438>,\n",
       "  <matplotlib.axis.XTick at 0x182b240ef0>,\n",
       "  <matplotlib.axis.XTick at 0x182b240ba8>,\n",
       "  <matplotlib.axis.XTick at 0x182b30ab38>,\n",
       "  <matplotlib.axis.XTick at 0x182b3130b8>,\n",
       "  <matplotlib.axis.XTick at 0x182b313518>,\n",
       "  <matplotlib.axis.XTick at 0x182b3139e8>,\n",
       "  <matplotlib.axis.XTick at 0x182b313eb8>,\n",
       "  <matplotlib.axis.XTick at 0x182b313940>,\n",
       "  <matplotlib.axis.XTick at 0x182b30a438>,\n",
       "  <matplotlib.axis.XTick at 0x182b31b518>,\n",
       "  <matplotlib.axis.XTick at 0x182b31b9e8>,\n",
       "  <matplotlib.axis.XTick at 0x182b31beb8>,\n",
       "  <matplotlib.axis.XTick at 0x182b3253c8>,\n",
       "  <matplotlib.axis.XTick at 0x182b325898>,\n",
       "  <matplotlib.axis.XTick at 0x182b325d68>,\n",
       "  <matplotlib.axis.XTick at 0x182b32e278>,\n",
       "  <matplotlib.axis.XTick at 0x182b325e48>,\n",
       "  <matplotlib.axis.XTick at 0x182b31b5c0>,\n",
       "  <matplotlib.axis.XTick at 0x182b32e6d8>],\n",
       " <a list of 20 Text xticklabel objects>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhAUFY4wxERYUjDHGRFhQMMYYE/H/ATSaBjtuL5+NAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dnn_val_error = np.subtract(1,dnn_training_acc)\n",
    "cnn_val_error = np.subtract(1, training_acc)\n",
    "plt.plot(get_epoch_train(dnn_val_error), label = 'DNN_Val')\n",
    "plt.plot(get_epoch_train(cnn_val_error), label = 'CNN_Val')\n",
    "#plt.plot(1-get_epoch_train(dnn_training_acc), label = 'DNN_Train')\n",
    "plt.legend()\n",
    "plt.title('DNN and CNN Validation Errors')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(range(0, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[231.44208,\n",
       " 37.596245,\n",
       " 28.312687,\n",
       " 23.955322,\n",
       " 19.74694,\n",
       " 16.957272,\n",
       " 16.191187,\n",
       " 15.802464,\n",
       " 14.355122,\n",
       " 15.121479,\n",
       " 13.521196,\n",
       " 13.94994,\n",
       " 12.396896,\n",
       " 12.632975,\n",
       " 11.885731,\n",
       " 12.0386715,\n",
       " 11.216082,\n",
       " 11.171538,\n",
       " 11.079141,\n",
       " 10.810454,\n",
       " 10.781751,\n",
       " 10.700941,\n",
       " 11.010537,\n",
       " 10.662947,\n",
       " 10.694799,\n",
       " 10.552916,\n",
       " 10.593338,\n",
       " 10.235792,\n",
       " 10.231449,\n",
       " 9.9508,\n",
       " 9.322928,\n",
       " 10.068743,\n",
       " 10.224859,\n",
       " 9.966866,\n",
       " 10.020269,\n",
       " 10.034309,\n",
       " 9.787824,\n",
       " 9.495302,\n",
       " 9.879574,\n",
       " 9.946131,\n",
       " 9.800985,\n",
       " 9.75963,\n",
       " 9.750882,\n",
       " 9.610172,\n",
       " 9.576997,\n",
       " 9.513669,\n",
       " 9.908118,\n",
       " 9.744646,\n",
       " 9.513958,\n",
       " 10.020612,\n",
       " 9.775442,\n",
       " 9.469998,\n",
       " 9.560353,\n",
       " 9.555538,\n",
       " 9.475026,\n",
       " 9.41777,\n",
       " 9.763625,\n",
       " 9.741418,\n",
       " 9.86803,\n",
       " 9.787024,\n",
       " 9.82249,\n",
       " 9.875655,\n",
       " 9.937802,\n",
       " 9.910905,\n",
       " 9.8686,\n",
       " 9.627584,\n",
       " 9.49172,\n",
       " 9.829611,\n",
       " 9.673227,\n",
       " 9.685626,\n",
       " 9.688829,\n",
       " 9.894509,\n",
       " 9.650685,\n",
       " 9.535175,\n",
       " 9.537003,\n",
       " 9.633533,\n",
       " 9.636129,\n",
       " 9.851536,\n",
       " 9.856202,\n",
       " 9.861621,\n",
       " 9.954188,\n",
       " 9.919897,\n",
       " 9.868355,\n",
       " 9.847136,\n",
       " 9.634909,\n",
       " 9.738376,\n",
       " 9.873119,\n",
       " 9.945397,\n",
       " 9.992166,\n",
       " 10.044545,\n",
       " 9.808685,\n",
       " 9.927677,\n",
       " 9.793943,\n",
       " 10.051674,\n",
       " 9.991765,\n",
       " 10.032411,\n",
       " 9.942983,\n",
       " 9.945456,\n",
       " 9.859761,\n",
       " 9.894467,\n",
       " 9.79174,\n",
       " 9.805656,\n",
       " 9.823548,\n",
       " 9.983762,\n",
       " 10.03621,\n",
       " 10.157896,\n",
       " 10.067734,\n",
       " 9.781167,\n",
       " 9.6740675,\n",
       " 9.662203,\n",
       " 9.84769,\n",
       " 9.80851,\n",
       " 9.800703,\n",
       " 9.89389,\n",
       " 9.943049,\n",
       " 9.996905,\n",
       " 10.035369,\n",
       " 9.992905,\n",
       " 9.972403,\n",
       " 9.962761,\n",
       " 9.914977]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_test_cross_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x1848dee3c8>,\n",
       "  <matplotlib.axis.XTick at 0x1848deea90>,\n",
       "  <matplotlib.axis.XTick at 0x1848deecc0>,\n",
       "  <matplotlib.axis.XTick at 0x18493fedd8>,\n",
       "  <matplotlib.axis.XTick at 0x18494062e8>,\n",
       "  <matplotlib.axis.XTick at 0x18494067b8>,\n",
       "  <matplotlib.axis.XTick at 0x1849406c88>,\n",
       "  <matplotlib.axis.XTick at 0x184940f198>,\n",
       "  <matplotlib.axis.XTick at 0x1849406898>,\n",
       "  <matplotlib.axis.XTick at 0x18493fec50>,\n",
       "  <matplotlib.axis.XTick at 0x184940f240>,\n",
       "  <matplotlib.axis.XTick at 0x184940fc88>,\n",
       "  <matplotlib.axis.XTick at 0x184b59d198>,\n",
       "  <matplotlib.axis.XTick at 0x184b59d668>,\n",
       "  <matplotlib.axis.XTick at 0x184b59db38>,\n",
       "  <matplotlib.axis.XTick at 0x184b5a60b8>,\n",
       "  <matplotlib.axis.XTick at 0x184b5a6518>,\n",
       "  <matplotlib.axis.XTick at 0x184b5a69e8>,\n",
       "  <matplotlib.axis.XTick at 0x184b59d5c0>,\n",
       "  <matplotlib.axis.XTick at 0x18493fe828>],\n",
       " <a list of 20 Text xticklabel objects>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8nFW9+PHPd2aSTNIm6ZqtbbpQ\nwNJKW6hcihtS9otUuAoqKipergub18sFWRSucAXcrlcULyrKolAEkeIPlMqiooi2NIUubK2lpFvS\nJU2zJzPf3x/nTDJJs8xMMpkk832/Xs9rnu3Mcyadzvc55zznHFFVjDHGZK9ApjNgjDEmsywQGGNM\nlrNAYIwxWc4CgTHGZDkLBMYYk+UsEBhjTJazQGBGNRGZIyINQ31uJonIySKyNdP5MNnDAkGWEpGt\nItIsIgdFpE5E/iIinxWRQNw5PxMRFZHj4vbNFRGN235WRFpEZEbcvl5/yESkUkQa4hYVkca47Xcn\n+zlUdYuqjh/qc0eiWCDr5++3dBDvvUdEju/n+Fki8kqq729GNgsE2e39qloIzARuAa4CftLjnH3A\nTQO8TyNw/UAXU9Vtqjo+tvjdC+P2/alnGhEJDvgpskQskPm/3QS/e37c3+/5TObPjF4WCAyqekBV\nVwLnAxeKyIK4w3cDR4vIe/t5i/8FPiIihw02LyJyn4h8X0R+KyKNwLtF5GwRqRKRehHZJiLXx53f\ns4TynIjc6Es4B/37TEr2XH/8U/56e0TkGhGpFpET+8j3gHkUkU/496gVkavjjheIyL0isl9ENgDH\nDuLvVyAi3/PX2Ski3xWRXH+sQkR+50uAe0XkSb//EWAy8LQvWXw+yWtOFpEH/N9pi4h8Ke7YfBH5\ns4gc8J/7p35/SETu8PsO+L/d3FQ/txkcCwSmk6r+DagG4qtomoD/Bm7uJ+l24EfAjUOUlY/69yoE\nngcagAtwd8HvBy4XkbMGSH8hUAqMA/492XNF5O24APdhYBowFSjr530SyeMJwFzgNOBGETnc7/8v\nYAYwBzjT5ydV3wVKgPnAPP96pT/2ZWA9MAUox5f0VPUcYC9wki9Z/CDJa/4IUFzJ8nTgMhE53x+7\nBfgl7u9SCfzY718OLAAOAyYCHwcOJHldM0QsEJiedgCTeuz7P6BSRM7oJ93XgfeLyPwhyMMjqvq8\nqkZVtVVVn1bVDX57HfAA0F8J5Seq+rqqNuF+hBalcO6HgF+r6l9UtRW4rr8MJ5jHG1S1RVVfBDYA\nC/3+84CbVHW/qr4J3N7ftfri7/w/CVzmS3l1wK24YAbQjgtqM1S1TVX/mMp1elyzAPgAcJWqNqrq\na7gA+vG4a84GSlW1WVX/HLd/AnAkoKr6sqrWDjY/JjUWCExP03DtAp38D+HX/NIr/5/4dtzd7WC9\nFb8hIkt9o3StiBwAPoO7q+3Lrrj1JqC/BuK+zq2Iz4eqNgL7+3qTRPKoqn1dq5zun/nNfvLbn+lA\nCHjVV//UAQ/hSgjg/v1qgT+IyGsickWK14lXjisN9Mz/NL9+OVAEVInIOhH5iN//GK7a8UfAbhG5\n3QcVkwEWCEwnEXkH7j/wc70c/inuDu7cft7iG8D7GEQdt9dzSNwHgIdxd7LFuOoFGeQ1BrIT98MK\ngIiMw1Vh9GUwedyFqxqKqUwuq512ABFglqpO8EuxqpYC+BLHpapaiWsPukFE/smnTXUY4p2435Ge\n+d/ur/mWqn4KFzD+HbhHRCrU+aaqLsKVjJYAl6aYBzNIFggMIlLk67MfAO5T1Zd7nqOqHcBXcU8W\n9cpXRXwL+M8hzmIhsE9VW/wjjh8eKMEQ+CXwARE53le5DFTSGUweHwSuEZEJIlIJXJJKhlW1BXeX\n/V3fgCviHtk9GUBElovIbBERXH181C8Au3FtFP0REQnHL75K7VHg6yIyzjf4Xgrc5xN8WETK1Y13\nX4cLOBEROUFEjhWREK59pS0uL2aYWSDIbo+JyEFcsf5a4NvAp/o5/37cHWB/vou7Kx1Kn8P90BwE\nrsH9cKaVqr4EfBEXEHbgGlP3Aq1pyONXcX/XrcATwD2p5RpwP8K1wBrcj/3jdP3Azwf+ABwEngH+\nW1X/7o/dBNzmn1z6XB/vfQTQHL+ISBnwr0AOsA34PXAHsMKneSfworiOfPcDF6nqblzp6h5ccNgC\nbCbFthEzeGIT0xgzMBEpwv1ozVTVtwY635jRxEoExvTB9w0oEJHxuCqvFy0ImLHIAoExfTsHVy1U\nDcwCPtLv2caMUlY1ZIwxWc5KBMYYk+VCmc5AIqZMmaKzZs3KdDaMMWZUWbNmzR5VnTrQeaMiEMya\nNYvVq1dnOhvGGDOqiEhCvdStasgYY7KcBQJjjMlyFgiMMSbLjYo2AmPM2NDe3k51dTUtLS2ZzsqY\nEg6HmT59Ojk5OSmlT2sgEJEJuFEYF+AGm/o08CpuHJJZuLFVzlPVPof3NcaMHdXV1RQWFjJr1izc\n2HdmsFSVvXv3Ul1dzezZs1N6j3RXDX0X+K2qvg031Owm4GrgKVU9HHjKbxtjskBLSwuTJ0+2IDCE\nRITJkycPqpSVtkAgIsXAe/CTofsZkepwU9Td7U+7Gze7kTEmS1gQGHqD/Zums0QwGzcc7k9FZK2I\n/NhP7lGqqrGhjHfh5oo9hIhcLCKrRWR1bW1qM9g9sraa+/6a6mRPxhiTHdIZCELAMcAdqroYaKRH\nNZCfrKLXwY5U9U5VXaKqS6ZOHbBjXK8ef3mXBQJjjBlAOgNBNVCtqi/47YdwgWG3iJQD+NeadGWg\ntCiPXfX2dIIxxgkGgyxatIj58+ezcOFCvvWtbxGNuonRnn32WUSExx57rPP8s846i2effRaAE088\nkSVLlnQeW716NSeeeGKf16qqquLxxx9PKZ91dXX84Ac/SCltKtIWCPxE3W+JyJF+1zJgI7ASuNDv\nuxA3zV1alBWFqWtqp6V9qCfMMsaMRvn5+VRVVbFhwwZWrVrFE088wY033th5fPr06dx88819pq+p\nqeGJJ55I6FqjKRCkux/BpcDP/ZyvW3DTIAaAB0XkIuBN4Lx0Xby0KAxATX0rlZML0nUZY0wKbnxs\nAxt31A/pex5VUcRX3z8/oXNLSkq48847ecc73sENN9wAwMKFC2lvb2fVqlWccsoph6S58sorufnm\nmznjjDP6fe+2tja+8pWv0NzczHPPPceXv/xlzjrrLC699FLWr19Pe3s7N9xwA8uXL2fDhg186lOf\noq2tjWg0ysMPP8z111/P5s2bWbRoEaeccgrf+MY3kv5bJCOtgUBVq4AlvRxals7rxsQCwa76FgsE\nxphDzJkzh0gkQk1NVw31tddey/XXX99rIFi6dCmPPPIIzzzzDIWFhX2+b25uLv/1X//F6tWruf12\nNxXzNddcw0knncRdd91FXV0dxx13HCeffDI//OEPufzyy7ngggtoa2sjEolwyy23sH79eqqqqob+\nQ/diTPcsLit2gWC3tRMYM+Ikeuc+3N7znvcA8Nxzz/V6/LrrruOmm27i1ltvTep9n3zySVauXMk3\nv/lNwPWp2LZtG0uXLuXmm2+murqac889l8MPP3xwHyAFY3qsodJCCwTGmL5t2bKFYDBISUlJt/3X\nXnstN910U69pTjrpJJqbm/nrX/+a1LVUlYcffpiqqiqqqqrYtm0b8+bN46Mf/SgrV64kPz+fM888\nk6effjrlz5OqMR0IivJDhHMCFgiMMYeora3ls5/9LJdccskhHbJOPfVU9u/fz0svvdRr2uuuu47b\nbrut3/cvLCzk4MGDndunnXYa3/ve94hND7x27VrABaM5c+Zw2WWXsXz5cl566aVD0qbbmA4EIkJp\nUZhd9a2ZzooxZgRobm7ufHz05JNP5tRTT+WrX/1qr+dee+21vPXWW70eO/PMMxmof9P73vc+Nm7c\nyKJFi1ixYgXXX3897e3tHH300cyfP5/rr78egAcffJAFCxawaNEi1q9fzyc+8QkmT57MO9/5ThYs\nWMCVV145uA+dgFExef2SJUs01RnKzvu/5wF48N+WDmWWjDEp2LRpE/Pmzct0Nsak3v62IrJGVXt7\nYKebMV0iAPfkkFUNGWNM38b0U0MAZUV5PHmgBVW1wa6MMUPud7/7HVdddVW3fbNnz+aRRx7JUI6S\nN+YDQWlRmNaOKPXNHRQXpDZpgzHG9OW0007jtNNOy3Q2BiUrqoYAG3PIGGP6MOYDgXUqM8aY/o35\nQBDrVGYlAmOM6d2YDwQlRXkA1FggMMaYXo35QBDOCTKhIMdKBMaYUTEfwY4dO/jgBz+YdLrBGPOB\nANy8BLsOWO9iY7LdSJmPoKOjo890FRUVPPTQQwldY6iM+cdHwT05VHPQSgTGjChPXA27Xh7a9yx7\nO5xxS0KnDvd8BJs2bWLz5s1s2bKFyspKvv71r/Pxj3+cxsZGAG6//XZOOOEEtm7dyllnncX69ev5\n2c9+xsqVK2lqamLz5s2cc845A45xlIqsKBGUFuWx64AFAmNMd33NR9DXyKNLly4lNzeXZ555pt/3\njc1HcP7551NVVcX5558PwMaNG/n973/P/fffT0lJCatWreLFF19kxYoVXHbZZb2+V1VVFStWrODl\nl19mxYoVfY5/NBhZUSIoKwqzp6GVjkiUUDArYp8xI1+Cd+7DLV3zEQCcffbZ5OfnA9De3s4ll1xC\nVVUVwWCQ1157rdc0y5Yto7i4GICjjjqKN998kxkzZiR97f5kxa9iSVGYqMKehrZMZ8UYM4IM53wE\nAOPGjetc/853vkNpaSnr1q1j9erVtLX1/vuUl5fXuR4MBvttX0hVVgSCsiLrVGaM6W645yPo6cCB\nA5SXlxMIBLj33nuJRCLJf4ghkhWBwIaZMMZAZucj6Onzn/88d999NwsXLuSVV17pVloYbmN+PgKA\nmoMtHHfzU3xt+Xw+vnTW0GXMGJMUm48gfWw+ggFMHpdHMCBWIjDGmF5kxVNDwYBQUphnncqMMUPO\n5iMYRaxTmTEjw1ibJGokzEcw2Cr+rKgaAutUZsxIEA6H2bt376B/uEwXVWXv3r2Ew+GU3yOtJQIR\n2QocBCJAh6ouEZFJwApgFrAVOE9V96czH+AeIX1+8950X8YY04/p06dTXV1NbW1tprMypoTDYaZP\nn55y+uGoGnqfqu6J274aeEpVbxGRq/32Vb0nHTolRWHqWzpobouQnxtM9+WMMb3Iyclh9uzZmc6G\n6SETVUPLgbv9+t3AB4bjotapzBhjepfuQKDAkyKyRkQu9vtKVXWnX98FlKY5D+6i1qnMGGN6le6q\noXep6nYRKQFWicgr8QdVVUWk11YjHzguBqisrBx0RsqK3XgdViIwxpju0loiUNXt/rUGeAQ4Dtgt\nIuUA/rWmj7R3quoSVV0yUFfuRJRa1ZAxxvQqbYFARMaJSGFsHTgVWA+sBC70p10IPJquPMQbnxei\nIDdoncqMMaaHdFYNlQKP+I4jIeAXqvpbEfk78KCIXAS8CZyXxjx0EhHKisLstk5lxhjTTdoCgapu\nARb2sn8vsCxd1+1PSVEeu61TmTHGdJM1PYsBKxEYY0wvsioQlBaF2V3fat3bjTEmTtYFgraOKHVN\n7ZnOijHGjBhZFQjKiq1TmTHG9JRVgaC0yHUqs0BgjDFdsiwQuBJBjQUCY4zplFWBoKTQVw1ZpzJj\njOmUVYEgNxRg8rhce4TUGGPiZFUgADcvgXUqM8aYLlkXCMqK8qxEYIwxcbIuEJQWha2NwBhj4mRl\nINjb2Ep7JJrprBhjzIiQdYGgrDiMKtQetFKBMcZAFgYC61RmjDHdZWEgsE5lxhgTL2sDwS57hNQY\nY4AsDASTCnLJCQq7rY3AGGOALAwEgYBQUmidyowxJibrAgG4BmPrVGaMMU5WBoKy4rC1ERhjjJeV\ngaCk0E1ZaYwxJksDQVlxmIbWDhpaOzKdFWOMybisDASxTmW7rS+BMcZkayBwfQksEBhjjAWCDOfE\nGGMyL8sDgTUYG2NM2gOBiARFZK2I/MZvzxaRF0TkDRFZISK56c5DT+PzQozPC9kjpMYYw/CUCC4H\nNsVt3wp8R1XnAvuBi4YhD4coLcqzqiFjjCHNgUBEpgP/DPzYbwtwEvCQP+Vu4APpzENfyorDFgiM\nMYb0lwj+B/hPIDYd2GSgTlVjD/BXA9N6SygiF4vIahFZXVtbO+QZK7VOZcYYA6QxEIjIWUCNqq5J\nJb2q3qmqS1R1ydSpU4c4d1BaHKbmYAvRqA75extjzGgSSuN7vxM4W0TOBMJAEfBdYIKIhHypYDqw\nPY156FNpYR7tEWVfUxtTxudlIgvGGDMipK1EoKpfVtXpqjoL+DDwtKpeADwDfNCfdiHwaLry0J+y\nYutLYIwxkJl+BFcB/y4ib+DaDH6Stit1tEH9jl4PlVinMmOMAdJbNdRJVZ8FnvXrW4DjhuO63Hcu\ndLTCZ1YdcqjMOpUZYwww1nsWlx0Nu16CyKGjjE4tzEPE5i42xpixHQgqFkNHC9S+csihnGCAyeOs\nU5kxxoz9QACwY22vh8uKLRAYY8zYDgST5kBeUZ+BoLQwzC5rIzDGZLmxHQgCAShf2HcgKA5TYyUC\nY0yWGzAQiMilIjJxODKTFhWLYfd69yhpD6WFYfY2ttHaEclAxowxZmRIpERQCvxdRB4UkdP9wHGj\nR8ViiLRBzcZDDpUVux7FtQetesgYk70GDASqeh1wOK7j1yeB10Xkv0XksDTnbWj002BsM5UZY0yC\nbQSqqsAuv3QAE4GHROS2NOZtaEycBeEJ/QaCXQesRGCMyV4D9iwWkcuBTwB7cPMKXKmq7SISAF7H\nDTM9com4UkEvgaDMSgTGGJPQEBOTgHNV9c34naoa9UNNj3wVi+Ev/wvtLZAT7tw9oSCH3FDAAoEx\nJqslUjX0BLAvtiEiRSLyTwCquqnPVCNJxWKIdsDuDd12i4hNWWmMyXqJBII7gIa47Qa/b/TobDB+\n8ZBDrlOZBQJjTPZKJBCIbywGXJUQwzRq6ZApng4FU2BH1SGHXKcyayw2xmSvRALBFhG5TERy/HI5\nsCXdGRtS/TQYx0oEcbHOGGOySiKB4LPACbgpJauBfwIuTmem0qJiMdRugrambrvLivNoaovQ0Hro\nUNXGGJMNBqziUdUa3FSTo1vFYtAo7HoZKv+pc3d8p7LCcE6mcmeMMRmTSD+CMHARMB83CT0Aqvrp\nNOZr6MX3MO4lEOw60MrcksJM5MwYYzIqkaqhe4Ey4DTgD8B04GA6M5UWReUwvuyQdgLrVGaMyXaJ\nBIK5qno90KiqdwP/jGsnGH16aTDuLBFYIDDGZKlEAkG7f60TkQVAMVCSviylUcVi2PMatHYVaPJz\ngxSFQzYvgTEmayUSCO708xFcB6wENgK3pjVX6VKxGFDY+VK33aVF1qnMGJO9+m0s9gPL1avqfuCP\nwJxhyVW6VCxyrzvWwqx3du4uKw6z2zqVGWOyVL8lAt+LeGSPLpqM8SVQNL3XdgJrLDbGZKtEqoZ+\nLyL/ISIzRGRSbEl7ztKlYlEvgSCPmoOtRKLWu9gYk30SCQTnA1/AVQ2t8cvqgRKJSFhE/iYi60Rk\ng4jc6PfPFpEXROQNEVkhIrmD+QBJq1gM+zZDc13nrrKiMJGosrfRqoeMMdknkakqZ/eyJNJW0Aqc\npKoLgUXA6SJyPK6h+TuqOhfYj+usNnxiHct2ruvcVRLrS2AzlRljslAiPYs/0dt+Vb2nv3R+xNLY\n8NU5flHgJOCjfv/dwA0M57DW8T2M57wX6N6p7O0UD1tWjDFmJEhkOOl3xK2HgWXAi0C/gQBARIK4\nqqS5wPeBzUCdqsZGeKsGpvWR9mL84HaVlZUJZDNBBZNgwsxu7QTWqcwYk80SGXTu0vhtEZkAPJDI\nm6tqBFjk0zwCvC3RjKnqncCdAEuWLBnaVtwePYynjM8lIFinMmNMVkqksbinRmB2MglUtQ54BlgK\nTBCRWACajhveenhVLIa6N6HJzcAZCgaYWphnJQJjTFYaMBCIyGMistIvvwFexd3dD5Ruqi8JICL5\nwCnAJlxA+KA/7ULg0VQzn7L4dgLP9SWwxmJjTPZJpI3gm3HrHcCbqlqdQLpy4G7fThAAHlTV34jI\nRuABEbkJWAv8JNlMD1r5Qve6Yy3MXQa4QPDWvqZ+EhljzNiUSCDYBuxU1RZwd/ciMktVt/aXSFVf\nAhb3sn8LcFwKeR06+RNg0mE9SgR5/H3rvgxmyhhjMiORNoJfAtG47YjfN7pVLO42mX1ZUZi6pnZa\n2iMZzJQxxgy/RAJBSFXbYht+fXh7A6dDxWKor4aGGqCrU1mNtRMYY7JMIoGgVkTOjm2IyHJgT/qy\nNEw6G4xdqaCzU9lBe3LIGJNdEgkEnwWuEZFtIrINuAr4t/RmaxiUHw1IZztB19zFFgiMMdklkQ5l\nm4HjRWS8324YIMnokFcIU47oDAQ2d7ExJlsl0o/gv0Vkgqo2qGqDiEz0j36OftOO6QwERfkhwjkB\nCwTGmKyTSNXQGb5nMAB+trIz05elYVSxGBp2Qf1ORMQ6lRljslIigSAoInmxDd9LOK+f80ePHj2M\nbe5iY0w2SiQQ/Bx4SkQuEpHPAKtww0ePfqULQILdAoFVDRljsk0ijcW3isg64GTcfAK/A2amO2PD\nIrcASubFNRjnsaq+BVVFRDKcOWOMGR6Jjj66GxcEPoSbWGZT2nI03GJzGKtSWhSmpT1KfXPHwOmM\nMWaM6DMQiMgRIvJVEXkF+B5uzCFR1fep6u3DlsN0q1gMTXvgQHVnXwLrVGaMySb9lQhewd39n6Wq\n71LV7+HGGRpb4hqMy4qtU5kxJvv0FwjOBXYCz4jIj0RkGTD2Ks5LF0AgB3aspbTQOpUZY7JPn4FA\nVX+tqh/GTS/5DHAFUCIid4jIqcOVwbQL5UHpUbBjLSVF7qlYCwTGmGwyYGOxqjaq6i9U9f24qSXX\n4sYbGjv8HMbhUIAJBTnWl8AYk1WSmrNYVfer6p2quixdGcqIisXQUgf7t1JmvYuNMVkmlcnrx564\nBuMS61RmjMkyFggAps6DYJ57cqgozwKBMSarWCAACOVC2QL35FBRmNqDrXREogOnM8aYMcACQUzF\nYti5jtLCXKIKexvbBk5jjDFjgAWCmIrF0FrPnMBuwDqVGWOyhwWCGN9gPL35VcD6EhhjsocFgpgp\nR0IonykHNwAWCIwx2cMCQUwwBOVHk1/7EsGAWKcyY0zWsEAQr2IxsvMlysaHrFOZMSZrpC0QiMgM\nEXlGRDaKyAYRudzvnyQiq0Tkdf86MV15SFrFYmhvZPG4vVY1ZIzJGuksEXQAX1LVo4DjgS+IyFHA\n1cBTqno48JTfHhl8g/GxOf+wQGCMyRppCwSqulNVX/TrB3Gzmk0DltM15/HdwAfSlYekTZ4LueOZ\np5vt8VFjTNYYljYCEZkFLAZeAEpVdac/tAso7SPNxSKyWkRW19bWDkc2IRCE8oXMbH2N+pYOmtvG\n3jw8xhjTU9oDgYiMBx4GrlDV+vhjqqq4uZAP4Uc5XaKqS6ZOnZrubHapWExJw6sEiVj1kDEmK6Q1\nEIhIDi4I/FxVf+V37xaRcn+8HKhJZx6SVrGYYLSVw2W7PUJqjMkK6XxqSICfAJtU9dtxh1YCF/r1\nC4FH05WHlPgG47cHtliJwBiTFUJpfO93Ah8HXhaRKr/vGuAW4EERuQh4EzgvjXlI3sTZaF4RCzs2\nWyAwxmSFtAUCVX2Ovie7H7kznAUCULGIhVv+wa+tU5kxJgtYz+JeSMVi3ibb2FNXP/DJxhgzylkg\n6E3FYnLoILz/1UznxBhj0s4CQW98g/GU+o0ZzogxxqSfBYLeTJhJc7CIma2v4bo6GGPM2GWBoDci\n7C2ez3w2U9fUnuncGGNMWlkg6EPL1KM5QqrZvW9/prNijDFpZYGgDzJtMTkSoXHbukxnxRhj0soC\nQR/yZy5xKzvWZjYjxhiTZhYI+jClYg61WkT+npcznRVjjEkrCwR9yM0JskGOYHbtM7DthUxnxxhj\n0sYCQT9+WvQ5DgSK4Z7l8OoTmc6OMcakhQWCfuRNmcV5HTeyf/xh6AMXwNr7Mp0lY4wZchYI+nH1\nGW9j4tQK3rXri6zPWwiPfgH+9G2wTmbGmDHEAkE/5kwdz68+dwL/efYSPt70JX4TPQGeupHIE1dD\nNJrp7BljzJCwQDCAYEC48IRZPPGlZTx22I3c1XE6wb/9kP33XQgdbZnOnjHGDJoFggSVF+fzfxce\nR8X5/8P3gxcwcctKNn/3TBrqreexMWZ0s0CQpNPfXs4nrvwuj1Rew8z6NWz7zjKeXbMh09kyxpiU\nWSBIQWE4h3M+fRVbT/kRc/QtKh89l2vueoxdB2xqS2PM6GOBYBDmvuuDhD65korcJq548xK+8O27\nuef5rUSi9lSRMWb0sEAwSKFZSwlf/CSTxudzb+AG/t/Kh/iXO/7Cpp02zaUxZnSwQDAUSuYR+tdV\n5E+ewS/yb2Xu3qd5//ee49bfvkJLeyTTuTPGmH7JaJiBa8mSJbp69epMZ2NgTfvgF+eh29fwcNkX\n+Y9/HEthXohFlRM4pnIix86cyKLKCRSFczKdU2NMFhCRNaq6ZMDzLBAMsbZG+OUn4fUnqV54BXfo\nv7BmWx2v7j6IKojAkaWFLPaB4diZE5k1uQARyXTOjTFjjAWCTIq0w8rLYN0vYO4pMOM4mosPY1N7\nOX/eP4G/Vzewdtt+DrZ0ADBpXC7HVE7gmJkTObZyIkdPn0B+bjDDH8IYM9olGghCw5GZrBPMgQ/8\nACbOhLU/hzdWkQ8cAxwjQZg0Gz3yCPYXzObVSAV/a5jCkzXt3LapBoBQQDiqoohjKidyVHkRpcVh\nSgrzKC0KM7Egx0oPxpghlbYSgYjcBZwF1KjqAr9vErACmAVsBc5T1QG75o66EkFPbY2w53XY8xrU\nvgp7XoXa12DfZoh2dJ4WKaygrmA2W2UaLzaX8sd9k3ixvZJG8jvPyQ0GmFqYR0lRXmdwKCnMo6So\nK1hYwDDGwAioGhKR9wANwD1xgeA2YJ+q3iIiVwMTVfWqgd5r1AeCvkTaYd8/fGB4NS5QvA7tje6U\n/Mm8/t7vs7lgEbvrW6g52EoS6/FRAAATg0lEQVSNf91d38Lu+hbqWzoOeeucoFBSGGZqYV7XMj6v\n1+1wjlVDGTMWZTwQ+EzMAn4TFwheBU5U1Z0iUg48q6pHDvQ+YzYQ9CUahfrtULMRfnct7P8HnH4L\nvOMzrrW5h5b2CDX1rdQcbGF3feshAWNPQyu1B1vZ29j7IHlF4VBcgAh3CxgTC3II5wQJ5wTIC8W/\nuvVwTpCcoD2FPNa0dkRobI3Q7B9/Frq+eoIg4vbFDsb2dZ0rCKBAJKpEVemIKtGoe430XFSJRKNE\notARjRL1r6oQCAhBEYKBHku3fRAMBNy+oDsWCEBAhI6I0h6J+qX7ekckSlsf6+2RKFGFcE6A/NwQ\nBTlB8nPdUpAbJD8nth4iPydIMJBYCVxVae2I0twWobGtg+a2CE1+aW7vcOutEZraOmhqj3DBcTMp\nLkjtScOR2kZQqqo7/fouoLSvE0XkYuBigMrKymHI2ggSCMCEGW6pPB5+dTE8/h+wswrO/BbkhLud\nHs4JUjm5gMrJBf2+bXskyr7GNmoPtnYtDa3dttdvP0DtwVYaWg8tZfQlGBDCIRcU8mKvncEjQE4w\nQCgghIIBcoLit916KCiEAgFyQ3HnxJ0bCgiBgCAiBMT9xxb8q9+O/YePnSP4V3+OqqIKUQUltu5u\ngKLxx1RRup8fiUZpaY/S0h6hpSPStd4epaUjQmt7pM/jre0ROqJKTlDIDbm/TW4o0PmaG/SvPdbz\n/HZeTpCcoBAMuL9NMCCHvHY7Fow/5vYrSkNrhIaWDhpbOzjY6l4bWjpoaItbb3VLo39tj4z8h0hG\nmtxQoEeAcDdJLe1R96PeFvE/+h0kM/jAKfNKUw4EiRruEkGdqk6IO75fVScO9D5ZVyLoKRqFZ78O\nf7wNpi2B8++Fooq0XrKprYM9B9vY39RGS3uE1o7Yj5177fcHsKPrtbU9Sns02u2urCOqndsd0bj9\nEXe3OFLFBzwX7AKEQ10lo85SUqgrEIYCQnvE3QG2dbg7zraOSNd25z63tPrt1vZI593pUA9ZMi43\nyPhwiHF5Icb7ZVxeiEL/Oi4vRGE4xDh/9wtdczGpX3chM7bedULn8bjflVjA6rxrj797jwtsgdir\nuKAW8IE9qkokSq+lh/h9sdJGZ6lDFVXtvBHJjbspyfHBNraeEwiQE3I3KbH1UCBAQKClI0pzWwfN\nbdHOu/SW2F18t/WOQ/a3RaLkhYKMy4uVIkLu1QcKt971ty7IDXUGkwK/Hc4JpNzeN1JLBLtFpDyu\naqhmmK8/OgUCcNK1UH40PPJZuPNEOO8eV1pIk4LcEJWTQwOWMoaa+iqEjoh2BhBV7bxjj/q7eQWi\n0a67+2iPcxQlGnXHYqWGWPWFK8F3lRoCcVUb0mNfIAD5/kc+U1Vgsc/UEY0Sif3I+aAZq3Jx23HH\no11BtevHPsi43BCBBKswTPYY7kCwErgQuMW/PjrM1x/d5r0fJs+FBz4KPzsLzvwGLPlUpnM1pETE\nVx9BPtaIDe5vEhQIBuzvYdIjbbc4InI/8DxwpIhUi8hFuABwioi8Dpzst00ySubBvz4Nc94Lv7kC\nHrvCZkozxgxK2koEqvqRPg4tS9c1s0b+RPjog/D01+C577ini867BwrLMp0zY8woZM/9jVaBIJx8\nA3zwp7DrZdduUL0mw5kyxoxGFghGuwXnwkWrIJgLPz0d1t6X6RwZY0YZCwRjQdkCuPhZqFwKj34B\nHr/S9Vo2xpgEWCAYKwomwcd+BUsvgb/dCfcsh4baTOfKGDMKWCAYS4IhOO1mOPdHsH2Nazd47UnY\n84abNCcazXQOjTEjkA1DPRYdfR5MOQJWfAx+8aGu/RKEgsluGTelx/oUV6ro3O9fQ7mZ+xzGmGFh\ngWCsqlgEn/szVP8dGvdC015o2gONe/z6Xti9wb0278cPFHCoCZUw72yYfw5MO7bXQe+MMaObBYKx\nLFwMc08e+LxIhwsGsWDRtLcrYGxf49ocnr8dimfAUcstKBgzxlggMK5tYfxUt/SmuQ5efQI2/hpe\n+D8LCsaMMTZnsUlOfFB44ymItkNxJRxl1UfGjDQjYmKaoWKBYISyoGDMiGaBwAyvWFDY8Ahsfror\nKMw8AYI5IIF+Fun7WN54mHSYG3V14ix7ismYJIzU+QjMWJU/ARZ9xC3NdfDq47Dh17D1OdBoH4v2\ncyzKIU8ySQAmzHRBYfJcmHxY13rRNDdvgzEmaRYIzNDLnwCLPuqWwVCFljrYuwX2vhG3vA5v/hna\nm7rODYV9ySEuOEyeC4WlEI1ApM0NuxFpd+vR9kP3dTvm10NhmHokTJ0H40tGT1VX60E4UA3tzTD1\nbZA7vBMMmdHFAoEZuUTckNvTj3VLPFU4uDMuOGx2rzUbXWkkmvicywnLnwQlR0HJ29y8EFPnudeC\nSUN/rf50tEH9drcc2A4H3vLr1X67GloPdJ0vQRfMyhe5/iXli6Ds7RYcTCdrIzBjT6Qd6ra5wNBQ\n40ZmDeb4JbfrNRC/L9c9Rtu57ve3NkDtK1CzyQWZ2Hprfdf1xpf54HBUV4CYeiSEi/rOYzQKHc3Q\n1gTtje7Ovdu6f22pO/THvqGGQ6rN8idB8fSupWiaew3muGHKd1TBzipo9ONPSQCmHAkViy04jGHW\nWGxMuqhC/Y4ewWEj1Lziftxjime4H+P2Zr80df3Ax583kJyC7j/uva0n8gMey/fOqq7AsKMKGv3U\n4Z3BYVFX6WHqkRDMg0DIN/qPkqqxkUAVOlogJz9jWbBAYMxwi0ah7s3uAaJ+p/shyC2AnHG9rI9z\nP/Td1gv8OQWQV+iqx9L1AxyrYosPDDuroGF37+dLwAWFQMiVqALBriARW48/Fsx1n63bZ8w/9HMe\ncty/houhsHxkPggQjULDLlf6rHsLDmzrWq/b5kpvHc3uabf4arnyhcNWnWiBwBiTuvqdLiDs3ewa\nz6MdvtE9tu63oz22ex6PtEJ7i6vyamvyJaNYqaglsbwE82DSbP8wwByYNKfrwYDCivQFibYmV1qq\ne8tVzXX+wPsf/APb3eePVzDZjc9VPMO95hW6Mb12Vrk0MRMqewSHRTBu8pB/BHt81BiTuqJyt6RT\nNNJVZdbedGigaGt0Y2Dt/4d7cmzfZnjj9y64xITCMHG2CwqT5nS9Tjqse0mit/G0mvbGDcgYNzBj\n0z63Hv9UWsz4MvcjPu1YOOoDbj22FE93JZq+NO2Dneu6l7w2rew6XjzDlRYqFkH5Yrfe17AvQ8xK\nBMaY0SMadY3m+za70sq+LW7Zu9kFjEhb17mhfPf4cHOda3TvS16Rq6qJDcseG5I9NkR77O6+eDqE\n8ob28zTvh50vdQ8O+7Z0HS+aBh972D2EkAIrERhjxp5AACbMcMucE7sfi0ZckNi72QeKLa6tI39i\n3Dwbk7rm2ohtD/WPezLyJ8Kc97olpuVA9+BQNC3t2bBAYIwZGwLBrmqaw96X6dykLlwMs9/tlmEy\nApvijTHGDCcLBMYYk+UsEBhjTJbLSCAQkdNF5FUReUNErs5EHowxxjjDHghEJAh8HzgDOAr4iIgc\nNdz5MMYY42SiRHAc8IaqblHVNuABYHkG8mGMMYbMBIJpwFtx29V+XzcicrGIrBaR1bW1tcOWOWOM\nyTYjtrFYVe9U1SWqumTq1OHpZm2MMdkoEx3KtgMz4ran+319WrNmzR4ReTPF600B9qSY1tJbektv\n6Udz+pkJnaWqw7rggs8WYDaQC6wD5qfxeqstvaW39JY+G9Mnugx7iUBVO0TkEuB3QBC4S1U3DHc+\njDHGOBkZa0hVHwcez8S1jTHGdDdiG4uH0J2W3tJbekufpekTMirmIzDGGJM+2VAiMMYY0w8LBMYY\nk+XGdCAYzOB2InKXiNSIyPoUrz1DRJ4RkY0iskFELk8yfVhE/iYi63z6G1PIQ1BE1orIb5JN69Nv\nFZGXRaRKRJKeK1REJojIQyLyiohsEpGlSaQ90l83ttSLyBVJXv+L/m+3XkTuF5Fwkukv92k3JHLt\n3r4zIjJJRFaJyOv+dWKS6T/krx8VkX6nHOwj/Tf83/8lEXlERCYkmf5rPm2ViDwpIhXJpI879iUR\nURGZkuT1bxCR7XHfgzOTvb6IXOr/BhtE5LYkr78i7tpbRaQqyfSLROSvsf9DInJckukXisjz/v/h\nYyJS1Ff6QRmOZ1QzseAeTd0MzKGrv8JRSaR/D3AMsD7F65cDx/j1QuC1JK8vwHi/ngO8AByfZB7+\nHfgF8JsUP8NWYMog/g3uBj7j13OBCYP4t9wFzEwizTTgH0C+334Q+GQS6RcA64EC3NN1vwfmJvud\nAW4DrvbrVwO3Jpl+HnAk8CywJIXrnwqE/PqtKVy/KG79MuCHyaT3+2fgHhd/s7/vUx/XvwH4jwT/\nzXpL/z7/b5fnt0uSzX/c8W8BX0ny+k8CZ/j1M4Fnk0z/d+C9fv3TwNcS/Q4ns4zlEsGgBrdT1T8C\n+1K9uKruVNUX/fpBYBO9jKnUT3pV1Qa/meOXhFv2RWQ68M/AjxPO9BASkWLcF/snAKrapqr9zCDe\nr2XAZlVNtnd5CMgXkRDuB31HEmnnAS+oapOqdgB/AM7tL0Ef35nluICIf/1AMulVdZOqvppIhvtI\n/6TPP8BfcT35k0lfH7c5jn6+g/38n/kO8J/9pR0gfUL6SP854BZVbfXn1KRyfRER4Dzg/iTTKxC7\niy+mn+9gH+mPAP7o11cB/9JX+sEYy4EgocHthoOIzAIW4+7qk0kX9EXRGmCVqiaT/n9w//miyVyz\nBwWeFJE1InJxkmlnA7XAT3311I9FZFyK+fgw/fwH7I2qbge+CWwDdgIHVPXJJN5iPfBuEZksIgW4\nu7kZA6TpTamq7vTru4DSFN5jqHwaeCLZRCJys4i8BVwAfCXJtMuB7aq6LtnrxrnEV0/d1V/VWh+O\nwP07viAifxCRd6SYh3cDu1X19STTXQF8w//9vgl8Ocn0G+i6gf0QqX0HBzSWA8GIICLjgYeBK3rc\nXQ1IVSOqugh3F3eciCxI8JpnATWquibpDHf3LlU9Bjd3xBdE5D1JpA3hirl3qOpioBFXNZIUEckF\nzgZ+mWS6ibj/QLOBCmCciHws0fSquglXlfIk8FugCogkk4de3lNJolQ3lETkWqAD+HmyaVX1WlWd\n4dNeksQ1C4BrSDJ49HAHcBiwCBfQv5Vk+hAwCTgeuBJ40N/dJ+sjJHkz4n0O+KL/+30RX0JOwqeB\nz4vIGlwVc1sKeRjQWA4ESQ9uN9REJAcXBH6uqr9K9X18lcozwOkJJnkncLaIbMVViZ0kIvelcN3t\n/rUGeARX3ZaoaqA6rhTzEC4wJOsM4EVV3Z1kupOBf6hqraq2A78CTkjmDVT1J6p6rKq+B9iPa+dJ\n1m4RKQfwr31WTaSLiHwSOAu4wAejVP2c5KomDsMF4nX+uzgdeFFEyhJ9A1Xd7W+IosCPSO47CO57\n+Ctf1fo3XAm5zwbr3viqxXOBFUleG+BC3HcP3M1MUvlX1VdU9VRVPRYXiDankIcBjeVA8HfgcBGZ\n7e8qPwysHK6L+7uOnwCbVPXbKaSfGnvCQ0TygVOAVxJJq6pfVtXpqjoL97mfVtWE74b9NceJSGFs\nHdfomPATVKq6C3hLRI70u5YBG5PJg5fqndg24HgRKfD/Fstw7TQJE5ES/1qJ+yH4RQr5WIn7McC/\nPprCe6RMRE7HVRGerapNKaQ/PG5zOQl+BwFU9WVVLVHVWf67WI17gGJXEtcvj9s8hyS+g96vcQ3G\niMgRuIcWkh3N82TgFVWtTjIduDaB9/r1k4CkqpbivoMB4DrghynkYWDpaIEeKQuuXvc1XBS9Nsm0\n9+OKou24L/BFSaZ/F64a4CVctUIVcGYS6Y8G1vr06+nnaYUB3udEUnhqCPe01Tq/bEj27+ffYxGw\n2n+GXwMTk0w/DtgLFKf42W/E/XCtB+7FPzmSRPo/4YLXOmBZKt8ZYDLwFO4H4PfApCTTn+PXW4Hd\nwO+STP8Grq0s9h3s76mf3tI/7P9+LwGPAdNS/T/DAE+h9XH9e4GX/fVXAuVJps8F7vOf4UXgpGTz\nD/wM+GyK//7vAtb479ALwLFJpr8c9xv2GnALfjSIoV5siAljjMlyY7lqyBhjTAIsEBhjTJazQGCM\nMVnOAoExxmQ5CwTGGJPlLBAYA4hIRLqPdpp0L+h+3ntWbyNyGjNSZGTOYmNGoGZ1w3kYk3WsRGBM\nP/wY9Lf58eD/JiJz/f5ZIvK0HwztKd/7GBEpFTfu/zq/xIa1CIrIj/yY+E/63uLGjAgWCIxx8ntU\nDZ0fd+yAqr4duB03qivA94C7VfVo3Bg8/+v3/y/wB1VdiBtbaYPffzjwfVWdD9SRpuGEjUmF9Sw2\nBhCRBlUd38v+rbhhCbb4QQR3qepkEdmDG+6g3e/fqapTRKQWmK5+/Hv/HrNww4gf7revAnJU9ab0\nfzJjBmYlAmMGpn2sJ6M1bj2Ctc+ZEcQCgTEDOz/u9Xm//hfcyK7gJmz5k19/CjcGfWxioeLhyqQx\nqbK7EmOc/B4Tk/9WVWOPkE4UkZdwd/Uf8fsuxc2+diVuJrZP+f2XA3eKyEW4O//P4UaUNGbEsjYC\nY/rh2wiWqGqyY9gbM2pY1ZAxxmQ5KxEYY0yWsxKBMcZkOQsExhiT5SwQGGNMlrNAYIwxWc4CgTHG\nZLn/D75qVuOYDdtXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(get_epoch_test(dnn_test_cross_ent), label = 'DNN_test')\n",
    "plt.plot(get_epoch_train(dnn_train_cross_ent), label = 'DNN_train')\n",
    "#plt.plot(1-get_epoch_train(dnn_training_acc), label = 'DNN_Train')\n",
    "plt.legend()\n",
    "plt.title('DNN Training and Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(range(0, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([<matplotlib.axis.XTick at 0x184b5e4240>,\n",
       "  <matplotlib.axis.XTick at 0x184b5d5e10>,\n",
       "  <matplotlib.axis.XTick at 0x184b5d58d0>,\n",
       "  <matplotlib.axis.XTick at 0x184b60e2e8>,\n",
       "  <matplotlib.axis.XTick at 0x184b60e7b8>,\n",
       "  <matplotlib.axis.XTick at 0x184b60ec88>,\n",
       "  <matplotlib.axis.XTick at 0x184b616198>,\n",
       "  <matplotlib.axis.XTick at 0x184b616668>,\n",
       "  <matplotlib.axis.XTick at 0x184b60e898>,\n",
       "  <matplotlib.axis.XTick at 0x184b603898>,\n",
       "  <matplotlib.axis.XTick at 0x184b616240>,\n",
       "  <matplotlib.axis.XTick at 0x184b688198>,\n",
       "  <matplotlib.axis.XTick at 0x184b688668>,\n",
       "  <matplotlib.axis.XTick at 0x184b688b38>,\n",
       "  <matplotlib.axis.XTick at 0x184b68e0b8>,\n",
       "  <matplotlib.axis.XTick at 0x184b68e518>,\n",
       "  <matplotlib.axis.XTick at 0x184b68e9e8>,\n",
       "  <matplotlib.axis.XTick at 0x184b68eeb8>,\n",
       "  <matplotlib.axis.XTick at 0x184b68e5c0>,\n",
       "  <matplotlib.axis.XTick at 0x184b6882e8>],\n",
       " <a list of 20 Text xticklabel objects>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcXGWd7/HPr6uqq9es3WkgISQs\nsnSAABFQcANZRQPOjOKgA4MKel1AvQqIzugdroKDgw7OZSYjKio6MAiyjSOIARxFJAlhDbImpANJ\nOgtJOp1eqvp3/3hOdao7vVR1uro6fb7v16teZ33OebpSOb/zLOc55u6IiEh8VZQ7AyIiUl4KBCIi\nMadAICIScwoEIiIxp0AgIhJzCgQiIjGnQCB7LDNLmFmbmc0ezX3LycwONDP16ZYxpUAQY2b212a2\nJLpAvm5mvzKzE6NtXzMzN7MP5O2fjNbNiZZ/FC0fm7fPoBey6Dy5T4+Z7chbPq/Y/Lt71t3r3P3V\n0dx3PMoLZIN9fx/cjWMvMbNzh9g+z8w6Rnp8Gf8UCGLKzD4PfAf4BtAEzAb+H7Awb7dNwNfNLDHE\noTYBVxVyzuhCXOfudcCrwHvz1t08QB6Thf01E19eIMt9f68BZ+Stu6XceZQ9lwJBDJnZZOD/AJ9y\n99vdfbu7d7v73e7+xbxd/xvoAj48xOFuAo4ws3eMQr6uMrNbzOznZrYN+LCZvcXM/mhmb0Slln82\ns1S0f/8Syk+j7b8ys21m9oiZzS1232j7GWb2vJltMbPrzez3ZnbBIPkuJI8Xm9mLZrbZzP45L23C\nzK4zs41m9jJw+m58f8moJPeKmW0ws5+Y2aRoW52Z3Wpmm6I8/NHMJpvZd4CjgB9FJYtrijxnjZnd\nYGZrzazFzK7J+9v3MbNfR9/LRjO7Ly/d16PvaquZrTCzt47075bdp0AQT28BqoA7htnPga8Cf5/7\nzz2AdkKp4v+OUt7OAX4GTAZuATLAJUADcALhQnnxEOn/OsrzNEKp4x+K3dfMZgC3Al+MzvsKcOwg\nx6DAPJ4JHEO46H7YzN4drf8kcCpwJPBm4AOM3GXAOwn/vvtG674dTS8m/HvuAzQCnwW63P1S4HHg\ngqhkcVmR57wKOBRojvJ/CvD5aNsVwNOE72XvaF/M7BjCd38E4d/5LGBNkeeVUaRAEE/TgQ3unhlu\nR3e/C2gFPjbEbv8GzDazM0Yhb/8TlUx63H2Huz/m7o+6e8bdXwYWAUOVPm5z9yXu3g3cDMwfwb5n\nAcvd/c5o23XAhsEOUmAev+nuW9x9JfBg3rk+AFzn7i3uvhG4eoj8DucTwGXuvtbddxBKfbm6/25C\nANg/yuefon1213nA37n7Rnd/nXBD8JG8c84E9nX3Lnd/OFqfAWoIwSPh7i+5+6pRyIuMkAJBPG0E\nGoqog/8KcCWhFLELd+8k3E0PdfddqNX5C2Z2iJndG1U9bCVc3BqGSL82b74dqBvBvvvk58PDyIwt\ngx2kwDwWdC5gRBfEqB1nJnBfVBXzBvAYkDKzKYTg9AfgDjNbHVXD7db//+j3M6NfnldF+YDwe2gF\nHoqq2S4FcPcnCL+pbwLroyqsxt3Ji+weBYJ4egToBM4uZGd3vx94EfhfQ+z2Q2AK8P7dzFv/Hkf/\nRqheONDdJwF/B9hunmM4rwOzcgtmZuy8uA1kd/L4OjurcSA02hfN3bPRsd7u7lPyPlXu/oa7d7j7\nV9z9YOBdhKqZv8olH+E5M8B6YL9++V8Tbd/s7p9x99nAB4Gvmdlx0bYfuvtbgAOAekLwlDJRIIgh\nd99CuFj9i5mdHTX4paIG0m8NkuxK4EtDHDMD/D2hnno01QNbgO1mdihDtw+MlnuAo83svdFd7yWE\napVS5PFW4FIzm2lm09m97+9fgWvMbCaAmTWZ2VnR/ClmdmhUCthKqJ7pidKtA/Yf7uBmVtXvY8DP\nCRf4aWa2F/Bl4KfR/gvNbG6035bofD0WuqO+3czShNJRR15epAwUCGLK3b9NaNT7CqH4vhr4NPDL\nQfb/PfCnYQ77c8Jd6Wj6AnA+sI1w513ybpLuvo5wB/tPhGq0AwgNqp0lyOMNwAPAU4SqnNtGlmsg\nVLU8TKiK2Qr8D6FxGkKp4+4oj08AdwK/iLZ9G/hoVKX0zUGOnQZ29PscR7hBeAFYASwFFrOzgboZ\neCg652LgG+7+GFBNaHfZSPi9pAk3EVImphfTiAwtqn9/DfhLd/9dufMjMtpUIhAZgJmdbmZTouqL\nrxJ6wAxXIhLZIykQiAzsROBlQrXZacA5Ue8okQlHVUMiIjGnEoGISMztEYN6NTQ0+Jw5c8qdDRGR\nPcrSpUs3uPuwD+vtEYFgzpw5LFmypNzZEBHZo5hZQU+qlzQQmNlKQh/iLJBx9wVmNo3Qz3oOsBL4\ngLtvLmU+RERkcGPRRvAud5/v7gui5cuBB9z9IMKDNJePQR5ERGQQ5WgsXkgYw55oWtB4NyIiUhql\nbiNwwmiIDvybuy8CmqLhaiGMyNg0UEIzuwi4CGD27HH9mlkRKVB3dzctLS10dOjNl6OpqqqKWbNm\nkUoN9tqQoZU6EJzo7muiF33cb2bP5W90d7dB3m8bBY1FAAsWLNDDDiITQEtLC/X19cyZM4cwFp3s\nLndn48aNtLS0MHfu3OETDKCkVUPunhuOdj3hbVjHAuvMbG+AaLq+lHkQkfGjo6OD6dOnKwiMIjNj\n+vTpu1XKKlkgMLNaM6vPzRNex/c0cBdhpEai6Z2lyoOIjD8KAqNvd7/TUlYNNRHehpQ7z8/c/b/N\n7DHgVjP7KOFtRrvzjtYh3fF4C9s7s3z4+P2G31lEJKZKFgiid7ceOcD6jcDJpTpvvnufXEvL5nYF\nAhGRIUzosYYa69O0btOAkSLS19q1azn33HM54IADOOaYYzjzzDN5/vnnMTOuv/763v0+/elP86Mf\n/QiACy64gJkzZ9LZGa4pGzZsYKihb1auXMnPfvazEefxG9/4xojTFmvCB4JN7V1ksnoLnogE7s45\n55zDO9/5Tl566SWWLl3KN7/5TdatW8eMGTP47ne/S1dX14BpE4kEP/jBDwo6z54UCPaIsYZGqrGu\nEnfYtL2LGZOqyp0dEcnz9buf4dnXto7qMQ/bZxJ//97mIfdZvHgxqVSKT3ziE73rjjzySFauXElj\nYyMnnHACN910Ex//+Md3SXvppZdy3XXXDbitv8svv5wVK1Ywf/58zj//fD772c9y+eWX8+CDD9LZ\n2cmnPvUpLr74Yl5//XU++MEPsnXrVjKZDDfccAP33nsvO3bsYP78+TQ3N3PzzTcX/2UUYWIHgvo0\nAK1tnQoEIgLA008/zTHHHDPo9ssuu4wzzjiDCy+8cJdts2fP5sQTT+QnP/kJ733ve4c8z9VXX821\n117LPffcA8CiRYuYPHkyjz32GJ2dnZxwwgmceuqp3H777Zx22mlceeWVZLNZ2tvbedvb3sb3vvc9\nli9fvnt/bIEmdCBoqIsCgdoJRMad4e7cy2X//ffnuOOOG7Ra54orrmDhwoW85z3vKeq49913H08+\n+SS33XYbAFu2bOGFF17gzW9+MxdeeCHd3d2cffbZzJ8/f7f/hmJN+DYCUCAQkZ2am5tZunTpkPt8\n+ctf5pprrmGgNzgedNBBzJ8/n1tvvbWo87o7119/PcuXL2f58uW88sornHrqqbz97W/n4YcfZubM\nmVxwwQX8+Mc/Luq4o2FCB4JciWBD28ANPyISPyeddBKdnZ0sWrSod92TTz7J6tWre5cPOeQQDjvs\nMO6+++4Bj3HllVdy7bXXDnme+vp6tm3b1rt82mmnccMNN9Dd3Q3A888/z/bt21m1ahVNTU18/OMf\n52Mf+xjLli0DIJVK9e5bahM6ENSmk9RUJlQiEJFeZsYdd9zBb37zGw444ACam5u54oor2Guvvfrs\nd+WVV9LS0jLgMZqbmzn66KOHPM8RRxxBIpHgyCOP5LrrruNjH/sYhx12GEcffTTz5s3j4osvJpPJ\n8OCDD3LkkUdy1FFHccstt3DJJZcAcNFFF3HEEUdw3nnnjc4fPoQ94uX1CxYs8JG+oewd/7iYI2ZN\n4foPHTXKuRKRYq1YsYJDDz203NmYkAb6bs1sad67YAY1oUsEAI11aTaoRCAiMqgJ3WsIQjvBi61t\n5c6GiExQTz31FB/5yEf6rEun0zz66KNlylHxJnwgaKxP88dXNpY7GyIyQR1++OFj1t+/VCZ81VBD\nXZo32rvpzGTLnRURkXFpwgeC3LMEG9WFVERkQLEJBBva1GAsIjKQCR8IGuoqAT1dLCIymAkfCFQi\nEJH+xvv7CN761reOKN1ITfhAoIHnRCTfeHgfQSaTGTLtH/7wh4LOMVomfPfRqlSC+qqkAoHIePOr\ny2HtU6N7zL0OhzOuHnKXcr2PYOrUqdx+++20tbWRzWa59957WbhwIZs3b6a7u5urrrqKhQsXAlBX\nV0dbWxsPPvggX/va12hoaOgdPvunP/3pbr+svr8JXyKAUD2kgedEBAp7H8G1115LNrtrl/P89xEM\n5+qrr+Ztb3sby5cv53Of+xwAy5Yt47bbbuOhhx6iqqqKO+64g2XLlrF48WK+8IUvDDja6eOPP853\nvvMdnn32WV5++WV+//vfF/HXFmbClwggVA+pRCAyzgxz514upXofAcApp5zCtGnTgFBF9eUvf5mH\nH36YiooK1qxZw7p163YZ/O7YY49l1qxZAMyfP5+VK1dy4oknFn3uocSmRNCqxmIRoXzvIwCora3t\nnb/55ptpbW1l6dKlLF++nKamJjo6OnZJk06ne+cTicSw7QsjEY9AoIHnRCRSrvcR9LdlyxZmzJhB\nKpVi8eLFrFq1qsi/ZPTEIxDUp9nWmaGjW8NMiMRdud5H0N95553HkiVLOPzww/nxj3/MIYccMvI/\najdN+PcRANz62Gq+9Isn+d2X3sW+02pGMWciUgy9j6B09D6CYfS+u1jtBCIiu4hNryHQQ2UiMvr0\nPoI9hIaZEBk/3H3UH4gqp/HwPoLdreKPRdXQdA08JzIuVFVVsXHjxt2+cMlO7s7GjRupqqoa8TFi\nUSJIJSqYWpNSiUCkzGbNmkVLSwutra3lzsqEUlVV1fvQ2UjEIhBA9FCZSgQiZZVKpZg7d265syH9\nxKJqCDTMhIjIYEoeCMwsYWaPm9k90fJcM3vUzF40s1vMrLLUeQANPCciMpixKBFcAqzIW74GuM7d\nDwQ2Ax8dgzz0lgjUSCUi0ldJA4GZzQLeA3w/WjbgJOC2aJebgLNLmYecxvo0O7qzbO/SMBMiIvlK\nXSL4DvAloCdang684e654fNagJkDJTSzi8xsiZktGY0eBo3RQ2UafE5EpK+SBQIzOwtY7+5Dj/c6\nCHdf5O4L3H1BY2PjbuenQcNMiIgMqJTdR08A3mdmZwJVwCTgu8AUM0tGpYJZwJoS5qGXSgQiIgMr\nWYnA3a9w91nuPgc4F/itu58HLAb+MtrtfODOUuUhnwaeExEZWDmeI7gM+LyZvUhoM7hxLE46rbaS\nCtMwEyIi/Y3Jk8Xu/iDwYDT/MnDsWJw3X6LCmFab1jATIiL9xObJYoCGukqVCERE+olVINB4QyIi\nu4pdINAwEyIifcUrEGiYCRGRXcQrENSn6cr2sLUjM/zOIiIxEbtAAOpCKiKSL1aBQC+xFxHZVawC\ngV5iLyKyq1gFApUIRER2FatAMKU6RbLCVCIQEckTq0BQUWF6d7GISD+xCgQADfWVGoFURCRP7AJB\nY50GnhMRyRe/QKDxhkRE+ohdIGioC+MN9fRomAkREYhhIGisT5Ptcd7Y0V3urIiIjAuxCwR6lkBE\npK/YBQI9XSwi0ldsA4FKBCIiQewCgaqGRET6il0gmFSVpDJZoaohEZFI7AKBmfW+qUxERGIYCAAa\n6tMaZkJEJBLLQKASgYjITvEMBPWVaiMQEYnEMxDUpdm0vYushpkQEYlpIKhP0+OwcbtKBSIisQwE\nepZARGSnWAaCncNMdJU5JyIi5RfLQKASgYjITrEMBBp4TkRkp5IFAjOrMrM/mdkTZvaMmX09Wj/X\nzB41sxfN7BYzqyxVHgZTm05SU5lQiUBEhNKWCDqBk9z9SGA+cLqZHQ9cA1zn7gcCm4GPljAPg2rQ\nQ2UiIkAJA4EHbdFiKvo4cBJwW7T+JuDsUuVhKI31eom9iAiUuI3AzBJmthxYD9wPvAS84e6ZaJcW\nYOYgaS8ysyVmtqS1tXXU86ZhJkREgpIGAnfPuvt8YBZwLHBIEWkXufsCd1/Q2Ng46nlr0DATIiLA\nGPUacvc3gMXAW4ApZpaMNs0C1oxFHvprrKtic3s3XZmecpxeRGTcGDYQmNlnzGxqsQc2s0YzmxLN\nVwOnACsIAeEvo93OB+4s9tijoaE+dFbSMBMiEneFlAiagMfM7FYzO93MrMBj7w0sNrMngceA+939\nHuAy4PNm9iIwHbhxJBnfXY3RQ2UbtunpYhGJt+RwO7j7V8zsq8CpwN8C3zOzW4Eb3f2lIdI9CRw1\nwPqXCe0FZdX7Evu2DmByeTMjIlJGBbURuLsDa6NPBpgK3GZm3yph3kpKw0yIiATDlgjM7BLgb4AN\nwPeBL7p7t5lVAC8AXyptFktDA8+JiATDBgJgGvB+d1+Vv9Lde8zsrNJkq/SqUgnqq5IqEYhI7BVS\nNfQrYFNuwcwmmdlxAO6+olQZGwuNdXqJvYhIIYHgBqAtb7ktWrfHa6jX08UiIoUEAosai4FQJURh\nVUrl98rv4Ln/GnRzY12aDQoEIhJzhQSCl83ss2aWij6XAC+XOmOj4pHvwW//YdDNjfWqGhIRKSQQ\nfAJ4K2EoiBbgOOCiUmZq1DQ1w4bnITPwxb6xPs22jgwd3dkxzpiIyPhRyANl64FzxyAvo6+pGXoy\nIRjsdfgumxvqwjATrds62XdazVjnTkRkXCjkOYIqwstjmoGq3Hp3v7CE+RodM5rDdN0zAwaC/FdW\nKhCISFwVUjX0E2Av4DTgIcKIodtKmalRM/1ASFSGQDCAxroQ19RzSETirJBAcKC7fxXY7u43Ae8h\ntBOMf4kkNB4yaCDIjUCqp4tFJM4KCQTd0fQNM5tHGKFtRumyNMqa5g0aCKbXarwhEZFCAsGi6H0E\nXwHuAp4lvIB+z9DUDG1rYfuGXTZVJiuYUpOKRiAVEYmnIRuLo4Hltrr7ZuBhYP8xydVoasprMN7/\nHbtsDg+VqWpIROJryBJB9BTxHjm6aK/8QDAAPVQmInFXSNXQb8zsf5vZvmY2Lfcpec5GS90MqG2E\n9YM0GNel9RJ7EYm1QsYM+mA0/VTeOmdPqiZqah66RKDGYhGJsUKeLJ47FhkpqaZ58Nj3oScLFYk+\nmxrr07R3ZdnemaE2vWeMpSciMpoKebL4bwZa7+4/Hv3slEhTM2Q6YNPL0HBQn025V1ZuaOtUIBCR\nWCrkyvfmvPkq4GRgGbBnBQKAdU/vEgh6X2K/rZP9pteOdc5ERMqukKqhz+Qvm9kU4D9KlqNSaDgY\nLBHaCZrP6bspb+A5EZE4KqTXUH/bgT2r3SBVFcYdWvfsLpvyB54TEYmjQtoI7ib0EoIQOA4Dbi1l\npkqiqRnWLN1l9fTaNBWmEoGIxFchbQTX5s1ngFXu3lKi/JROUzM8czt0bIWqSb2rExXGtNpKWjXw\nnIjEVCGB4FXgdXfvADCzajOb4+4rS5qz0dY0L0zXr4DZfQdPbajTswQiEl+FtBH8J9CTt5yN1u1Z\n8nsO9aNhJkQkzgoJBEl37603ieYrS5elEpk8C9KTB3zCOAw8p0AgIvFUSCBoNbP35RbMbCGw65jO\n450ZNB0G6wfuOdTa1om7D5BQRGRiKyQQfAL4spm9amavApcBF5c2WyWSG3Oo3wW/oS5NV6aHbZ2Z\nMmVMRKR8Cnmg7CXgeDOri5bbSp6rUmlqhs6tsGU1TJnduzr/6eJJValy5U5EpCyGLRGY2TfMbIq7\nt7l7m5lNNbOrxiJzoy7Xc6hfO0F+IBARiZtCqobOcPc3cgvR28rOHC5R9P6CxWb2rJk9Y2aXROun\nmdn9ZvZCNJ068uwXacahYdqv51D+wHMiInFTSCBImFk6t2Bm1UB6iP1zMsAX3P0w4HjgU2Z2GHA5\n8IC7HwQ8EC2PjXQ9TNlPJQIRkTyFPFB2M/CAmf0QMOAC4KbhErn768Dr0fw2M1sBzAQWAu+MdrsJ\neJDQAD02mubtMubQlOoUiQpTIBCRWBq2RODu1wBXAYcCBwO/BvYr5iRmNgc4CngUaIqCBMBaoGmQ\nNBeZ2RIzW9La2lrM6YbW1AwbX4Dujt5VFRVGQ12lqoZEJJYKHX10HWHgub8CTgJWFHqCqLfRL4BL\n3X1r/jYPHfcH7Lzv7ovcfYG7L2hsbCz0dMNragbvgdbn+qzWKytFJK4GrRoyszcBH4o+G4BbAHP3\ndxV6cDNLEYLAze5+e7R6nZnt7e6vm9newPoR534k8nsO7TO/d3V4ib0GnhOR+BmqRPAc4e7/LHc/\n0d2vJ4wzVBAzM+BGYIW7/1PepruA86P584E7i8vybpo2F5LVuzYYa+A5EYmpoQLB+wmNvYvN7N/N\n7GRCY3GhTgA+ApxkZsujz5nA1cApZvYC8O5oeexUJGDGIbt0IW2sT7OhrZOeHg0zISLxMmjVkLv/\nEvilmdUSevpcCswwsxuAO9z9vqEO7O7/w+CB4+QR5nd0NDXD87/us6qhLk2mx9myo5uptXvemHoi\nIiNVSK+h7e7+M3d/LzALeJyx7O5ZCk3zYHsrtO1snuh9lkA9h0QkZop6Z7G7b45685T3jn53DfBu\ngtzTxWonEJG4GcnL6/d8M3KBYGeDsV5iLyJxFc9AUDsd6vceMBCoRCAicRPPQAAw47A+VUOTqpJU\nJirURiAisRPfQNDUDK1/hmx4GY2Z6eliEYmlGAeCeZDtgo0v9q5qUCAQkRiKcSDYtedQY12lhpkQ\nkdiJbyBoeBNUJHdpMFaJQETiJr6BIFkJDQf3CQQNdWk2be8kq2EmRCRG4hsIAJoOg/U7X1LTWJ+m\nx2HTdlUPiUh8xDwQNMOW1bAjvJK5UU8Xi0gMxTwQRO8miEoFDXq6WERiKOaBoO9QEyoRiEgcxTsQ\n1O8N1VN7u5BqBFIRiaN4BwKzUD0UlQhq00mqUwk2qEQgIjES70AAYcyh9SugpweIniVQiUBEYkSB\noKkZutrgjVUANNRVqrFYRGJFgSDXcyjXYKyni0UkZhQIZhwCmAKBiMSWAkFlLUzbv7fnUENdms3t\n3XRne8qcMRGRsaFAAKGdIK9EALBRo5CKSEwoEEAIBJtehq723pfYq8FYROJCgQCiJ4wdWlfo3cUi\nEjsKBNBnqAkNMyEicaNAADBlDqRqQyDQMBMiEjMKBAAVFeHdBOueoSqVoD6dVIlARGJDgSCnqTl0\nIXXXMBMiEisKBDkzmmHHZti2loa6tAaeE5HYUCDIyW8wVolARGJEgSCn6bAwXfd0GHhOJQIRiYmS\nBQIz+4GZrTezp/PWTTOz+83shWg6tVTnL1r1VJg0q7dEsLUjQ0d3tty5EhEpuVKWCH4EnN5v3eXA\nA+5+EPBAtDx+RENNNOrdxSISIyULBO7+MLCp3+qFwE3R/E3A2aU6/4g0NcOGP9NYbQBs0HhDIhID\nY91G0OTur0fza4GmwXY0s4vMbImZLWltbR2j3DVDT4ZZPWsAPV0sIvFQtsZid3fAh9i+yN0XuPuC\nxsbGsclU1HOosf1FQFVDIhIPYx0I1pnZ3gDRdP0Yn39o0w+ERCWTtvwZUIlAROJhrAPBXcD50fz5\nwJ1jfP6hJVLQeDCJ1meZUpNSIBCRWChl99GfA48AB5tZi5l9FLgaOMXMXgDeHS2PL03zYN0z4eli\nVQ2JSAwkS3Vgd//QIJtOLtU5R0VTMzzxc+bu1UnrtlS5cyMiUnJ6sri/GeEJ48NTLRpmQkRiQYGg\nv6Z5ALyJVzXMhIjEggJBf3UzoKaB/bpfYXtXlvauTLlzJCJSUgoE/ZlBUzNNHdGzBNv0dLGITGwK\nBANpmsfkbS9SQQ+tbR3lzo2ISEkpEAykqZlEtoP9bJ2eJRCRCU+BYCDRuwkOttW0auA5EZngSvYc\nwR6t8RDcKji04lWVCERkxNydTI/TmemhsztLV7aHzu6evGmWzu6esD3TQ2cmS1c0n5v+9bGzmVxT\n2meaFAgGkqrGph/IERtbuE+BQCR2Mtke2ruztHdmaevsZltHhm0dGdo6M7R1ZNjWmWFbRzdt0bqw\nnKGto7vPPts7M/QMOrRmYU45bIYCQdk0NXPwpkf4mR4qExnXOjNZtndm2d4ZLsq5aXtXlrbODDu6\nsmzvytDemaU96hK+vSvLjq4M2zuz0QU/02dbV6anoHPXVCaoSyepr0pSV5WiPp1kRn0VdVVJ6tLh\nk05WkE5VkE4mqExWkE5WRNNE3nzfdenkzv2rUqWvwVcgGExTM/s8cwd/XvUaP/njKs46fG+m1laW\nO1cieyR3pyvbw46ucDHe0Z3tN58ZcH17Z4a2znCBzl3kt3eGC/z2rrDcnS3slrvCoKYySU1lgtp0\nkupUgtp0ginVKfaZXEVNZZLadILqygS10X41lbmLfJL6dDStSlGXTlJbmSCZmBjNrAoEg4meMD6q\n6nW++ssE/+fuZ3jnwTM456iZnHTIDKpSiTJnUGR0ZHucrt466Wyon86G5Y68C3N73gW7vSv/gt1/\n3c6Lem7dju4s2SLrSCoTFVRHd9y16XBRrksnaaxPUxvdbeemuYv7znVhuTbvwp9OVmBmJfoW92wK\nBIOJxhz6zrsquWifE/nl42u4c/lr3P/sOuqrkpw5b2/OPmomx82dRkWFflwytroyPbzR3sXm9m42\nt3f1md+8Pcy/0d7F1o5MXsNjtveCn7vQd2Z6ir5A5+TusKsrE9RUJqhOJXrvohvq0mFddCGuTiX6\n7Fcd7dd/fUgT5ifK3faeQIFgMFNmQ2U9tu4Zmhf8Lc37TObyMw7lkZc2cvvjLdzz5GvcsmQ1+0yu\nYuFRMznnqJm8qam+3LmWPURPj7M9qu7INURu6+jubZDMzW/d0Z13sd85bescfOiTdLKCqTWVTK2t\npL4qyeTqFJWJil3qoysTO+stavmAAAAQ4klEQVSlK3Preuunw/Z0qoLqVK6aZGe1SXVlQnfYE4iF\nN0aObwsWLPAlS5aM/YlvPA22tMDp34RD3gMVO6uD2rsy3P/sOn75+BoefmED2R7nsL0ncc5RM3nf\n/H1omlQ19vmVUdPT47R1hd4fHd2humSwaecQ23d09/T2Lum92EeNmcP91zOD+nSSqbWVTKmpZGpN\niqk1lUyJplNrB1hXU0l1paotJTCzpe6+YNj9FAiG8PKDcNdn4Y1VMHUuvOVTMP88qKzps9uGtk7u\neeI17nh8DU+0bKHC4IQDGzh7/kxOPnQGU2rUyDyW3J1sT+i/3dGdZcuObrbuyLBlR/eAn60DrNvW\n0T2ibn9mUJVMkE5VUJVMUJWqoL4qRX1V1OiYDvOTqvo2PNZH87n96qtS1KQSqnaU3aJAMFp6srDi\nbvjD9bBmCVRPhTd/DI69KIxU2s9LrW3c+fga7li+htWbdgBQX5Vk36k17DutOprunJ81tSYWd3C5\nqpD8qo+tHZneO+X8vtq5O+dcF8Bsj9OdDXXZ2R6nu6eHbDZc6DM9Tiba1h1tL7TOO5UwJlenmFSd\nYvIgn/qqJFWpqEtfNO1dji70+dNUwlRdIuOGAsFoc4fVj4aA8Ny94f3GR3wQ3vJpmHHIALs7S1dt\nZtmrm1m9aQerN7ezelM7LZt30Nmvj3JDXZp9p1Uza2oN+06tDoEiChwz6qtIJyvG5M4w2+O9PT62\nd+6c7ujORtUcWXZ09fR298tf7ugOvUV2dPfQkesGGHX/29aRoa2rsKqQunSSSXl3ydWVCVKJChIV\nRiphJCoqSFZY3rKRzK1LGMl+y+lkYtCLfFVKddwysSkQlNLGl+CRf4HlN0OmAw46Dd76aZjztnA1\nG4K709rWyepNO2iJgkNvoNjczmtvdAx4R5u7E61KVVCdSoS70lSCqmToYperhgiNeInefSvMeh+m\nyZ/2Xujz1nV0F/YQTU6u10hVKkF1ZQU1qSRVlQmqozyG3h/JXapC6qOHbeqrUn3WqypEZHQpEIyF\n7RvgsRvhT4ugfQPsfSS85TPQfHYoMYxAJtvD61s6WL05lB42tnXR0Z2lI5OloytLR3cPHZnQN7sj\n1zAZ3X13dPf03rnnxjOBEERqc32tK5PUpBN9+lf3mfbbnuseuPPCvnOqahCR8U2BYCx174Anbwml\nhA3Pw6RZcPwn4ei/gapJZctWtsdxd/XHFokpBYJy6OmBF+4L7Qir/gfSk2DeX8CcE2H28TB5Vrlz\nKCIxUmgg0ANlo6miAg4+PXzWLINHvgdP/Scs/WHYPnnfEBBmHw+z3wKNh4Y0IiJlpEBQKjOPhr/8\nAWQzsO5pePWP8Ooj8MrvQnAASE+GfY/dGRhmHg2p6vLmW0RiR4Gg1BJJ2Gd++Bz/idAN9Y1VOwPD\nq3+E394f9q1IwT5H7QwM+x4HtdPLm38RmfDURjAetG8KzyjkAsOaZdDTHbZNPwhmLYCZx8CsN0NT\n84h7JIlIvKiNYE9SMw0OPiN8IPRCeu3xEBhWPwYv/gae+HnYlqyCvefnBYcFoe1B3ThFZIQUCMaj\nVDXs99bwgag66dUwxEXLUmh5DP7075D9XtheOyOUFmYdAzMXhLaGtEZCFZHCKBDsCcxg6n7hM+8v\nwrpMV2iEXrMUWpaEIPHne3MJoPGQEBgaD4WqyeF5hvSkaJq3nNIoqSJxpzaCiaR9E7y2bGepYc1S\n2LFp6DSJyrwAkT+dHKbVU6Bmeqi+qpm+81M9TUFEZJxTG0Ec1UyDA98dPhCqlDq2QOdW6NjabzrY\n+q2w/eWdy51bBz9fZd2uAaJ/0Khrgmn7Q22j2jFExikFgonMLNzRV08Z+TGyGdixGdo3DvDZ1Hd5\nwwthXde2XY+TngTT5sK0A2D6AX2nNdMUJETKqCyBwMxOB74LJIDvu/vV5ciHFCCRhLrG8ClUpnNn\nkNj6Gmx6CTa9HEZtfW0ZPPtL8LyRTqsmh1LDLkFi/xAkRKSkxjwQmFkC+BfgFKAFeMzM7nL3Z8c6\nL1IiyTRM2jt89pq36/ZMV3iobuNLIUjkpi1/gqd/AeS1WyWrQ1tEIh2Om6yCZGWY9q7L25ao7LdP\nZXjFqCXCtCIZzVfkzSejfSry5qP1EJ7pyHZDTyaaRsv587lt2a6++wGkakJPsFQNVNYOMF8b3nrX\nZ76mz6tRRUqpHCWCY4EX3f1lADP7D2AhoEAQF8lKaDgofPrLdMLmlTuDw7a1YV22M0xzn9xyx5a8\n5Y4QZDId4YKc6ehb8hgLicrwhHgiGaYQ8tG1nT4BrqBjRcFtSIUe08ByUyNa2Dnff7rLtn7H6rM4\n3PZ+M332778ub9tA1YW7dG7xYbb3z9oA5x7wXIVWVY6ks02R399f3xKqVUuoHIFgJrA6b7kFOK7/\nTmZ2EXARwOzZs8cmZ1J+yTQ0Hhw+o6EnG30y4NG0pydvPretZ+dy/n54KBkkUn0v8InKaF3etorE\n4G0d7iEgdO8IQaG7PZru6De/Hbrad85nOhn2ojRc+4p7+Dv6TBlg3SDb+hxrl4MPcK4Btucfd5d9\n++/Tb93uBqKB8jrQuQbdtpvf/6DHHu7ckWS68OOP0LhtLHb3RcAiCN1Hy5wd2VNVRFVCVJY3H2ZR\n1U+12j1k3CnHGMhrgH3zlmdF60REpAzKEQgeAw4ys7lmVgmcC9xVhnyIiAhlqBpy94yZfRr4NaH7\n6A/c/ZmxzoeIiARlaSNw9/8C/qsc5xYRkb70nkQRkZhTIBARiTkFAhGRmFMgEBGJuT3ifQRm1gqs\nGmHyBmDDbpxe6ZVe6ZV+T02/n7sPP2Kku0/oD7BE6ZVe6ZU+jukL/ahqSEQk5hQIRERiLg6BYJHS\nK73SK31M0xdkj2gsFhGR0olDiUBERIagQCAiEnMTOhCY2elm9mcze9HMLi8y7Q/MbL2ZPT3Cc+9r\nZovN7Fkze8bMLikyfZWZ/cnMnojSf30EeUiY2eNmdk+xaaP0K83sKTNbbmZLRpB+ipndZmbPmdkK\nM3tLEWkPjs6b+2w1s0uLPP/nou/uaTP7uZkN997H/ukvidI+U8i5B/rNmNk0M7vfzF6IplOLTP9X\n0fl7zGzBCM7/j9H3/6SZ3WFmU4pM/w9R2uVmdp+Z7VNM+rxtXzAzN7OGIs//NTNbk/c7OLPY85vZ\nZ6Lv4Bkz+1aR578l79wrzWx5kennm9kfc/+HzOzYItMfaWaPRP8P7zazSYOl3y1j0Ue1HB/CENcv\nAfsTXk/1BHBYEenfDhwNPD3C8+8NHB3N1wPPF3l+A+qi+RTwKHB8kXn4PPAz4J4R/g0rgYbd+De4\nCfhYNF8JTNmNf8u1hIdjCk0zE3gFqI6WbwUuKCL9POBpoIYwSu9vgAOL/c0A3wIuj+YvB64pMv2h\nwMHAg8CCEZz/VCAZzV8zgvNPypv/LPCvxaSP1u9LGHZ+1VC/p0HO/zXgfxf4bzZQ+ndF/3bpaHlG\nsfnP2/5t4O+KPP99wBnR/JnAg0Wmfwx4RzR/IfAPhf6Gi/lM5BLBscCL7v6yu3cB/wEsLDSxuz8M\nbBrpyd39dXdfFs1vA1YQLk6Fpnd3b4sWU9Gn4JZ9M5sFvAf4fsGZHkVmNpnww74RwN273P2NER7u\nZOAldy/26fIkUG1mScIF/bUi0h4KPOru7e6eAR4C3j9UgkF+MwsJAZFoenYx6d19hbv/uZAMD5L+\nvij/AH8kvBGwmPRb8xZrGeI3OMT/meuALw2Vdpj0BRkk/SeBq929M9pn/UjOb2YGfAD4eZHpHcjd\nxU9miN/gIOnfBDwczd8P/MVg6XfHRA4EM4HVecstFHEhHk1mNgc4inBXX0y6RFQUXQ/c7+7FpP8O\n4T9fTzHn7MeB+8xsqZldVGTauUAr8MOoeur7ZlY7wnycyxD/AQfi7muAa4FXgdeBLe5+XxGHeBp4\nm5lNN7Mawt3cvsOkGUiTu78eza8FmkZwjNFyIfCrYhOZ2f81s9XAecDfFZl2IbDG3Z8o9rx5Ph1V\nT/1gqKq1QbyJ8O/4qJk9ZGZvHmEe3gasc/cXikx3KfCP0fd3LXBFkemfYecN7F8xst/gsCZyIBgX\nzKwO+AVwab+7q2G5e9bd5xPu4o41s3kFnvMsYL27Ly06w32d6O5HA2cAnzKztxeRNkko5t7g7kcB\n2wlVI0Wx8DrT9wH/WWS6qYT/QHOBfYBaM/twoendfQWhKuU+4L+B5UC2mDwMcEyniFLdaDKzK4EM\ncHOxad39SnffN0r76SLOWQN8mSKDRz83AAcA8wkB/dtFpk8C04DjgS8Ct0Z398X6EEXejEQ+CXwu\n+v4+R1RCLsKFwP8ys6WEKuauEeRhWBM5EKyhb/ScFa0bM2aWIgSBm9399pEeJ6pSWQycXmCSE4D3\nmdlKQpXYSWb20xGcd000XQ/cQahuK1QL0JJXirmNEBiKdQawzN3XFZnu3cAr7t7q7t3A7cBbizmA\nu9/o7se4+9uBzYR2nmKtM7O9AaLpoFUTpWJmFwBnAedFwWikbqa4qokDCIH4iei3OAtYZmZ7FXoA\nd18X3RD1AP9Ocb9BCL/D26Oq1j8RSsiDNlgPJKpafD9wS5HnBjif8NuDcDNTVP7d/Tl3P9XdjyEE\nopdGkIdhTeRA8BhwkJnNje4qzwXuGquTR3cdNwIr3P2fRpC+MdfDw8yqgVOA5wpJ6+5XuPssd59D\n+Lt/6+4F3w1H56w1s/rcPKHRseAeVO6+FlhtZgdHq04Gni0mD5GR3om9ChxvZjXRv8XJhHaagpnZ\njGg6m3Ah+NkI8nEX4WJANL1zBMcYMTM7nVBF+D53bx9B+oPyFhdS4G8QwN2fcvcZ7j4n+i22EDpQ\nrC3i/HvnLZ5DEb/ByC8JDcaY2ZsInRaKHc3z3cBz7t5SZDoIbQLviOZPAoqqWsr7DVYAXwH+dQR5\nGF4pWqDHy4dQr/s8IYpeWWTanxOKot2EH/BHi0x/IqEa4ElCtcJy4Mwi0h8BPB6lf5oheisMc5x3\nMoJeQ4TeVk9En2eK/f6iY8wHlkR/wy+BqUWmrwU2ApNH+Ld/nXDhehr4CVHPkSLS/44QvJ4ATh7J\nbwaYDjxAuAD8BphWZPpzovlOYB3w6yLTv0hoK8v9Bofq9TNQ+l9E39+TwN3AzJH+n2GYXmiDnP8n\nwFPR+e8C9i4yfSXw0+hvWAacVGz+gR8Bnxjhv/+JwNLoN/QocEyR6S8hXMOeB64mGg1itD8aYkJE\nJOYmctWQiIgUQIFARCTmFAhERGJOgUBEJOYUCEREYk6BQAQws6z1He206Keghzj2nIFG5BQZL5Ll\nzoDIOLHDw3AeIrGjEoHIEKIx6L8VjQf/JzM7MFo/x8x+Gw2G9kD09DFm1mRh3P8nok9uWIuEmf17\nNCb+fdHT4iLjggKBSFDdr2rog3nbtrj74cD3CKO6AlwP3OTuRxDG4PnnaP0/Aw+5+5GEsZWeidYf\nBPyLuzcDb1Ci4YRFRkJPFosAZtbm7nUDrF9JGJbg5WgQwbXuPt3MNhCGO+iO1r/u7g1m1grM8mj8\n++gYcwjDiB8ULV8GpNz9qtL/ZSLDU4lAZHg+yHwxOvPms6h9TsYRBQKR4X0wb/pINP8HwsiuEF7Y\n8rto/gHCGPS5FwtNHqtMioyU7kpEgup+Lyb/b3fPdSGdamZPEu7qPxSt+wzh7WtfJLyJ7W+j9ZcA\ni8zso4Q7/08SRpQUGbfURiAyhKiNYIG7FzuGvcgeQ1VDIiIxpxKBiEjMqUQgIhJzCgQiIjGnQCAi\nEnMKBCIiMadAICISc/8fojp6g+kBbMIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(get_epoch_test(test_cross_ent), label = 'CNN_test')\n",
    "plt.plot(get_epoch_train(train_cross_ent), label = 'CNN_train')\n",
    "#plt.plot(1-get_epoch_train(dnn_training_acc), label = 'DNN_Train')\n",
    "plt.legend()\n",
    "plt.title('CNN Training and Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(range(0, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
